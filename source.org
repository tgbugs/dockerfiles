# -*- orgstrap-cypher: sha256; orgstrap-norm-func-name: orgstrap-norm-func--dprp-1-0; orgstrap-block-checksum: 2bfed64839aea1c94c0d1008c52d436dec56b8cd0ac7cdc2c643de014c25dda1; -*-
# [[orgstrap][jump to the orgstrap block for this file]]
#+title: Literate source for docker files

#+property: header-args :eval no-export
# #+property: header-args:bash :var BUILDKIT_PROGRESS="plain"
#+property: header-args:conf :mkdirp yes :noweb yes
#+property: header-args:dockerfile :noweb yes :mkdirp yes :comments link
#+property: header-args:screen :session org-session :cmd bash :noweb no-export :terminal (or)
#+options: broken-links:mark

#+name: orgstrap-shebang
#+begin_src bash :eval never :results none :exports none
set -e "-C" "-e" "-e"
{ null=/dev/null;} > "${null:=/dev/null}"
{ args=;file=;MyInvocation=;__p=$(mktemp -d);touch ${__p}/=;chmod +x ${__p}/=;__op=$PATH;PATH=${__p}:$PATH;} > "${null}"
$file = $MyInvocation.MyCommand.Source
{ file=$0;PATH=$__op;rm ${__p}/=;rmdir ${__p};} > "${null}"
emacs -batch -no-site-file -eval "(let (vc-follow-symlinks) (defun org-restart-font-lock ()) (defun orgstrap--confirm-eval (l _) (not (memq (intern l) '(elisp emacs-lisp)))) (let ((file (pop argv)) enable-local-variables) (find-file-literally file) (end-of-line) (when (eq (char-before) ?\^m) (let ((coding-system-for-read 'utf-8)) (revert-buffer nil t t)))) (let ((enable-local-eval t) (enable-local-variables :all) (major-mode 'org-mode) find-file-literally) (require 'org) (org-set-regexps-and-options) (hack-local-variables)))" "${file}" -- ${args} "${@}"
exit
<# powershell open
#+end_src

Quickstart. Install Emacs and Docker. Then run
#+begin_src bash
./source.org setup
./source.org --check-paths
./source.org build
#+end_src

* Setup
:PROPERTIES:
:CUSTOM_ID: setup
:END:
** Host
*** Emacs
This file needs Emacs installed on the host system.

At some point we will get bootstrap worked out so that only docker or
podman is needed, but we aren't there yet.

On windows-nt you will need to symlink this file to source.ps1 if you
want to invoke it from powershell.

*** Bash
This file needs bash installed on the host system.

Most linux distros and macos already have some version of bash,
however on windows you will need to install git so that git bash is
present on the host system. An error is thrown if it is missing or
if the wsl2 bash is detected.

*** Docker
This file needs docker or podman installed on the host system.

There are certain subtle differences between docker running on each of
the major operating systems so some additional configuration may be
required.

We account for the variants that we known about, but have definitely
missed a few, and they can be confusing and hard to debug. Turns out
docker is for building containers not for creating robust and portable
workflows.

** Server
*** docker config
Docker files in this repo use buildkit features. To enable it include
the following in [[/etc/docker/daemon.json]].
#+name: docker-daemon-config
#+begin_src json :tangle /etc/docker/daemon.json :tangle no
{"experimental": true,
 "features": {"buildkit": true}}
#+end_src

Also make sure that you have the docker buildx plugin installed,
e.g. on gentoo app-containers/docker-buildx.

*** portage ssh keys
Make sure that =/var/lib/portage/home/.ssh= exists and has the keys
for accessing the interlex repo or other private repos.

*** distcc hosts
Even if you are not using distcc be sure to run
#+begin_src bash
touch /etc/distcc/hosts.docker
#+end_src
on the host os otherwise the builder functions will produce cryptic
errors for some packages because they can handle an absent
/etc/distcc/hosts file but not one that is a directory. Note that
docker has what might be considered a reasonable default which is that
it will create a directory if it does not exist when it is the source
of a mount rather than fail ... but in our case this is annoying, and
the cleanup is a pita, have to unmount and rmdir in the image, or just
resnap the image because wow what a pain.

** Client
Building the precursor images in =gentoo/stage3= for this repo from
[[https://github.com/gentoo/gentoo-docker-images][scratch]] requires the =buildx= extension which requires experimental
features to be enabled in the client.

If you want to push the images to a remote docker repository add the
auths section as well (and fill it in with your credentials).
#+name: docker-client-config
#+begin_src json :tangle ~/.docker/config.json :tangle no
{"experimental": "enabled",
 "auths": {
   "https://index.docker.io/v1/": {
     "auth": "XXX NOT A REAL KEY FILL ME IN XXX"}}}
#+end_src

** Package host
Prepare package host folders. The server will be started automatically
below if it is not already up.
#+begin_src screen
# this step is now done as part of ./source.org setup
mkdir -p <<&host-binpkgs-root-path()>>/<<&host-binpkgs-repo-name()>>
mkdir -p <<&host-binpkgs-root-path()>>/cross/gnu/x86_64-pc-linux-musl
mkdir -p <<&host-binpkgs-root-path()>>/cross/gnu/x86_64-gentoo-linux-musl
#+end_src
*** prepare all binpkg repos
We have to prepare all the binpkg repos so that during a first build
they have some basic metadata, otherwise trying to emerge binpkgs will
fail with strange errors in strange ways.
#+name: &run-quickpkg-first-time
#+begin_src bash :noweb yes :results none :var _use_podman=(if use-podman "1" "") :var _in_path_binpkgs=(or path-binpkgs)
# bash-safe-block
shopt -s expand_aliases
if [ -n "${_use_podman}" ]; then
alias docker=podman
fi
for stage3 in {amd64-hardened-openrc,amd64-musl-hardened}; do
    docker pull docker.io/gentoo/stage3:${stage3}
    docker run \
    -v ${_in_path_binpkgs}:/var/cache/binpkgs \
    -e QUICKPKG_DEFAULT_OPTS="--include-unmodified-config=y --umask=022" \
    --rm \
    docker.io/gentoo/stage3:${stage3} \
    quickpkg "virtual/ssh";
done
#+end_src

** Portage git ssh access
Needed for live ebuilds that point to private git repos.
Eventually this should be baked into the docker in docker top level image.

This section assumes that the host system is running gentoo.
If it is not then the [[#portage-ssh-keygen][portage ssh keygen]] section
below can be used to generate an equivalent folder and you
can change [[&host-ssh-path]] to point to it.

You will need to generate ssh keys for the host system if they do not
already exist, and you will need to register them with the git remote.

This is similar to what we do for [[#portage-maven][portage-maven]].

The default location for the portage home directory is now
=/var/lib/portage/home/.ssh= which means that the ssh config and
private keys can be stored there persistently and safely without
violating the sandbox during package fetch.

In the even that you have to deal with some strange legacy case you
may want to symlink a path outside the sandbox to a path inside the
sandbox due to the change in home directory from =/var/tmp/portage= ->
=/var/lib/portage/home=. Inside the images we have to run
=ln -s /var/tmp/portage/.ssh /var/lib/portage/home/.ssh=.

If =/var/tmp/portage= is still the portage home folder and it is a
ramdisk that is wiped on reboot you will want the following.
#+begin_src bash :tangle /su::/etc/local.d/20portage-symlinks.start :tangle-mode 0755 :tangle no
# relink .ssh across restarts
ln -s /mnt/str/portage/.ssh /var/tmp/portage/
chown -h portage:portage /var/tmp/portage/.ssh
#+end_src

*** portage ssh keygen
:PROPERTIES:
:CUSTOM_ID: portage-ssh-keygen
:END:

Example ssh folder [[file:./portage-ssh]]
#+begin_src bash :results none
if [ ! -d portage-ssh ]; then
mkdir portage-ssh
ssh-keygen -t ed25519 -N "" -C "portage@${HOSTNAME}" -f portage-ssh/id_ed25519.${HOSTNAME}.portage
echo "
Host github.com
HostName github.com
user git
IdentityFile ~/.ssh/id_ed25519.${HOSTNAME}.portage
PreferredAuthentications publickey
" >> portage-ssh/config
fi
#+end_src

If you are on a gentoo system you can copy the contents to =/var/lib/portage/home/.ssh=.
A suggested alternative on non-gentoo systems is =~/files/portage/home/.ssh=.

Move the generated keys to a persistent location where they won't
accidentally be deleted e.g. by a call to git clean.
#+begin_src bash :eval never :noweb yes
mv ./portage-ssh/* <<&host-ssh-path()>>/
#+end_src

Add the contents of =portage-ssh/id_ed25519.${HOSTNAME}.portage.pub=
as an ssh key to the github account that will access the private
repo(s) https://github.com/settings/keys.  Usually this is a robot
account with limited access.

#+begin_src bash :noweb yes :results drawer
cat <<&host-ssh-path()>>/id_ed25519.${HOSTNAME}.portage.pub
#+end_src

Run the following in an interactive terminal to confirm the hostkeys
and ssh mount is working.  The login to github does not need to be
successful at this step.
#+begin_src bash :noweb yes
<<&screen-pass-vars()>>
<<&builder-vars>>

# connect to github, check and accept the host keys
docker run \
<<&builder-args>>
--rm -it \
tgbugs/musl:updated \
runuser -u portage -- ssh -T git@github.com

# make sure known_hosts was persisted to the host system
cat ${_path_ssh}/known_hosts | grep github
#+end_src

Correct permissions on the ssh folder contents in a running container
and ensure they are persisted correctly on the host.
#+begin_src bash
<<&screen-pass-vars()>>
<<&builder-vars>>

docker run \
<<&builder-args>>
--rm \
tgbugs/musl:updated \
sh -c 'chown -R 250:250 /var/lib/portage/home/.ssh; chmod 0600 /var/lib/portage/home/.ssh/*;'
#+end_src

Confirm everything works.
#+begin_src bash :noweb yes :results drawer
<<&screen-pass-vars()>>
<<&builder-vars>>

# connect to github and confirm ssh key
2>&1 \
docker run \
<<&builder-args>>
--rm \
tgbugs/musl:updated \
runuser -u portage -- ssh -T git@github.com
#+end_src

** git ignore
This takes care of itself.
#+begin_src conf :tangle .gitignore
.gitignore
docker-profile/*
repos/*
common/*
musl/*
gnu/*
other/*
sckan/*
bin/*
helper-repos/*
portage-ssh/*
retfile/*
#+end_src
* Ops
** CLI
Be sure to complete the [[#setup][Setup section]] first, e.g. run
=./setup.org setup= and =./setup.org --check-paths=.

The standard way to use this file to build is to run the following block
#+begin_src bash :eval never
./source.org build --refresh --repos --resnap --live-rebuild
#+end_src

NOTICE: avoid running this command from inside the =org-session=
screen, input will be severely broken

** Build
If you are bootstrapping this file from scratch you will need to build
dependent images in order.

To prepare a fresh cycle of images.
# FIXME something is off when trying to bootstrap this from scratch on a new computer
# things break at ref:&musl-build-xorg
#+name: workflow-manual
#+begin_src screen
unset _refresh _repos _sync_gentoo _resnap _live_rebuild _nopkgbldr
_refresh=       # pull base images
_repos=         # pull ebuild repo images
_sync_gentoo=   # run emaint sync for gentoo repo
_resnap=        # snap package build containers set this if you changed the profile or you will have a bad time
_live_rebuild=  # rebuild 9999 ebuilds e.g. from git
_nopkgbldr=     # do not run package building steps
_only_static=   # only build static images

<<&workflow-common>>
#+end_src

#+name: &workflow-vars
#+begin_src bash
# XXX NOTE that these are embedded in the docker files right now
_img_portage=${_img_portage:-docker.io/gentoo/portage:latest}

_musl_img_stage3=${_musl_img_stage3:-docker.io/gentoo/stage3:amd64-musl-hardened}

__is3_m1=${_musl_img_stage3/docker.io\//}
__is3_0=${__is3_m1/\//-}
__is3_1=${__is3_0/:/-}
__src_from_img=${__is3_1/gentoo/latest}
_musl_src_stage3=${__src_stage3:-${__src_from_img}}
unset __is3_m1 __is3_0 __is3_1 __src_from_img

_gnu_img_stage3=${_gnu_img_stage3:-docker.io/gentoo/stage3:amd64-hardened-openrc}

# FIXME DRY

__is3_m1=${_gnu_img_stage3/docker.io\//}
__is3_0=${__is3_m1/\//-}
__is3_1=${__is3_0/:/-}
__src_from_img=${__is3_1/gentoo/latest}
_gnu_src_stage3=${__gnu_src_stage3:-${__src_from_img}}
unset __is3_m1 __is3_0 __is3_1 __src_from_img
#+end_src

#+name: package-server-cli
#+begin_src screen
HISTFILE=~/.org_session_history
<<&screen-set-vars>>
<<&workflow-vars>>
<<&builder-vars>>
<<&package-server-funs>>
package-server-running || package-server
#+end_src

#+name: workflow-cli
#+begin_src screen
HISTFILE=~/.org_session_history
<<&screen-set-vars>>
<<&workflow-common>>
#+end_src

#+name: bash-async-package-server-cli
#+begin_src bash :session package-server :async yes :results none :noweb yes
# bash-safe-block
<<package-server-cli>>
#+end_src

#+name: bash-async-workflow-cli
#+begin_src bash :session org-session :async yes :results none :noweb yes :var BUILDKIT_PROGRESS="plain"
# bash-safe-block
export BUILDKIT_PROGRESS=$BUILDKIT_PROGRESS
<<workflow-cli>>
#+end_src

Source =./bin/workflow-funs.sh= on changes after tangle.
#+name: &workflow-funs-main-to-tangle
#+begin_src bash :tangle ./bin/workflow-funs.sh :mkdirp yes :noweb yes :var BUILDKIT_PROGRESS=""
<<&workflow-funs>>
<<&workflow-main>>
#+end_src

Note that we can't =source ./bin/workflow-funs.sh= in this context
because tangle is called inside =run-main=. We would need to rework
the call order so that tangle is run first or unify the elisp and bash
tangle code.
#+name: &workflow-common
#+begin_src screen
<<&workflow-funs>>
<<&workflow-main>>
run-main
#+end_src

#+name: &workflow-main
#+begin_src screen
function run-main () {
if [ -n "${_refresh}" ]; then  # FIXME implicit global variable when it is hard to set the variable in some contexts :/
    pull musl || echo "ERROR pull musl failed with ${?}"
    pull gnu || echo "ERROR pull gnu failed with ${?}"
fi
if [ -n "${_refresh}" ] || [ -n "${_repos}" ]; then
    pull-portage || { CODE=$?; echo "ERROR failed to pull portage"; return ${CODE}; }
fi
package-server-running || { CODE=$?; echo "ERROR package-server is not running"; return ${CODE}; }
pushd ~/git/dockerfiles  # FIXME hardcoded path
tangle && {
  # FIXME popd sigh ... unwind-protect when
  run-profile || return $?
  run-common || return $?
  run-gnu || return $?
  run-musl || return $?
}
# package host
# build a bunch of packages
popd
}
#+end_src

There are many ways to run the package server, the easiest one is if
the host system has python. If not, given the tools at hand and the
data we pull anyway, the best alternative is the musl image python.
#+name: &package-server-funs
#+begin_src screen
function package-server-running () {
local host_python have_host_python check_command
host_python=$(command -v python)
have_host_python=$?
check_command="import sys, socket as k; s = k.socket(k.AF_INET, k.SOCK_STREAM); r = s.connect_ex(('localhost', <<&host-binpkgs-port()>>)); s.close(); sys.exit(r)"
if [ ${have_host_python} -eq 0 ]; then
   echo using host python for package-server-running 1>&2
   ${host_python} -c "${check_command}" || return $?
else
    echo using ${_musl_img_stage3} python for package-server 1>&2
    docker run --network host --rm ${_musl_img_stage3} python -c "${check_command}" || return $?
fi
}

function package-server () {
local host_python have_host_python
host_python=$(command -v python)
have_host_python=$?
if [ ! -d "${_path_binpkgs}" ]; then
    echo package-server setup incomplete missing ${_path_binpkgs} 1>&2
    echo please run ./source.org setup 1>&2
    return 1
fi
if [ ! -f "${_path_binpkgs}/Packages" ]; then
    echo package-server setup incomplete missing ${_path_binpkgs}/Packages 1>&2
    echo please run ./source.org setup 1>&2
    return 1
fi
if [ ${have_host_python} -eq 0 ]; then
   echo using host python for package-server 1>&2
   ${host_python} -m http.server -d ${_path_binpkgs_root} --bind 127.0.0.1 <<&host-binpkgs-port()>>
else
    echo using ${_musl_img_stage3} python for package-server-running 1>&2
    docker run \
           -v ${_path_binpkgs_root}:/tmp/binpkgs \
           -p <<&host-binpkgs-port()>>:8000 \
           --rm -t ${_musl_img_stage3} \
           python -m http.server -d /tmp/binpkgs 8000
fi
}
#+end_src

# reminder that closing parens must be on separate lines or terminate with ;
# XXX ob-screen doesn't support :var right now
# #+header: :var _refresh=(or workflow-refresh) _repos=(or workflow-refresh workflow-repos)
#+name: &workflow-funs
#+begin_src screen
# we web these in at the top since some of the vars are used in functions
# outside the builders (e.g. package-server)

shopt -s expand_aliases
if [ -n "${_use_podman}" ]; then
alias docker=podman
fi

<<&workflow-vars>>

<<&builder-vars>>

<<&package-server-funs>>

<<&ghc-funs>>

<<&sbcl-funs>>

function pull-portage () {
# these are updated more or less in sync with the upstream snapshot source
local BEFORE="$(docker image inspect ${_img_portage} --format '{{.Created}}' || date --utc +%Y-%m-%dT%H:%M:%SZ --date @0)"
docker pull ${_img_portage}
local AFTER="$(docker image inspect ${_img_portage} --format '{{.Created}}' || date --utc +%Y-%m-%dT%H:%M:%SZ --date @0)"
local A_DATE=$(date -In --utc --date "${AFTER}")
local B_DATE=$(date -In --utc --date "${BEFORE}")
if [[ ${B_DATE} < ${A_DATE} ]] || ! docker container inspect local-portage-snap > /dev/null; then
    docker rm local-portage-snap
    docker create -v /var/db/repos/gentoo --name local-portage-snap ${_img_portage} /bin/true
fi
if [ -n "${_use_podman}" ]; then
    [ -n "$(podman container ls -q --filter name=local-portage-snap --filter status=initialized)" ] || \
    podman container init local-portage-snap
fi
}

function pull () {
local _src_stage3 _img_stage3
if [ "${1}" == "gnu" ]; then
    _src_stage3=${_gnu_src_stage3}
    _img_stage3=${_gnu_img_stage3}
elif [ "${1}" == "musl" ]; then
    _src_stage3=${_musl_src_stage3}
    _img_stage3=${_musl_img_stage3}
else
    echo unknown libc "${1}"
    return 1
fi
# echo src img ${_src_stage3} ${_img_stage3}

# avoid spurious pulls where the underlying stage3 has not changed
local DIST="https://distfiles.gentoo.org/releases/amd64/autobuilds"
local STAGE3_LATEST="$(curl --fail --silent "${DIST}/${_src_stage3}.txt" |\
    grep -B99 '^-----BEGIN PGP SIGNATURE-----$' | tail -n 2 | head -n 1 |\
    cut -f 1 -d'/' | sed -r 's/(....)(..)(..)T(..)(..)(..)/\1-\2-\3T\4:\5:\6/')"
local LOCDOC_LATEST="$(docker image inspect ${_img_stage3} --format '{{.Created}}' || date --utc +%Y-%m-%dT%H:%M:%SZ --date @0)"
local S3_DATE=$(date -In --utc --date "${STAGE3_LATEST}")
local LD_DATE=$(date -In --utc --date "${LOCDOC_LATEST}")
# XXX there is technically a narrow window between the release of
# a stage3 and the building of a docker image where this might fail
# have to use double square brackets for this to work correctly
[[ ${S3_DATE} < ${LD_DATE} ]] || \
docker pull ${_img_stage3}
}

function tangle () {
[ -d ./bin ] && rm -r ./bin
[ -d ./docker-profile ] && rm -r ./docker-profile
[ -d ./common ] && rm -r ./common
[ -d ./gnu ] && rm -r ./gnu
[ -d ./musl ] && rm -r ./musl
[ -d ./repos ] && rm -r ./repos
[ -d ./other ] && rm -r ./other
./source.org tangle
return $?
}

<<&container-check>>

<<&quickpkg-image>>

<<&builder-resnap>>

<<&builder-bootstrap>>

<<&builder-world>>

<<&builder-arb>>

<<&builder-arb-priv>>

<<&builder-debug>>

<<&cross-bootstrap-sbcl>>

<<&image-testing>>

function run-profile () {
<<&build-profile-base>> || return $?;
<<&build-profile-gnu>> || return $?;
<<&build-profile-musl>> || return $?;
<<&build-profile-x>> || return $?;
<<&build-profile-nox>> || return $?;
<<&build-profile-pypy3>> || return $?;
<<&build-profile-static>> || return $?;
}

function run-common () {
local REPOS="${_repos}"
<<&build-user>> || return $?;
<<&build-portage-maven>> || return $?;

# ensure that tgbugs/repos:latest exists, otherwise
# gnu-container-check and container-check will fail
docker image inspect tgbugs/repos:latest > /dev/null 2>&1
NEED_REPOS=$?

# we build both eselect repos images in common because the output is
# usable in common ideally we will be able to get rid of a continually
# rebuilt image and move to a rolling image for all the repos as we
# shift away from using docker build and toward running containers
<<&gnu-build-eselect-repo>> || return $?
<<&musl-build-eselect-repo>> || return $?;
  { [ -z $REPOS ] && [ ${NEED_REPOS} -eq 0 ]; } || {
  <<&repos-build-repos>> || return $?;
  <<&re-local-repos-snap>>
  echo repos done;
  }
}

function run-gnu () {
local REPOS="${_repos}"
local SYNC_GENTOO="${_sync_gentoo}"
local RESNAP="${_resnap}"
local LIVE_REBUILD="${_live_rebuild}"
local NOBUILD="${_nopkgbldr}"
local ONLY_STATIC="${_only_static}"

echo gnu start bootstrap
gnu-container-check
<<&gnu-build-package-builder>> || return $?
<<&gnu-run-package-builder-quickpkg>> || return $?

echo gnu builder start
docker image inspect ${_tg_pbs} > /dev/null 2>&1
NEED_RESNAP=$?
{ [ -z $RESNAP ] && [ ${NEED_RESNAP} -eq 0 ]; } || gnu-builder-resnap || return $?
[ ! -z $NOBUILD ] || gnu-builder-bootstrap || return $?
[ ! -z $NOBUILD ] || gnu-builder-world || return $?
#[ ! -z $NOBUILD ] || gnu-cross-musl-sbcl || return $?
}

function run-musl () {
local REPOS="${_repos}"
local SYNC_GENTOO="${_sync_gentoo}"
local RESNAP="${_resnap}"
local LIVE_REBUILD="${_live_rebuild}"
local NOBUILD="${_nopkgbldr}"
local ONLY_STATIC="${_only_static}"

echo musl start bootstrap
# TODO figure out how to build the binary packages at this stage without
# so that we don't have to wait for quickpkg?
container-check
<<&musl-build-updated>> || return $?; echo mbu;
  <<&musl-run-updated-quickpkg>> || return $?; echo mruq;
  <<&musl-run-ghc-fetch-etc>> || return $?; echo mgfe;
  <<&musl-run-sbcl-generate-patches-etc>> || return $?; echo msgp;
  <<&musl-build-updated-user>> || return $?; echo mbuu;

  # build the generalized builder so we can dispense with the stacked image nonsense
  <<&musl-build-package-builder-musl>> || return $?; echo mbpbm;

  # TODO conditional to speed things up
  # <<&musl-run-free-harf-nonsense>> || return $?; echo mrfhn;

  <<&musl-build-nox>> || return $?; echo mbnox;
  <<&musl-run-nox-quickpkg>> || return $?; echo mrnoxq;

  <<&musl-build-openjdk-nox>> || return $?; echo mbnoxjdk;
    <<&musl-run-openjdk-nox-quickpkg>> || return $?; echo mrnoxjdkq;

  <<&musl-build-package-builder-nox>> || return $?; echo mbpbn;
  <<&musl-build-binpkg-only-nox>> || return $?; echo mbbon;

  <<&musl-build-pypy3>> || return $?; echo mbpypy3;
    <<&musl-run-pypy3-quickpkg>> || return $?; echo mrpypy3q;

    # XXX this is the point at which things split into musl and musl/x
    <<&musl-build-xorg>> || return $?; echo mbx;
    <<&musl-run-xorg-quickpkg>> || return $?; echo mrxq;

    <<&musl-build-openjdk>> || return $?; echo mbjdk;
      <<&musl-run-openjdk-quickpkg>> || return $?; echo mrjdkq;

    <<&musl-build-package-builder>> || return $?; echo mbpb;
    <<&musl-build-binpkg-only>> || return $?; echo mbpo;

    # XXX split to musl/static/x
    <<&musl-build-static-xorg>> || return $?; echo mbsx;
    <<&musl-run-static-xorg-quickpkg>> || return $?; echo mrsxp;

    <<&musl-build-static-package-builder>> || return $?; echo mbspb;
    <<&musl-build-static-binpkg-only>> || return $?; echo mbsbo;

# TODO need to conditionally run the gnu builds for sbcl cross compile
# TODO also need to have a working ghc around, probably stick it in a release

if [ -z $ONLY_STATIC ]; then

# TODO build any new packages
echo musl builder start
docker image inspect ${_tm_pbs} > /dev/null 2>&1
NEED_RESNAP=$?
{ [ -z $RESNAP ] && [ ${NEED_RESNAP} -eq 0 ]; } || builder-resnap || return $?
# FIXME autodetect the --no-build case
[ ! -z $NOBUILD ] || builder-bootstrap || return $?
#[ ! -z $NOBUILD ] || musl-bootstrap-sbcl || return $?
[ ! -z $NOBUILD ] || \
<<&musl-run-build-need-priv>>
[ ! -z $NOBUILD ] || builder-world || return $?
{ [ -z $NOBUILD ] && [ -n $LIVE_REBUILD ]; } && builder-smart-live-rebuild || return $?
{ [ -z $NOBUILD ] && [ -n $LIVE_REBUILD ]; } && builder-smart-also-live-rebuild || return $?
# TODO
# builder-license || return $?

fi

echo musl static builder start
docker image inspect ${_tm_s_pbs} > /dev/null 2>&1
NEED_RESNAP=$?
{ [ -z $RESNAP ] && [ ${NEED_RESNAP} -eq 0 ]; } || static-builder-resnap || return $?
[ ! -z $NOBUILD ] || static-builder-bootstrap || return $?
#[ ! -z $NOBUILD ] || static-musl-bootstrap-sbcl || return $?
# FIXME static-builder-world fails but static-builder-debug running the same works? with @world and _then_ @docker?
[ ! -z $NOBUILD ] || static-builder-world || return $?  # FIXME if this is not run once at the start then something fails above
{ [ -z $NOBUILD ] && [ -n $LIVE_REBUILD ]; } && static-builder-smart-live-rebuild || return $?  # no live builds right now


echo musl nox builder start
docker image inspect ${_tm_n_pbs} > /dev/null 2>&1
NEED_RESNAP=$?
{ [ -z $RESNAP ] && [ ${NEED_RESNAP} -eq 0 ]; } || nox-builder-resnap || return $?
[ ! -z $NOBUILD ] || nox-builder-bootstrap || return $?
# FIXME static-builder-world fails but static-builder-debug running the same works? with @world and _then_ @docker?
[ ! -z $NOBUILD ] || nox-builder-world || return $?  # FIXME if this is not run once at the start then something fails above
# { [ -z $NOBUILD ] && [ -n $LIVE_REBUILD ]; } && nox-builder-smart-live-rebuild || return $?  # no live builds right now

# TODO consider whether we need to rebuild baselayout openrc sgml-common due to config issues with quickpkg

# image builds

echo start static image builds

## sbcl
<<&musl-build-sbcl>> || return $?; echo mbsbcl;
<<&musl-build-sbcl-user>> || return $?; echo mbsbclu;
<<&musl-build-sbcl-stripped>> || return $?; echo mbsbcls;

if [ -n "${ONLY_STATIC}" ]; then
return 0
fi

echo start image builds

## emacs
<<&musl-build-emacs>> || return $?; echo mbe;  # XXX fail on stale profile is very confusing

## kg
<<&musl-build-kg-release>> || return $?; echo mbkgr;
<<&musl-build-kg-release-user>> || return $?; echo mbkgru;
<<&musl-build-kg-dev>> || return $?; echo mbkgd;
<<&musl-build-kg-dev-user>> || return $?; echo mbkgdu;
<<&musl-build-tgbugs-dev>> || return $?; echo mbtgd;
<<&musl-build-tgbugs-dev-user>> || return $?; echo mbtgdu;

## FIXME nox-builder-arb virtual/jdk somehow never got packaged? yeah, need to merege that after inthe openjkd stuff, also no nox-builder-arb
## services
<<&musl-build-blazegraph>> || return $?; echo mbb;

## sbcl
<<&musl-build-sbcl>> || return $?; echo mbsbcl;
<<&musl-build-sbcl-user>> || return $?; echo mbsbclu;
<<&musl-build-sbcl-stripped>> || return $?; echo mbsbcls;

## racket
<<&musl-build-racket>> || return $?; echo mbrac;
<<&musl-build-racket-user>> || return $?; echo mbracu;

## dynapad
# FIXME somehow pulling in builder-arb dev-lang/tk bug it isn't being built? worlds must be misaligned or not included?
# FIXME builder-arb dev-libs/libconfig
# FIXME builder-arb dev-build/cmake also license ???
<<&musl-build-dynapad-base>> || return $?; echo mbdb;
<<&musl-build-dynapad-user>> || return $?; echo mbdbu;
#<<&musl-build-dynapad>> || return $?; # needs to be done by hand

## NIF-ontology
<<&musl-build-protege>> || return $?; echo mbp;
<<&musl-build-NIF-ontology>> || return $?; echo mbno;

## postgresql
<<&musl-build-postgresql>> || return $?; echo mbpsql;

## interlex
<<&musl-build-interlex>> || return $?; echo mbilx;

## sparcron
<<&musl-build-sparcron>> || return $?; echo mbsp;
<<&musl-build-sparcron-user>> || return $?; echo mbspu;
<<&musl-build-sparcron-live>> || return $?; echo mbspl;
<<&musl-build-sparcron-live-user>> || return $?; echo mbsplu;

}
#+end_src

# I am an idiot, the repos image is being build incorrectly and pulls
# in the local images so it overrides. DUH.

#+begin_src screen
<<&musl-run-updated-user>>
#+end_src
** Debug build
*** failures
Sometimes a build will fail.
As long as you aren't using buildkit features such as mount you can
rerun a build command with ~DOCKER_BUILDKIT=0~ prepended which will
keep the intermediate containers around so you can attach to the last
known good layer and try to run things yourself.

Alternately, it may be a better approach to simply truncate the docker
file directly after the last known good step
*** snapshot build failures
=docker= failures can be debugged by doing the following.
#+begin_src bash :eval never
docker container ls -a
#+end_src

Check for non-zero exits and run the following to snapshot them.
#+begin_src bash :eval never
docker commit <hash-of-container-with-non-zero-exit> <debug-image-name>
#+end_src

After that you run the following.
#+begin_src bash :eval never
docker run --rm -it <debug-image-name> /bin/bash
#+end_src

Add any options you need to e.g. mount the right volumes etc.

Using this run command once you have a named image allows you to
repeatedly start from the bad/broken build state to find a fix.

*** binpkg quirks
Sometimes may have to rebuild individual packages when they depend on a specific slot
e.g. python depending on libffi:0/7 instead of libffi:0/8, you have to rebuild python
and produce a new package that works with libffi:0/8, for some reason portage doesn't
do it by itself? Possibly missing =--with-bdeps=y= or something?
** Test
Run all =&test-= blocks in the file. =screen= blocks are converted to bash.
#+begin_src bash
./source.org test
#+end_src
# TODO overview of images with and without tests

Some tests (e.g. for tgbugs/musl:sparcron-user) need access to configs.
You can also set default values by modifying the [[orgstrap]] block.
#+begin_src bash
./source.org test --check-paths \
--path-sparcron-sparcur-config /var/lib/sparc/.config/sparcur/docker-config.yaml \
--path-sparcron-secrets        ~/ni/dev/secrets-sparcron.yaml \
--path-sparcron-gsaro          ~/ni/dev/sparc-curation-8*.json
#+end_src

** Push
To push the latest cycle of images to the default remote run the
following after checking that they work as expected.

# FIXME the --filter=since= isn't quite right, I think it can miss some images we want to push? not entirely sure?
#+begin_src bash :results code
for _image in $(docker image ls \
--filter=reference="tgbugs/musl:*" \
--filter=since='tgbugs/musl:eselect-repo' \
--format "{{.Repository}}:{{.Tag}}" | grep -v snap);
do
    echo docker push "${_image};\\"
done
#+end_src

Other things images we don't push right now.
#+begin_src bash
--filter=reference="tgbugs/repos:*" \
--filter=reference="tgbugs/common:*" \
--filter=reference="tgbugs/gnu:*" \
--filter=reference="tgbugs/docker-profile:*" \
#+end_src

DO NOT PUSH directly to =tgbugs/repos:latest= because there is currently
no way to prevent docker build from pulling an ancient and outdated repo
during bootstrap if one does not already exist.

If for some reason you need to retag so that you can, e.g. push to a
namespace that you control this can be done as follows.
#+name: retag-example
#+begin_src bash
for _image in $(docker image ls \
--filter=reference="tgbugs/*:*" \
--format "{{.Repository}}:{{.Tag}}");
do
    echo docker tag ${_image} $(echo ${_image} | sed 's,^tgbugs/,other/,')
done
#+end_src

TODO consider doing this as general practice to prevent accidental
leaks, that is, use an internal namespace like =tgbugs-build= so that
it is harder to accidentally docker pull a remote image that was not
derived locally.

** Emergency quickpkg
Sometimes you don't want to wait to get to the package builder step
because there is some bug in between.
#+name: &docker-quickpkg
#+begin_src bash
function docker-quickpkg () {
# FIXME TODO pass the image to package
docker run \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-e QUICKPKG_DEFAULT_OPTS="--include-unmodified-config=y --umask=022" \
--rm \
tgbugs/musl:static-xorg \
quickpkg \
${@}
}
#+end_src
** Cleanup
#+name: &docker-cleanup
#+begin_src screen
docker container prune --force
docker volume    prune --force
docker image     prune --force
docker builder   prune --force
#+end_src
** package maint
sometimes you might have a case where a package gets renamed and
the old package keeps getting pulled in the solution is as follows
(example was for setuptools_scm when it was renamed to setuptools-scm)
it might be possible with the new gpkg format to rewrite the embedded metadata
but for now doing a full rebuild is guaranteed safe
# consider eclean packages ??? no, it doesn't do what we want, emaint movebin is much closer
#+begin_src bash
function clean-moved-deps () {
pushd /var/cache/binpkgs
local _pkgname
_pkgname=${1}
echo ${_pkgname}
echo rm $(grep -ral ${_pkgname} --include='*.xpak' --include='*.gpkg.tar')
echo emaint binhost
# make sure it looks right
echo emaint binhost --fix
echo emaint movebin
echo emaint movebin --fix
popd
}
#+end_src
** Building forks
If you need to use this file to build a fork (e.g. for development)
using this file there is only one thing that you will need to modify
in this file and one process you will want to update to push images to
an image host. Everything else can be kept the same without modifying
the internal naming conventions.

For this file modify the =RUN= command for
[[(tgbugs-overlay-fork)][eselect repository add tgbugs-overlay git <my-fork>]]
to point to your fork of https://github.com/tgbugs/tgbugs-overlay.

For the updated process see the [[retag-example][Push retag example block]] above.
* Default variables
:PROPERTIES:
:header-args:elisp: :eval yes
:END:
Default variable values that will eventually have cli overrides.
** path-dockerfiles
#+name: &host-dockerfiles-path
#+begin_src elisp :var value=(or path-dockerfiles)
value
#+end_src

** path-binpkgs-root
#+name: &default-host-binpkgs-root-path
: ~/files/binpkgs

#+name: &host-binpkgs-root-path
#+begin_src elisp :noweb yes :var value=(or path-binpkgs-root)
value
#+end_src

** host-binpkgs-repo-name
NOTE: we will not be making the repo name configurable, it only
appears to be for implementation convenience.
#+name: &host-binpkgs-repo-name
: multi

** host-binpkgs-port
#+name: &host-binpkgs-port
: 8089

** path-distfiles
#+name: &default-host-distfiles-path
: /mnt/str/portage/distfiles

#+name: &host-distfiles-path
#+begin_src elisp :noweb yes :var value=(or path-distfiles)
value
#+end_src

** path-distcc-hosts
#+name: &default-host-distcc-hosts-path
: /etc/distcc/hosts.docker

#+name: &host-distcc-hosts-path
#+begin_src elisp :noweb yes :var value=(or path-distcc-hosts)
value
#+end_src

** path-ssh
#+name: &default-host-ssh-path
: /var/lib/portage/home/.ssh

#+name: &host-ssh-path
#+begin_src elisp :noweb yes :var value=(or path-ssh)
value
#+end_src

** helper-repos
#+name: &helper-repos
: helper-repos
# FIXME grrrr the need to be able to set these computationally
# passed via the command line means that we have to descend into elisp
# or we have to have a oneshot self modifying configuration command
# which is also bad because it breaks version control

# ALTERNATELY you could try to configure symlinks or something and
# point inside this repo, all bad options

# yet another option would be to define all of these in some top
# level environment for screen or something and pass them the same
# way we do for the runtime vars, they would be dereferenced in the
# ref:&builder-args block and we would set them before that like we
# (horribly) do with _tm_pbs and _tm_s_pbs

# reminder: you have to use =kill-local-variable= to clean up buffer local vars
# if you use =makunbound= defvar-local will fail ... and even then there are issues
# so sometimes you just have to kill and reopen the buffer (sigh)

** emerge-jobs
#+name: &default-emerge-jobs
: 4

#+name: &emerge-jobs
#+begin_src elisp :var value=(or emerge-jobs)
value
#+end_src

* Next
** TODO proper process for consistent behavior in nearly all conditions
for the builder track it seems that sequential updates starting from
stage3 and running uDN world or similar for each profile are likely to
be the simplest to understand and the least likely to fall into weird
stage3 vs portage mismatch issues

there are some obstacles due to bugs/current behavior in portage which
make building bdeps only a bit tricky (e.g. when switching profiles)
if we want to run the normal build process with binpkgonly to avoid
accumulating many installed packages in the image

1. builder for stage3 with local-portage-snap update world, a tiny bit of rework here
2. git and eselect repo
3. repos
4. profiles
5. iterate through profiles and build/update everything

then to compose images start from ... ??? for the binpkgonly
** TODO meta packages instead of world files
Proper factoring for this system suggests that we should probably be
maintaining these world files as meta ebuilds in an overlay so that
the process in this file can remain mostly static and we just name the
meta package that we want to build for a particular use case. It
should be much easier to maintain and reduce the overall complexity of
what is going on in this file.
** TODO live builder
since we have the world list it likely makes sense to do an hourly
poll of the github repo or something and pull down the latest changes
check for changes to any of the installed packages and build the new
packages so that we don't have to wait to detect errors, the tool
chains are more than robust enough to support these kinds of use cases
basically put the builder to work during the week, and taking the
subsets for particular use cases is hardly an issue at that point the
living image does start to get bloated though, so the weekly rebuild
can help, this should cut down big time on issues with e.g. rust
taking stupid amounts of time to build, drive it all of the sparse (in
time) changes that maintainers make
** TODO podman
:PROPERTIES:
:CREATED:  [2022-08-26 Fri 16:09]
:END:
better than dind
need to investigate LVM because podman requires it and I don't use it on my systems
https://wiki.gentoo.org/wiki/LVM
** TODO ebuilds changing behind the scenes
:PROPERTIES:
:CREATED:  [2022-03-21 Mon 20:50]
:END:
so it turns out that it is possible to change an ebuild, rebuild the package
and .. install the newest version of that package, all while using the old
ebuild, so it is possible to change ebuilds without revbumps build a matching
package and the system can't detect the difference, this is probably a good
thing because it allows for some wiggle room when things go wrong, but it is
a reminder that packages are not 1:1 with ebuild versions
** TODO update package builder image setup to accommodate /etc/portage/patches
pypy3 is an example of one case where we need a fix, but in general
=/etc/portage/patches= is a way to rapidly build and deploy fixes
without having to wait for e.g. a full pull request cycle to finish.
** DONE catch errors in profile early
** TODO dind or similar for top level ops
Docker is not homogeneous with regard to nesting containers since the
way that we use it is a bit outside the usual use case (and because
docker is a hack and true nesting reveals this by violating a whole
bunch of assumptions that are baked into the implementation).

As a result, a hack is required to be able to fake nesting. In this
case the simplest approach seems to be to make the ur-host's docker
process accessible to the top level ops container. Not truly
homogeneous, but better than nothing. This is done by mounting the
socket for the docker daemon when you run the top level build image.

Since this is a build process security considerations are identical
for the true host and the top level image. If we weren't running in
the top level image we would be running on the true host directly so
sandboxing is irrelevant.

An example approach would be to run something like the following.
#+begin_src bash
docker run -v /var/run/docker.sock:/var/run/docker.sock tgbugs/musl:docker
#+end_src

** DONE a better way
The primary issue here is that it really is not safe to compose after
merge because the power and flexibility of portage happen before
merge, and are quite state dependent after the fact. The key then is
to be able to create images that do compose well, and the only at
the very end materialize them by installing all the packages at once.

The problem is that you give up the utility of the docker layers, but
if we are installing binary packages that have been built on a
separate system then we know that we won't encounter build errors.

The final obstacle to full composability in this way is the issue of
incompatible use flags, but I think it is safe to say that it is not
really possible to solve that problem.

This consideration suggests that the layers of docker images, while
useful, are fundamentally at odds with composability when there are
files inside images that track state (e.g. =/var/lib/portage/world=).

** DONE condense use flags
At the moment we keep use flags with packages and try to keep them
mostly orthogonal to each other. However, at a certain point it is
going to be easier to maintain a single shared use flag image that
will be synchronized across all images. Granular control is nice from
a learning and minimal specification point of view, but from an
engineering an maintenance point of view it is vastly easier easier to
maintain a single shared use flag image that will be synchronized
across all images. Granular control is nice from a learning and
minimal specification point of view, but from an engineering an
maintenance point of view it is simpler to unify the individual image
environments into a single file.
** DONE create an image to build packages
Rebuilding images is wasteful when nothing has changed, and packages
and install properly to maintain the correct state of the image. While
=COPY --from= works, it mangles things like =/var/lib/portage/world=,
and if use flags were changed on a dependency by another source image
then unusual and unexpected errors could occur. This is another reason
to move to manage use flags one or two images, one image for cases
where X11 is not needed, and another where it is.

In fact, I'm fairly certain that having a shared use flag environment
is necessary for it to be possible to safely compose packages and
images. Composition across environments requires something like nix
where each package carries around its own environment. It might be
possible to do better than this by allowing composition in cases where
the environments are compatible, but that would still require
computation at composition time, you can't just layer images an expect
things to work.

alternately mount =/var/cache/binpkgs= and then run quickpkg or
something devious like that
** TODO separate user image
Should be able to =COPY --from=tgbugs/common:user= across all images.
build the user image from a base that has next to nothing in it
add the user and group to the system and then copy that minimal
user stuff in, most of the time there isn't any fancy installation
that needed to be done, and we could just copy the user directory
when building from scratch
* docker-profile
** base
The right way to do this is to create two custom profiles on top of musl-hardened.

https://wiki.gentoo.org/wiki/Profile_(Portage)#custom

Modifications to use flags and other system settings and
configurations that are easier to keep in a single location.
# FIXME this may need to be versioned, or we just force rebuild on all
# the images from scratch which we often have to do anyway, though some
# packages may not be affect by profile changes
*** build
#+name: &build-profile-base
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:base \
--file docker-profile/base/Dockerfile .
#+end_src

*** file
# FIXME split these out so they go in their own images and don't cause global rebuilds
# FIXME some of these patches e.g. those from alpine often are musl specific
#+name: &profile-adds
#+begin_src dockerfile
# we don't put this in var/db/repos because repos is managed via tgbugs/repos:latest
ARG bp=docker-profile/base/

ADD ${bp}docker-profile                          var/db/docker-profile
ADD ${bp}docker-profile.conf                     etc/portage/repos.conf/docker-profile.conf
ADD ${bp}binrepos-multi.conf                     etc/portage/binrepos.conf/multi.conf
ADD ${bp}package.accept_keywords                 etc/portage/package.accept_keywords/profile
ADD ${bp}package.accept_keywords.haskell.gentoo  etc/portage/package.accept_keywords/profile.haskell
ADD ${bp}package.mask                            etc/portage/package.mask/profile
ADD ${bp}package.unmask                          etc/portage/package.unmask/profile
ADD ${bp}package.use                             etc/portage/package.use/profile
ADD ${bp}emacs.env                               etc/portage/env/app-editors/emacs
ADD ${bp}vim-core.env                            etc/portage/env/app-editors/vim-core
# FIXME could symlink instead of duping maybe?
ADD ${bp}vim-core.env                            etc/portage/env/app-editors/vim
ADD ${bp}vim-core.env                            etc/portage/env/app-editors/gvim
ADD ${bp}erlang.env                              etc/portage/env/dev-lang/erlang
ADD ${bp}no-site-docs.env                        etc/portage/env/dev-python/pipenv
ADD ${bp}no-site-docs.env                        etc/portage/env/dev-python/google-auth-oauthlib
ADD ${bp}rabbitmq.env                            etc/portage/env/net-misc/rabbitmq-server
ADD ${bp}pandoc-lua-engine.env                   etc/portage/env/dev-haskell/pandoc-lua-engine
ADD ${bp}ghc-rtsopts.env                         etc/portage/env/ghc-rtsopts
ADD ${bp}no-distcc.env                           etc/portage/env/no-distcc
ADD ${bp}package.env                             etc/portage/package.env/profile
# general pataches
ADD ${bp}portage-onlydeps-license.patch          etc/portage/patches/sys-apps/portage/portage-onlydeps-license.patch
ADD ${bp}rdflib-float-nonorm.patch               etc/portage/patches/dev-python/rdflib-7.1.1/rdflib-float-nonorm.patch
ADD ${bp}cloudpickle-pypy3.11.patch              etc/portage/patches/dev-python/cloudpickle-3.1.1/cloudpickle-pypy3.11.patch
RUN \
   ln -s rdflib-7.1.1 etc/portage/patches/dev-python/rdflib-7.1.3 \
;  ln -s rdflib-7.1.1 etc/portage/patches/dev-python/rdflib-7.1.4

# ghc memory issue workaround
RUN \
echo GHCRTS=-M$(( $(cat /proc/meminfo | grep MemTotal | awk '{ print $2 }') / ( <<&emerge-jobs()>> * 1024 )))M > etc/portage/env/ghc-rts-m
RUN \
sh -c '\
<<&populate-dev-haskell-env>>
'
#+end_src

#+name: &populate-dev-haskell-env
#+begin_src bash :eval never
set -e -o pipefail; \
cd etc/portage/env; \
cpvs=$(cat ../package.accept_keywords/profile.haskell <(cat ../package.unmask/profile | grep haskell | awk -F':' '{ print $1 }') <(echo dev-haskell/dot2graphml)); \
for cpv in ${cpvs}; do \
if [ -f ${cpv} ]; then \
    cat ghc-rtsopts >> ${cpv}; \
else \
    ln -s ../ghc-rtsopts ${cpv}; \
fi \
done; \
#+end_src

# has been fixed in a better way by upstream
# ADD ${bp}patchelf-musl-no-dt-mips-xhash.patch    etc/portage/patches/dev-util/patchelf/patchelf-musl-no-dt-mips-xhash.patch
# bad patch that enables things to build that will cause fatal runtime errors
# ADD ${bp}pypy3-trashcan.patch                    etc/portage/patches/dev-python/pypy3_x-exe/trashcan.patch

#+begin_src dockerfile :tangle ./docker-profile/base/Dockerfile
FROM docker.io/library/busybox:latest AS builder

WORKDIR /build

<<&profile-adds>>

FROM scratch

WORKDIR /
COPY --from=builder /build /
#+end_src

*** etc
**** repos.conf
#+begin_src conf :tangle ./docker-profile/base/docker-profile.conf
[docker-profile]
location = /var/db/docker-profile
#+end_src
**** binrepos.conf
=tgbugs-multi-sigh= needed to work around linux and macos/windows-nt
networking differences without forcing the docker sources to be
different between build platforms (which would utterly defeat the
point of trying to use docker for reproducibility and robustness).
#+begin_src conf :tangle ./docker-profile/base/binrepos-multi.conf
[tgbugs-multi]
priority = 100
sync-uri = http://local.binhost:<<&host-binpkgs-port()>>/multi

[tgbugs-multi-sigh]
priority = 99
sync-uri = http://host.docker.internal:<<&host-binpkgs-port()>>/multi
#+end_src
**** package.accept_keywords
#+begin_src conf :tangle ./docker-profile/base/package.accept_keywords
app-text/pandoc-cli::gentoo
dev-python/*::tgbugs-overlay
dev-racket/*::racket-overlay
dev-scheme/racket::tgbugs-overlay
dev-haskell/*::tgbugs-overlay **
dev-haskell/*::haskell
dev-lang/ghc::gentoo
app-admin/haskell-updater::gentoo
dev-util/shellcheck::gentoo
sys-apps/racket-where::racket-overlay
#+end_src

We only use the =::haskell= overlay for the graphviz dependencies.
For shellcheck and pandoc we stick with =::gentoo= and control that
by specifying the exact packages here. Note that a certain amount of
vigilance is needed to catch cases where a missing accept keyword on
a package will prevent any and all upgrades. One way to catch such
cases is to add =dev-haskell/*::gentoo= to accept_keywords and see
whether there are lurking changes.
#+begin_src conf :tangle ./docker-profile/base/package.accept_keywords.haskell.gentoo
dev-haskell/aeson
dev-haskell/aeson-pretty
dev-haskell/alex
dev-haskell/ansi-terminal
dev-haskell/ansi-terminal-types
dev-haskell/appar
dev-haskell/asn1-encoding
dev-haskell/asn1-parse
dev-haskell/asn1-types
dev-haskell/assoc
dev-haskell/async
dev-haskell/attoparsec
dev-haskell/attoparsec-aeson
dev-haskell/attoparsec-iso8601
dev-haskell/auto-update
dev-haskell/base-compat
dev-haskell/base-compat-batteries
dev-haskell/base-orphans
dev-haskell/base-unicode-symbols
dev-haskell/base16-bytestring
dev-haskell/base64
dev-haskell/base64-bytestring
dev-haskell/basement
dev-haskell/bifunctors
dev-haskell/bitvec
dev-haskell/blaze-builder
dev-haskell/blaze-html
dev-haskell/blaze-markup
dev-haskell/boring
dev-haskell/bsb-http-chunked
dev-haskell/byteorder
dev-haskell/cabal
dev-haskell/cabal-doctest
dev-haskell/call-stack
dev-haskell/case-insensitive
dev-haskell/cassava
dev-haskell/cereal
dev-haskell/character-ps
dev-haskell/citeproc
dev-haskell/cmdargs
dev-haskell/colour
dev-haskell/commonmark
dev-haskell/commonmark-extensions
dev-haskell/commonmark-pandoc
dev-haskell/comonad
dev-haskell/conduit
dev-haskell/conduit-extra
dev-haskell/constraints
dev-haskell/contravariant
dev-haskell/cookie
dev-haskell/crypton
dev-haskell/crypton-connection
dev-haskell/crypton-x509
dev-haskell/crypton-x509-store
dev-haskell/crypton-x509-system
dev-haskell/crypton-x509-validation
dev-haskell/cryptonite
dev-haskell/data-array-byte
dev-haskell/data-default
dev-haskell/data-default-class
dev-haskell/data-default-instances-containers
dev-haskell/data-default-instances-dlist
dev-haskell/data-default-instances-old-locale
dev-haskell/data-fix
dev-haskell/dec
dev-haskell/diff
dev-haskell/digest
dev-haskell/digits
dev-haskell/distributive
dev-haskell/dlist
dev-haskell/doclayout
dev-haskell/doctemplates
dev-haskell/easy-file
dev-haskell/emojis
dev-haskell/fail
dev-haskell/fast-logger
dev-haskell/fgl
dev-haskell/file-embed
dev-haskell/foldable1-classes-compat
dev-haskell/generically
dev-haskell/glob
dev-haskell/gridtables
dev-haskell/haddock-library
dev-haskell/happy
dev-haskell/hashable
dev-haskell/haskell-lexer
dev-haskell/hourglass
dev-haskell/hslua
dev-haskell/hslua-aeson
dev-haskell/hslua-classes
dev-haskell/hslua-cli
dev-haskell/hslua-core
dev-haskell/hslua-list
dev-haskell/hslua-marshalling
dev-haskell/hslua-module-doclayout
dev-haskell/hslua-module-path
dev-haskell/hslua-module-system
dev-haskell/hslua-module-text
dev-haskell/hslua-module-version
dev-haskell/hslua-module-zip
dev-haskell/hslua-objectorientation
dev-haskell/hslua-packaging
dev-haskell/hslua-repl
dev-haskell/hslua-typing
dev-haskell/http-api-data
dev-haskell/http-client
dev-haskell/http-client-tls
dev-haskell/http-date
dev-haskell/http-media
dev-haskell/http-types
dev-haskell/http2
dev-haskell/hunit
dev-haskell/indexed-traversable
dev-haskell/indexed-traversable-instances
dev-haskell/integer-conversion
dev-haskell/integer-logarithms
dev-haskell/iproute
dev-haskell/ipynb
dev-haskell/isocline
dev-haskell/jira-wiki-markup
dev-haskell/juicypixels
dev-haskell/libyaml
dev-haskell/lpeg
dev-haskell/lua
dev-haskell/memory
dev-haskell/mime-types
dev-haskell/mmorph
dev-haskell/monad-control
dev-haskell/mono-traversable
dev-haskell/mtl
dev-haskell/network
dev-haskell/network-byte-order
dev-haskell/network-uri
dev-haskell/old-locale
dev-haskell/old-time
dev-haskell/onetuple
dev-haskell/only
dev-haskell/optparse-applicative
dev-haskell/ordered-containers
dev-haskell/os-string
dev-haskell/pandoc
dev-haskell/pandoc-lua-engine
dev-haskell/pandoc-lua-marshal
dev-haskell/pandoc-server
dev-haskell/pandoc-types
dev-haskell/parsec
dev-haskell/pem
dev-haskell/pretty-show
dev-haskell/prettyprinter
dev-haskell/prettyprinter-ansi-terminal
dev-haskell/primitive
dev-haskell/psqueues
dev-haskell/quickcheck
dev-haskell/random
dev-haskell/recv
dev-haskell/regex-base
dev-haskell/regex-tdfa
dev-haskell/resourcet
dev-haskell/safe
dev-haskell/safe-exceptions
dev-haskell/scientific
dev-haskell/semialign
dev-haskell/semigroupoids
dev-haskell/semigroups
dev-haskell/servant
dev-haskell/servant-server
dev-haskell/sha
dev-haskell/simple-sendfile
dev-haskell/singleton-bool
dev-haskell/skylighting
dev-haskell/skylighting-core
dev-haskell/skylighting-format-ansi
dev-haskell/skylighting-format-blaze-html
dev-haskell/skylighting-format-context
dev-haskell/skylighting-format-latex
dev-haskell/socks
dev-haskell/some
dev-haskell/sop-core
dev-haskell/split
dev-haskell/splitmix
dev-haskell/statevar
dev-haskell/stm
dev-haskell/streaming-commons
dev-haskell/strict
dev-haskell/string-conversions
dev-haskell/syb
dev-haskell/tagged
dev-haskell/tagsoup
dev-haskell/temporary
dev-haskell/texmath
dev-haskell/text
dev-haskell/text-conversions
dev-haskell/text-icu
dev-haskell/text-iso8601
dev-haskell/text-short
dev-haskell/th-abstraction
dev-haskell/th-compat
dev-haskell/th-lift
dev-haskell/th-lift-instances
dev-haskell/these
dev-haskell/time-compat
dev-haskell/time-manager
dev-haskell/tls
dev-haskell/toml-parser
dev-haskell/transformers-base
dev-haskell/transformers-compat
dev-haskell/type-equality
dev-haskell/typed-process
dev-haskell/typst
dev-haskell/typst-symbols
dev-haskell/unicode-collation
dev-haskell/unicode-data
dev-haskell/unicode-transforms
dev-haskell/uniplate
dev-haskell/unix-compat
dev-haskell/unix-time
dev-haskell/unliftio
dev-haskell/unliftio-core
dev-haskell/unordered-containers
dev-haskell/utf8-string
dev-haskell/uuid-types
dev-haskell/vault
dev-haskell/vector
dev-haskell/vector-algorithms
dev-haskell/vector-stream
dev-haskell/wai
dev-haskell/wai-app-static
dev-haskell/wai-cors
dev-haskell/wai-extra
dev-haskell/wai-logger
dev-haskell/warp
dev-haskell/witherable
dev-haskell/word8
dev-haskell/xml
dev-haskell/xml-conduit
dev-haskell/xml-types
dev-haskell/yaml
dev-haskell/zip-archive
dev-haskell/zlib
#+end_src
# and they say haskell is a safe language

**** package.mask
#+begin_src conf :tangle ./docker-profile/base/package.mask
dev-scheme/racket::gentoo
dev-haskell/*::haskell
dev-java/openjdk-bin  # libc linking issues, must compile openjdk
<dev-lang/rust-1.70.0::musl  # stale
dev-lang/rust-bin  # prefer our own
<virtual/jdk-17
<virtual/jre-17
#+end_src
**** package.unmask
#+begin_src conf :tangle ./docker-profile/base/package.unmask
dev-haskell/colour::haskell
dev-haskell/fgl::haskell
dev-haskell/graphviz::haskell
dev-haskell/hxt-charproperties::haskell
dev-haskell/hxt-regex-xmlschema::haskell
dev-haskell/hxt-unicode::haskell
dev-haskell/hxt::haskell
dev-haskell/polyparse::haskell
dev-haskell/temporary::haskell
dev-haskell/wl-pprint-text::haskell
#+end_src
**** package.use
#+begin_src conf :tangle ./docker-profile/base/package.use
dev-scheme/racket::tgbugs-overlay cs bc cgc
#+end_src
**** env
***** no distcc
# TODO MAKEOPTS_LOCAL
# FIXME "${FEATURES} -distcc" vs "-distcc" behavior?
# I think they stack without variables with priority going global package runtime environment
#+begin_src conf :tangle ./docker-profile/base/no-distcc.env
FEATURES="-distcc"
#+end_src

***** ghc-rtsopts
Even though this takes more files it keeps the system specific part small.
FIXME TODO keep this in sync with ghc-package.eclass automatically
#+begin_src ebuild :tangle ./docker-profile/base/ghc-rtsopts.env
ghc-make-args() {
    local ghc_make_args=();
    if ghc-supports-smp && ghc-supports-parallel-make; then
        echo "-j$(makeopts_jobs) -rtsopts=all +RTS -A256M -qb0 -RTS";
        ghc_make_args=();
    fi;
    echo "${ghc_make_args[@]}";
}
#+end_src

***** no site-packages/docs
=dev-python/{pipenv,google-auth-oauthlib}=
hack to remove packages installing =site-packages/docs= nonsense
https://github.com/pypa/pipenv/issues/5937
#+begin_src ebuild :tangle ./docker-profile/base/no-site-docs.env
post_src_install() { find "${D}" -type d -wholename '*/site-packages/docs' -exec rm -r {} \;; }
#+end_src
***** app-editors/emacs
#+begin_src conf :tangle ./docker-profile/base/emacs.env
NATIVE_FULL_AOT=1
#+end_src
***** app-editors/vim-core
Also needed for vim and gvim.
#+begin_src ebuild :tangle ./docker-profile/base/vim-core.env
post_src_prepare() {
	# Fix bug #908961
	if use elibc_musl; then
		# Musl's locale support is not great, it doesn't support the cp932
		# charset.
		sed -e "/ja.sjis/d" -i "${S}"/src/po/Make_all.mak
	fi
}
#+end_src
***** dev-lang/erlang
https://bugs.gentoo.org/857099
https://github.com/OpenRC/openrc/blob/master/service-script-guide.md#be-wary-of-need-net-dependencies
#+begin_src ebuild :tangle ./docker-profile/base/erlang.env
post_src_install() { sed -i '/need/d' "${D}"/etc/init.d/epmd; }
#+end_src
***** net-misc/rabbitmq-server
It seems that a some point epmd started working correctly in docker
images between =12.3.1= and =12.3.2.2=, therefore rabbitmq can't start
its own empd and fails to connect. This removes the depend statement
in the init file that pulls in system epmd, which if started will
cause rabbitmq to fail to start. The correct solution is to figure out
how to correctly configure rabbitmq, but for now this should restore
the old behavior.
#+begin_src ebuild :tangle ./docker-profile/base/rabbitmq.env
post_src_install() { sed -i '/need/d' "${D}"/etc/init.d/rabbitmq; }
#+end_src
***** dev-haskell/pandoc-lua-engine
Workaround for https://bugs.gentoo.org/916785.
Not clear what is actually going wrong, but ensuring that
the ghc package cache is actually up to date solves the problem.
#+begin_src ebuild :tangle ./docker-profile/base/pandoc-lua-engine.env
pre_pkg_setup() {
	ghc-recache-db
}
#+end_src

**** package.env
# some continusing nonsense, a manual build in builder-debug is somehow different enough that it works?
# XXX manually setting FEATURES=-distcc worked, but it seems that stacking features in make.conf doesn't?
#+begin_src conf :tangle ./docker-profile/base/package.env
dev-python/pypy3 no-distcc
dev-build/cmake no-distcc
dev-util/colm no-distcc
# cases where the issue disappears in builder-debug
dev-util/lapack no-distcc
dev-python/numpy no-distcc
dev-scheme/racket no-distcc  # somehow cc to compile zuo is broken and produces a segfault
dev-haskell/* ghc-rts-m
#+end_src

**** patches
***** dev-python/cloudpickle
See https://github.com/cloudpipe/cloudpickle/pull/566
#+begin_src diff :tangle ./docker-profile/base/cloudpickle-pypy3.11.patch
diff --git a/cloudpickle/cloudpickle.py b/cloudpickle/cloudpickle.py
index 4d532e5..34ea690 100644
--- a/cloudpickle/cloudpickle.py
+++ b/cloudpickle/cloudpickle.py
@@ -837,29 +837,51 @@ def _code_reduce(obj):
     co_varnames = tuple(name for name in obj.co_varnames)
     co_freevars = tuple(name for name in obj.co_freevars)
     co_cellvars = tuple(name for name in obj.co_cellvars)
-    if hasattr(obj, "co_exceptiontable"):
+    if hasattr(obj, "co_qualname"):
         # Python 3.11 and later: there are some new attributes
         # related to the enhanced exceptions.
-        args = (
-            obj.co_argcount,
-            obj.co_posonlyargcount,
-            obj.co_kwonlyargcount,
-            obj.co_nlocals,
-            obj.co_stacksize,
-            obj.co_flags,
-            obj.co_code,
-            obj.co_consts,
-            co_names,
-            co_varnames,
-            obj.co_filename,
-            co_name,
-            obj.co_qualname,
-            obj.co_firstlineno,
-            obj.co_linetable,
-            obj.co_exceptiontable,
-            co_freevars,
-            co_cellvars,
-        )
+        if hasattr(obj, "co_exceptiontable"):
+            args = (
+                obj.co_argcount,
+                obj.co_posonlyargcount,
+                obj.co_kwonlyargcount,
+                obj.co_nlocals,
+                obj.co_stacksize,
+                obj.co_flags,
+                obj.co_code,
+                obj.co_consts,
+                co_names,
+                co_varnames,
+                obj.co_filename,
+                co_name,
+                obj.co_qualname,
+                obj.co_firstlineno,
+                obj.co_linetable,
+                obj.co_exceptiontable,
+                co_freevars,
+                co_cellvars,
+            )
+        else:
+            # pypy 3.11 7.3.19+ has co_qualname but not co_exceptiontable
+            args = (
+                obj.co_argcount,
+                obj.co_posonlyargcount,
+                obj.co_kwonlyargcount,
+                obj.co_nlocals,
+                obj.co_stacksize,
+                obj.co_flags,
+                obj.co_code,
+                obj.co_consts,
+                co_names,
+                co_varnames,
+                obj.co_filename,
+                co_name,
+                obj.co_qualname,
+                obj.co_firstlineno,
+                obj.co_linetable,
+                co_freevars,
+                co_cellvars,
+            )
     elif hasattr(obj, "co_linetable"):
         # Python 3.10 and later: obj.co_lnotab is deprecated and constructor
         # expects obj.co_linetable instead.
#+end_src

***** dev-python/rdflib
See https://github.com/RDFLib/rdflib/pull/3020
#+begin_src diff :tangle ./docker-profile/base/rdflib-float-nonorm.patch
diff --git a/rdflib/plugins/parsers/notation3.py b/rdflib/plugins/parsers/notation3.py
index da71405e..9bc83daa 100755
--- a/rdflib/plugins/parsers/notation3.py
+++ b/rdflib/plugins/parsers/notation3.py
@@ -380,6 +380,10 @@ interesting = re.compile(r"""[\\\r\n\"\']""")
 langcode = re.compile(r"[a-zA-Z0-9]+(-[a-zA-Z0-9]+)*")
 
 
+class sfloat(str):
+    """ don't normalize raw XSD.double string representation """
+
+
 class SinkParser:
     def __init__(
         self,
@@ -1528,7 +1532,7 @@ class SinkParser:
                 m = exponent_syntax.match(argstr, i)
                 if m:
                     j = m.end()
-                    res.append(float(argstr[i:j]))
+                    res.append(sfloat(argstr[i:j]))
                     return j
 
                 m = decimal_syntax.match(argstr, i)
@@ -1911,7 +1915,7 @@ class RDFSink:
     def normalise(
         self,
         f: Optional[Formula],
-        n: Union[Tuple[int, str], bool, int, Decimal, float, _AnyT],
+        n: Union[Tuple[int, str], bool, int, Decimal, sfloat, _AnyT],
     ) -> Union[URIRef, Literal, BNode, _AnyT]:
         if isinstance(n, tuple):
             return URIRef(str(n[1]))
@@ -1931,7 +1935,7 @@ class RDFSink:
             s = Literal(value, datatype=DECIMAL_DATATYPE)
             return s
 
-        if isinstance(n, float):
+        if isinstance(n, sfloat):
             s = Literal(str(n), datatype=DOUBLE_DATATYPE)
             return s
 
-- 
2.45.3
#+end_src

***** sys-apps/portage
#+begin_src diff :tangle ./docker-profile/base/portage-onlydeps-license.patch
From 3ead5d1a1e35baf5d26716b02c6e44d98b2db14f Mon Sep 17 00:00:00 2001
From: Tom Gillespie <tgbugs@gmail.com>
Date: Sun, 3 Sep 2023 20:57:32 -0700
Subject: [PATCH] emerge: allow --onlydeps merge if root package(s) masked by
 license

This commit makes it possible to merge dependencies for packages that
are masked by license since the masked package is not actually going
to be installed.

Signed-off-by: Tom Gillespie <tgbugs@gmail.com>
---
 lib/_emerge/depgraph.py | 22 +++++++++++++++++-----
 1 file changed, 17 insertions(+), 5 deletions(-)

diff --git a/lib/_emerge/depgraph.py b/lib/_emerge/depgraph.py
index 01a49bcb5..654801db0 100644
--- a/lib/_emerge/depgraph.py
+++ b/lib/_emerge/depgraph.py
@@ -5250,7 +5250,13 @@ class depgraph:
         # set below is reserved for cases where there are *zero* other
         # problems. For reference, see backtrack_depgraph, where it skips the
         # get_best_run() call when success_without_autounmask is True.
-        if self._have_autounmask_changes():
+
+        if onlydeps and self._dynamic_config._needed_license_changes:
+            # needed license changes should only be fatal for packages that
+            # would actually be installed
+            return True, myfavorites
+
+        elif self._have_autounmask_changes():
             # We failed if the user needs to change the configuration
             self._dynamic_config._success_without_autounmask = True
             if (
@@ -6998,7 +7004,7 @@ class depgraph:
 
         return pkg, existing
 
-    def _pkg_visibility_check(self, pkg, autounmask_level=None, trust_graph=True):
+    def _pkg_visibility_check(self, pkg, autounmask_level=None, trust_graph=True, onlydeps=False):
         if pkg.visible:
             return True
 
@@ -7037,7 +7043,11 @@ class depgraph:
             elif hint.key == "p_mask":
                 masked_by_p_mask = True
             elif hint.key == "license":
-                missing_licenses = hint.value
+                if not onlydeps:
+                    # onlydeps should only be set to True for top level packages
+                    # don't block an onlydeps merge when the package blocked it
+                    # would not be merged
+                    missing_licenses = hint.value
             else:
                 masked_by_something_else = True
 
@@ -7073,7 +7083,7 @@ class depgraph:
                 and not autounmask_level.allow_missing_keywords
             )
             or (masked_by_p_mask and not autounmask_level.allow_unmasks)
-            or (missing_licenses and not autounmask_level.allow_license_changes)
+            or (missing_licenses and not autounmask_level.allow_license_changes and not onlydeps)
         ):
             # We are not allowed to do the needed changes.
             return False
@@ -7402,7 +7412,9 @@ class depgraph:
                         # _dep_check_composite_db, in order to prevent
                         # incorrect choices in || deps like bug #351828.
 
-                        if not self._pkg_visibility_check(pkg, autounmask_level):
+                        # We pass onlydeps here so that masks on packages
+                        # that would not be installed are not fatal
+                        if not self._pkg_visibility_check(pkg, autounmask_level, onlydeps=onlydeps):
                             continue
 
                         # Enable upgrade or downgrade to a version
-- 
2.41.0

#+end_src

*** profiles
#+begin_src conf :tangle ./docker-profile/base/docker-profile/metadata/layout.conf
masters = gentoo
profile-formats = portage-2
#+end_src

#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/repo_name
docker-profile
#+end_src

# NOTE that tgbugs/musl/x is listed here but not populated until later
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/profiles.desc
amd64 tgbugs                 dev
amd64 tgbugs/x               dev
amd64 tgbugs/nox             dev
amd64 tgbugs/pypy3           dev
amd64 tgbugs/static          dev

amd64 tgbugs/gnu             dev
amd64 tgbugs/gnu/x           dev
amd64 tgbugs/gnu/nox         dev

amd64 tgbugs/musl            dev
amd64 tgbugs/musl/x          dev
amd64 tgbugs/musl/nox        dev

amd64 tgbugs/musl/static     dev
amd64 tgbugs/musl/static/x   dev
amd64 tgbugs/musl/static/nox dev

amd64 tgbugs/musl/pypy3      dev
amd64 tgbugs/musl/pypy3/x    dev
amd64 tgbugs/musl/pypy3/nox  dev
#+end_src
**** packages
Useful to keep these out of file:/var/lib/portage/world so that individual
docker files can just =ADD= their world file and then =emerge @world=. It
also makes it much easier for the package builder to operate based on world files.
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/packages
*dev-vcs/git
*app-eselect/eselect-repository
*media-fonts/dejavu
*media-libs/fontconfig
*media-libs/freetype
#+end_src
**** make.defaults
# old, we use INSTALL_MASK for simplicity
#+begin_comment
See warning about https://wiki.gentoo.org/wiki/Localization/Guide#LINGUAS.
We are safe here because this base profile is shared between all our
systems and because we do not redistribute the binary packages.

We restrict =LINGUAS= here to reduce the size of the images that are
produced.  Larger images with localization enabled can be produced by
removing the restriction, but are not included by default. This
approach is likely better than using =INSTALL_MASK=.
#+end_comment

# USE="-doc"
# LINGUAS="en"
# for some reason empty video cards does not actually disable all the flags

# NOTE: the hardened profile sets USE=-cli and USE=-jit and some other stuff
# that changes behavior [[/usr/portage/profiles/features/hardened/make.defaults]]

Normally we don't set =USE== in make.conf, however there is no way to set
global use flags in a profile without doing so.
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/make.defaults
INSTALL_MASK="${INSTALL_MASK}
/usr/share/locale
-/usr/share/locale/en
-/usr/share/locale/en@boldquot
-/usr/share/locale/en@quot
-/usr/share/locale/en@shaw
-/usr/share/locale/en_US"

## binary package settings

BINPKG_FORMAT="gpkg"
BINPKG_COMPRESS="zstd"
BINPKG_COMPRESS_FLAGS_ZSTD="--ultra -22"

# this overrides any parent profile that sets EMERGE_DEFAULT_OPTS
# according to the wiki --jobs MUST be set here and explains why
# all the packages try to merge at the same time, not inheriting
# from existing EMERGE_DEFAULT_OPTS prevents duplicate entires
# if I had to guess I would say that setting this in a profile is
# probably a bad idea, we'll do it for now for consistency
EMERGE_DEFAULT_OPTS="--jobs <<&emerge-jobs()>> --binpkg-respect-use=y"

# icu is needed due to musl collation issues
# jemalloc can improve performance re issues with musl allocator
USE="${USE} icu jemalloc"

USE="${USE} -gstreamer"

VIDEO_CARDS="-*"

# ensure that packages are readable by other users via umask 022
# use unmodified config in case a config file is modified, configs
# should never wind up modified when using package builder images
# see https://bugs.gentoo.org/307455 for more
# FIXME XXX current issues include
# /etc/hosts -> sys-apps/baselayout
# /etc/rc.conf -> sys-apps/openrc
# /etc/sgml/catalog -> app-text/sgml-common
# which seem to have been modified by other merges
QUICKPKG_DEFAULT_OPTS="--include-unmodified-config=y --umask=022"

## account id mappings

ACCT_GROUP_BLAZEGRAPH_ID=834
ACCT_USER_BLAZEGRAPH_ID="${ACCT_GROUP_BLAZEGRAPH_ID}"

ACCT_GROUP_SCIGRAPH_ID=835
ACCT_USER_SCIGRAPH_ID="${ACCT_GROUP_SCIGRAPH_ID}"

ACCT_GROUP_SPARC_ID=836
ACCT_USER_SPARC_ID="${ACCT_GROUP_SPARC_ID}"

ACCT_GROUP_PROTCUR_ID=837
ACCT_USER_PROTCUR_ID="${ACCT_GROUP_PROTCUR_ID}"

ACCT_GROUP_SCIBOT_ID=838
ACCT_USER_SCIBOT_ID="${ACCT_GROUP_SCIBOT_ID}"

ACCT_GROUP_INTERLEX_ID=839
ACCT_USER_INTERLEX_ID="${ACCT_GROUP_INTERLEX_ID}"

ACCT_GROUP_NIFSTD_TOOLS_ID=840
ACCT_USER_NIFSTD_TOOLS_ID="${ACCT_GROUP_NIFSTD_TOOLS_ID}"

ACCT_GROUP_METABASE_ID=841
ACCT_USER_METABASE_ID="${ACCT_GROUP_METABASE_ID}"

## git-r3 settings

# reduce clone times for dev repos for live ebuilds
EGIT_CLONE_TYPE=mirror

EGIT_OVERRIDE_REPO_SCIGRAPH_SCIGRAPH=https://github.com/SciCrunch/SciGraph.git
EGIT_OVERRIDE_BRANCH_SCIGRAPH_SCIGRAPH=no-images

# temporary commit override until the converter fixes are merged
EGIT_OVERRIDE_BRANCH_OPEN_PHYSIOLOGY_OPEN_PHYSIOLOGY_VIEWER=fix-wrapper
#+end_src

# FIXME the ACCT_ and EGIT_OVERRIDE_ should probably be in env, but we rebuild
# this profile so frequently I think putting it in make.defaults is probably ok

**** mask
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/package.mask
# dynapad
>=media-gfx/imagemagick-7
#+end_src
**** unmask
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/package.unmask
# gtknor
<gnome-base/librsvg-2.41
dev-python/dicttoxml
#+end_src
**** accept_keywords
# FIXME pipdeptree requires hatch-vcs but bug in portage means that it isn't pulled in correctly ;_;
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/package.accept_keywords
dev-python/pipenv ~amd64
# needed for pipenv
dev-python/dparse ~amd64
dev-python/pipdeptree ~amd64
dev-python/plette ~amd64
dev-python/pythonfinder ~amd64
dev-python/shellingham ~amd64

app-misc/yq ~amd64

# harfbuzz 3.1.2 needs freetype-2.11.1 otherwise build fails
=media-libs/freetype-2.11.1 ~amd64

# needed for media-gfx/renderdoc
dev-libs/miniz ~amd64

# tgbugs-overlay
app-misc/protege-bin ~amd64
dev-db/blazegraph-bin ~amd64
dev-db/pguri **
dev-java/robot-bin ~amd64
dev-java/scigraph-bin ~amd64
dev-node/apinat-converter **
#dev-scheme/racket ~amd64  # profile can't restrict by repo :(
sci-libs/readstat ~amd64

# tgbugs-overlay python
dev-python/interlex **
dev-python/sparcur **

# sparcur
app-text/xlsx2csv ~amd64
dev-python/semver ~amd64
dev-python/click-didyoumean ~amd64
dev-db/redict ~amd64
dev-libs/hiredict ~amd64

# gtknor
<gnome-base/librsvg-2.41 **

# emacs
app-emacs/vterm ~amd64
app-emacs/zmq ~amd64

# sbcl
dev-lisp/asdf ~amd64
dev-lisp/uiop ~amd64
dev-lisp/sbcl ~amd64

# pypy3
dev-lang/pypy ~amd64
dev-lang/pypy3-exe ~amd64
dev-python/pypy3-exe ~amd64
dev-python/pypy3 ~amd64
dev-python/pypy3_10-exe ~amd64
dev-python/pypy3_10 ~amd64
#+end_src
# probably have to put dev-python/*::tgbugs-overlay in /etc/portage/package.accept_keywords/profile
# dev-python/pyontutils ~amd64
# XXX if we introduce pypy3 this is going to be a mess

# interesting issue with dev-python/interlex ** nominally being completely
# irrelevant and orthognal to the rest of the contstraints on other images
# that will never install it, it technically triggers a rebuild of everything
# because we make the profile a dependency, we mitigate this by using binpkgs
# but really we should be able to put things like this in the package builder
# image and snapshot and then only in the docker files that will actually
# install that package itself ... hrm ... unfortunately that is WAY harder
# for someone to understand and track than it is to stick it in here and
# rebuild everything ... sigh, eventually we will implement this optimization
**** package.use
# TODO consider dev-db/sqlite secure-delete
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/package.use
# setpriv command
sys-apps/util-linux caps

# font rendering
media-libs/freetype -cleartype-hinting -cleartype_hinting

# reduce deps
dev-libs/uriparser -doc

# needed to ensure that -egl doesn't introduce conflicts
x11-base/xorg-server minimal

app-editors/emacs dynamic-loading gmp json threads

# gdb don't pull in the world
sys-devel/gdb -nls -python

# pyzmq
net-libs/zeromq drafts
dev-python/pyzmq drafts

# rust bootstrap from source
dev-lang/rust mrustc-bootstrap

# needed for apinat-converter
net-libs/nodejs npm

dev-scheme/racket jit

# graphviz
media-libs/gd truetype fontconfig

# needed for inkscape
app-text/poppler cairo

# needed for redict
dev-libs/jemalloc stats

# pypy3
dev-lang/pypy sqlite
dev-python/pypy3-exe jit
dev-python/pypy3 sqlite
dev-python/pypy3_10-exe jit
dev-python/pypy3_10 sqlite

# uwsgi needs at least one backend enabled
www-servers/uwsgi python

# needed for matplotlib apparently
media-gfx/imagemagick jpeg tiff
virtual/imagemagick-tools jpeg tiff
dev-python/pillow webp tiff  # tiff needed for inkscape
media-libs/tiff jpeg  # lol

# keep ipykernel deps minimal for emacs-jupyter
dev-python/ipython -smp

# needed for scipy
dev-python/numpy lapack

# tgbugs-overlay added the stats keyword to avoid scipy but it works on pypy3 now
dev-python/seaborn stats

# tgbugs-overlay python
app-arch/brotli python  # needed by aiohttp by elasticsearch-py
dev-python/interlex alt database
dev-python/orthauth yaml
dev-python/pint babel uncertainties
dev-python/sparcur cron  # XXX FIXME not all images want to pull in the cron deps, or the dashboard deps
dev-python/sxpyr -cli  # XXX FIXME avoid circular dep on clifun
#+end_src
**** package.use.mask
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/package.use.mask
media-libs/libepoxy -egl
#+end_src
**** use.mask
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/use.mask
# reduce deps
perl
gtk
cups
postscript

# reduce xorg deps
llvm
egl
gles2
gallium
dbus
vala
introspection
elogind

# allow pypy3 as a python target
-python_targets_pypy3
-python_targets_pypy3_11
#+end_src
**** x/
intentionally empty
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/x/parent
..
#+end_src
**** nox/
intentionally empty
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/nox/parent
..
#+end_src
**** pypy3/
intentionally empty
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/pypy3/parent
..
#+end_src
**** static/
intentionally empty
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/static/parent
..
#+end_src
**** gnu/
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/gnu/parent
gentoo:default/linux/amd64/23.0/hardened
..
#+end_src
**** gnu/x/
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/gnu/x/parent
..
../../x
#+end_src
**** gnu/nox/
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/gnu/nox/parent
..
../../nox
#+end_src
**** musl/
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/parent
gentoo:default/linux/amd64/23.0/musl/hardened
..
#+end_src
**** musl/x/
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/x/parent
..
../../x
#+end_src
**** musl/nox/
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/nox/parent
..
../../nox
#+end_src
**** musl/pypy3
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/pypy3/parent
..
../../pypy3
#+end_src
**** musl/pypy3/x
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/pypy3/x/parent
..
../../../x
#+end_src
**** musl/pypy3/nox
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/pypy3/nox/parent
..
../../../nox
#+end_src
**** musl/static
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/static/parent
..
../../static
#+end_src
**** musl/static/x
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/static/x/parent
..
../../../x
#+end_src
**** musl/static/nox
***** parent
#+begin_src conf :tangle ./docker-profile/base/docker-profile/profiles/tgbugs/musl/static/nox/parent
..
../../../nox
#+end_src
** musl
TODO separate musl and gnu specific stuff
*** build
#+name: &build-profile-musl
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:musl \
--build-arg PROFILE_AXIS=musl \
--file docker-profile/axes-musl.Dockerfile .
#+end_src
*** file

#+begin_src dockerfile :tangle ./docker-profile/axes-musl.Dockerfile
<<&profile-axes>>

ARG bp=docker-profile/base/

# profile specific patches
ADD ${bp}musl-find_library.patch                 etc/portage/patches/dev-lang/python:2.7/musl-find_library.patch
ADD ${bp}musl-include-sys-time.patch             etc/portage/patches/dev-python/pypy3_x-exe/musl-include-sys-time.patch
ADD ${bp}musl-fix-stdio-defs.patch               etc/portage/patches/dev-python/pypy3_x-exe/musl-fix-stdio-defs.patch
ADD ${bp}pypy3-json-str-subclass-safety.patch    etc/portage/patches/dev-python/pypy3_x/json-str-subclass-safety.patch
ADD ${bp}musl-renderdoc-plthook-elf.patch        etc/portage/patches/media-gfx/renderdoc/musl-renderdoc-plthook-elf.patch
ADD ${bp}musl-renderdoc-execinfo.patch           etc/portage/patches/media-gfx/renderdoc/musl-renderdoc-execinfo.patch
ADD ${bp}tini-musl-basename.patch                etc/portage/patches/sys-process/tini-0.19.0/tini-musl-basename.patch
#+end_src

#+name: &profile-axes
#+begin_src dockerfile :tangle ./docker-profile/axes.Dockerfile
FROM docker.io/library/busybox:latest AS builder

ARG PROFILE_AXIS=x

WORKDIR /build

ADD docker-profile/${PROFILE_AXIS}/docker-profile var/db/docker-profile

FROM scratch

WORKDIR /
COPY --from=builder /build /
#+end_src

*** patches
**** sys-process/tini
From https://github.com/krallin/tini/pull/223
See also https://bugs.gentoo.org/934990
#+begin_src diff :tangle ./docker-profile/base/tini-musl-basename.patch
From 10479a6eef32f8e64fd5bf894dee9c7a6f21ce4c Mon Sep 17 00:00:00 2001
Date: Sun, 14 Apr 2024 15:33:51 +0200
Subject: [PATCH] Support POSIX basename() from musl libc

Musl libc 1.2.5 removed the definition of the basename() function from
string.h and only provides it in libgen.h as the POSIX standard
defines it.

basename() modifies the input string, copy it first with strdup(), If
strdup() returns NULL the code will handle it.

---
 src/tini.c | 15 +++++++++++----
 1 file changed, 11 insertions(+), 4 deletions(-)

diff --git a/src/tini.c b/src/tini.c
index 7914d3a..41d1506 100644
--- a/src/tini.c
+++ b/src/tini.c
@@ -14,6 +14,7 @@
 #include <stdlib.h>
 #include <unistd.h>
 #include <stdbool.h>
+#include <libgen.h>
 
 #include "tiniConfig.h"
 #include "tiniLicense.h"
@@ -224,14 +225,19 @@ int spawn(const signal_configuration_t* const sigconf_ptr, char* const argv[], i
 }
 
 void print_usage(char* const name, FILE* const file) {
-	fprintf(file, "%s (%s)\n", basename(name), TINI_VERSION_STRING);
+	char *dirc, *bname;
+
+	dirc = strdup(name);
+	bname = basename(dirc);
+
+	fprintf(file, "%s (%s)\n", bname, TINI_VERSION_STRING);
 
 #if TINI_MINIMAL
-	fprintf(file, "Usage: %s PROGRAM [ARGS] | --version\n\n", basename(name));
+	fprintf(file, "Usage: %s PROGRAM [ARGS] | --version\n\n", bname);
 #else
-	fprintf(file, "Usage: %s [OPTIONS] PROGRAM -- [ARGS] | --version\n\n", basename(name));
+	fprintf(file, "Usage: %s [OPTIONS] PROGRAM -- [ARGS] | --version\n\n", bname);
 #endif
-	fprintf(file, "Execute a program under the supervision of a valid init process (%s)\n\n", basename(name));
+	fprintf(file, "Execute a program under the supervision of a valid init process (%s)\n\n", bname);
 
 	fprintf(file, "Command line options:\n\n");
 
@@ -261,6 +267,7 @@ void print_usage(char* const name, FILE* const file) {
 	fprintf(file, "  %s: Send signals to the child's process group.\n", KILL_PROCESS_GROUP_GROUP_ENV_VAR);
 
 	fprintf(file, "\n");
+	free(dirc);
 }
 
 void print_license(FILE* const file) {
-- 
2.45.3
#+end_src
*** profiles
**** package.use
#+begin_src conf :tangle ./docker-profile/musl/docker-profile/profiles/tgbugs/musl/package.use
dev-lang/ghc ghcbootstrap
#+end_src
**** package.use.mask
Reminder: can't use package.use =-flag= at profile level because
=/etc/portage/package.use= will overwrite.
#+begin_src conf :tangle ./docker-profile/musl/docker-profile/profiles/tgbugs/musl/package.use.mask
# clisp with X fails to build on musl
dev-lisp/clisp X
#+end_src
**** package.use.force
We have a cross compile bootstrap and the clisp build is broken.
Have to use =package.use.force= because the main gentoo musl profile sets it there.

This is no longer needed because we can emerge cross compiled packages
directly and don't have to do the image side loading dance anymore.
#+begin_src conf :tangle ./docker-profile/musl/docker-profile/profiles/tgbugs/musl/package.use.force :tangle no
dev-lisp/sbcl -system-bootstrap
#+end_src
**** package.unmask
#+begin_src conf :tangle ./docker-profile/musl/docker-profile/profiles/tgbugs/musl/package.unmask
dev-lisp/sbcl
media-gfx/renderdoc
#+end_src
** gnu
TODO need the gnu specific tweaks
*** build
#+name: &build-profile-gnu
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:gnu \
--build-arg PROFILE_AXIS=gnu \
--file docker-profile/axes.Dockerfile .
#+end_src
*** profiles
**** keep
FIXME temp until there is actually a gnu specific file to mkdirp on
#+begin_src conf :tangle ./docker-profile/gnu/docker-profile/profiles/tgbugs/gnu/.keep :mkdirp yes
#+end_src
** x
*** build
#+name: &build-profile-x
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:x \
--build-arg PROFILE_AXIS=x \
--file docker-profile/axes.Dockerfile .
#+end_src
*** profiles
**** parent
#+begin_src conf :tangle ./docker-profile/x/docker-profile/profiles/tgbugs/x/parent
..
#+end_src
**** make.defaults
#+begin_src conf :tangle ./docker-profile/x/docker-profile/profiles/tgbugs/x/make.defaults
USE="${USE} X"
VIDEO_CARDS="-*"
#+end_src
**** package.use
# we might consider including svg and libxml2 because they are already pulled in by racket and some other components
# app-editors/emacs libxml2 svg
#+begin_src conf :tangle ./docker-profile/x/docker-profile/profiles/tgbugs/x/package.use
# ,*/* X # FIXME it seems that wildcards are not allowed in here so for now has to be done later

media-libs/freetype harfbuzz

# the mesa ebuilds in the main tree are missing the fact that
# gbm expects egl to be enabled, if it is not build errors
media-libs/mesa -gbm

app-editors/emacs gui jpeg png Xaw3d xft # XXX note that latest reccomendations are to use harfbuzz + cairo for text shaping (or something like that)
app-emacs/emacs-common gui

# avoid extra deps
dev-build/cmake -ncurses

# xdg-utils build time dep pulled in by cups somehow
app-text/xmlto text
#+end_src

**** mask
# Looks like the mesa issue has been fixed.
# The media-libs/mesa-21.1 set fails to build even with all the use flags set correctly.
# Same issue with media-libs/mesa-21.1 https://bugs.gentoo.org/828491. Currently 21.2.6
# is the only one that will compile correctly.
#+begin_src conf :tangle ./docker-profile/x/docker-profile/profiles/tgbugs/x/package.mask
#+end_src
**** accept_keywords
#+begin_src conf :tangle ./docker-profile/x/docker-profile/profiles/tgbugs/x/package.accept_keywords
#+end_src
** nox
Explicit nox profile.
*** build
#+name: &build-profile-nox
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:nox \
--build-arg PROFILE_AXIS=nox \
--file docker-profile/axes.Dockerfile .
#+end_src
*** file
*** etc
**** package.unmask
#+begin_src conf :tangle ./docker-profile/nox/package.unmask
dev-scheme/racket::gentoo
#+end_src
*** profiles
**** parent
FIXME see if we actually need this, I think we might due to klobbering by =COPY=?
#+begin_src conf :tangle ./docker-profile/nox/docker-profile/profiles/tgbugs/nox/parent :tangle no
..
#+end_src

**** package.use
#+begin_src conf :tangle ./docker-profile/nox/docker-profile/profiles/tgbugs/nox/package.use
# java
dev-java/icedtea headless-awt
dev-java/openjdk headless-awt
dev-java/openjdk-bin headless-awt
virtual/jdk headless-awt

# racket
dev-scheme/racket minimal
#+end_src

** pypy3
*** build
#+name: &build-profile-pypy3
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:pypy3 \
--build-arg PROFILE_AXIS=pypy3 \
--file docker-profile/axes.Dockerfile .
#+end_src
*** file
#+begin_src dockerfile
#+end_src
*** profiles
**** package.use
FIXME this is almost certainly not going to produce what we want
because =+pypy3= doesn't just append to existing in the same way that
=-*= removes all existing.
#+begin_src conf :tangle ./docker-profile/pypy3/docker-profile/profiles/tgbugs/pypy3/package.use
*/* PYTHON_TARGETS: +pypy3_11
#+end_src
** static
*** build
#+name: &build-profile-static
#+begin_src screen
docker build \
--tag tgbugs/docker-profile:static \
--build-arg PROFILE_AXIS=static \
--file docker-profile/axes.Dockerfile .
#+end_src

*** profiles
***** make.defaults
We only set =static-libs= not =static= because =static= statically
links the executable which we rarely want, in which case a positive
static use flag should be added below, rather than turning off nearly
every instance of =static= that we encounter.
#+begin_src conf :tangle ./docker-profile/static/docker-profile/profiles/tgbugs/static/make.defaults
USE="${USE} static-libs"
#+end_src
***** package.use
#+begin_src conf :tangle ./docker-profile/static/docker-profile/profiles/tgbugs/static/package.use
# sbcl static vs non-static hack to prevent accidentally installing unpatched sbcl
dev-lisp/sbcl source
#+end_src
***** package.use old :ARCHIVE:
#+begin_src conf :tangle ./docker-profile/static/docker-profile/profiles/tgbugs/static/package.use :tangle no
# don't build openssh with static because it conflicts with the
# pie use flag for hardened which cannot be unset
net-misc/openssh -static

# bzip2 is completely broken if compiled with either of these use flags ???
# that is, it will compile but will leave the system unable to compress anything
app-arch/bzip2 -static

# trying to build with static causes a configure error due to container projections
# building with security=insecure supposedly can work around this
# cross compile check process_vm_readv # ccc process_vm_readv
# FIXME, further reading seems to suggest that we don't actually want static? just
# static-libs? so going to try that
app-arch/gzip -static
sys-apps/debianutils -static
sys-apps/coreutils -static
sys-devel/patch -static
sys-apps/findutils -static
sys-apps/sed -static
sys-devel/make -static
net-misc/wget -static
sys-apps/diffutils -static
sys-apps/grep -static
app-editors/nano -static
sys-devel/flex -static
sys-devel/bison -static
#+end_src

#+begin_src bash
echo \
sys-devel/bison \
-static >> /etc/portage/package.use/sigh && \
emerge -uDN @world
#+end_src
* profiles
break profiles into its own to level section in hopes of finding a better way to compose images
** package.license
To avoid tainting other images with various licenses, images that need specific licenses should only be accepted
when building the specific package that requires the license and the corresponding file should only be included
to accept the license in the images where the package will be used. Another way around some of these issues might
be to create an image that can be mounted as a volume by users which contains only that specific package, however
this depends on the license and might only work for e.g. something like media-fonts/corefonts. We aren't going to
go that route for now, and if I can find any guidance on redistributing docker images containing these I will add
and update here. However I can't find any clear guidance at this time.
*** dev-racket
There is an issue with =racket-overlay= licenses that is cause
by https://pkgs.racket-lang.org missing license metadata for a
very large number of projects and rather than translating a missing
license as simply missing, it is listed as all-rights-reserved which
is not accurate. This is a workaround for that issue until it can be
resolved.

#+name: license-dev-racket
#+begin_src conf :tangle ./musl/package-builder/package.license/dev-racket :mkdirp yes
# FIXME HACK
dev-racket/*::racket-overlay all-rights-reserved
#+end_src

*** media-fonts/corefonts
#+name: license-media-libs/corefonts
#+begin_src conf :tangle ./musl/package-builder/package.license/media-fonts/corefonts :mkdirp yes
media-fonts/corefonts MSttfEULA
#+end_src

#+begin_src conf :tangle ./musl/kg-dev/package.license/media-fonts/corefonts :mkdirp yes
<<license-media-libs/corefonts>>
#+end_src

** tweaks
Temporary tweaks that we don't want to stick in the profile because they
only affect a subset of images and are presumed safe to apply independently.
FIXME not clear that this should actually live under profiles but kind of makes sense here?
#+name: tweak-accept-keywords
#+begin_src conf :tangle ./musl/package-builder/accept_keywords :mkdirp yes
#+end_src

** debug
#+name: debug-profile
#+begin_src bash :results file :var img="base"
_name=profile-${img}.tar
docker create --name="tmp_$$" tgbugs/docker-profile:${img} true > /dev/null &&
docker export tmp_$$ > ${_name} &&
docker rm tmp_$$ > /dev/null
printf ${_name}
#+end_src

#+call: debug-profile(img="musl")

#+call: debug-profile(img="x")

** axes
*** libc
These are present in the base profile image since the existence and basic structure is present in the base
the actual customization for static, python, and xorg is kept in separate images, if there are actually
some non-orthogonal settings then we'll have to figure out how to handle them, but so far there haven't been
any interactions for e.g. pypy3 xorg that need special treatment.
**** gnu
#+name: &profile-gnu-free
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:gnu / /
#+end_src
**** musl
#+name: &profile-musl-free
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:musl / /
#+end_src
*** python
**** pypy3
#+name: &profile-pypy3
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:pypy3 / /
#+end_src
*** static
**** static
#+name: &profile-static
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:static / /
#+end_src
*** xorg
**** x
#+name: &profile-x
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:x / /
#+end_src
**** nox
#+name: &profile-nox
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:nox / /
#+end_src
** impossible/not meaningful combinations
- gnu static
  - gnu can't statically link
- musl pypy3 static
  - at the moment there is almost no reason to have pypy3 with
    static-libs since pypy3 can't be statically linked, and
    static-libs is primarily used to generate thin images
- gnu pypy3
  - might want this in the future, but for now we don't have a use
    case since gnu images are primarily used to cross compile the few
    packages that don't have musl binaries, pypy3 doesn't help much in those cases
** compose
these profiles are mutually exclusive? well, not quite, for builders
gnu and musl are mutex

so how do we compose these?
do we compose them via noweb with multiple =COPY --from=image= or what?
do we make it possible to parameterize a single docker file to say which
subtrees to pull in for a given concrete image and build all of them?

composing via =COPY= would seem to avoid a duplication of rebuilding
but noweb doesn't have good composability, on the other hand composing
by stacking images causes a proliferation of profile images that require
cascading rebuilds to make sure that everything is up to date, and might
also require a proliferation of dockerfiles if we can't figure out how to
deal with conditional copies in the blank case

in summary, going to start with noweb and see how it plays out

annoyingly there isn't a simple way to validate a profile actually matches
the image it is built from right now ...
*** /
#+name: &profile-base
#+begin_src dockerfile
COPY --from=tgbugs/docker-profile:base / /
#+end_src
*** /gnu
#+name: &profile-gnu
#+begin_src dockerfile
<<&profile-base>>
<<&profile-gnu-free>>
#+end_src
*** /gnu/x
#+name: &profile-gnu-x
#+begin_src dockerfile
<<&profile-gnu>>
<<&profile-x>>
#+end_src
*** /gnu/nox
#+name: &profile-gnu-nox
#+begin_src dockerfile
<<&profile-gnu>>
<<&profile-nox>>
#+end_src
*** /gnu/all
#+name: &profile-gnu-all
#+begin_src dockerfile
<<&profile-gnu>>
<<&profile-x>>
<<&profile-nox>>
#+end_src
*** /musl
#+name: &profile-musl
#+begin_src dockerfile
<<&profile-base>>
<<&profile-musl-free>>
#+end_src
*** /musl/x
#+name: &profile-musl-x
#+begin_src dockerfile
<<&profile-musl>>
<<&profile-x>>
#+end_src
*** /musl/nox
#+name: &profile-musl-nox
#+begin_src dockerfile
<<&profile-musl>>
<<&profile-nox>>
#+end_src
*** /musl/pypy3
#+name: &profile-musl-pypy3
#+begin_src dockerfile
<<&profile-musl>>
<<&profile-pypy3>>
#+end_src
*** /musl/pypy3/x
#+name: &profile-musl-pypy3-x
#+begin_src dockerfile
<<&profile-musl-pypy3>>
<<&profile-x>>
#+end_src
*** /musl/pypy3/nox
#+name: &profile-musl-pypy3-nox
#+begin_src dockerfile
<<&profile-musl-pypy3>>
<<&profile-nox>>
#+end_src
*** /musl/static
#+name: &profile-musl-static
#+begin_src dockerfile
<<&profile-musl>>
<<&profile-static>>
#+end_src
*** /musl/static/x
#+name: &profile-musl-static-x
#+begin_src dockerfile
<<&profile-musl-static>>
<<&profile-x>>
#+end_src
*** /musl/static/nox
#+name: &profile-musl-static-x
#+begin_src dockerfile
<<&profile-musl-static>>
<<&profile-nox>>
#+end_src
*** /musl/all
#+name: &profile-musl-all
#+begin_src dockerfile
<<&profile-musl>>
<<&profile-x>>
<<&profile-nox>>
<<&profile-pypy3>>
<<&profile-static>>
#+end_src
*** all not libc
#+name: &profile-all-not-libc
#+begin_src dockerfile
<<&profile-x>>
<<&profile-nox>>
<<&profile-pypy3>>
<<&profile-static>>
#+end_src
*** static/x
Sometimes we need to add multiple profiles to existing images that already have the base profile.
#+name: &profile-static-x
#+begin_src dockerfile
<<&profile-static>>
<<&profile-x>>
#+end_src

* repos
Overlays can take up quite a bit of space so it is better to mount
them the same way we mount the gentoo repo during build so that we can
keep the images a bit slimmer. We can publish the build images
independently, and it is also worth noting that from a reproducibility
perspective the exact ebuilds are stored in file:/var/db/pkg/.

*** run
**** debug
#+begin_src bash
docker create -v /var/db/repos --name portage-snap docker.io/gentoo/portage:latest /bin/true
if [ -n "${_use_podman}" ]; then
[ -n "$(podman container ls -q --filter name=portage-snap --filter status=initialized)" ] || \
podman container init portage-snap
fi
docker run \
--volumes-from portage-snap \
--entrypoint /bin/bash \
-it tgbugs/repos:latest
#+end_src
**** rebuild for single package change
yes this is hilariously inefficient we can try to find a better way in the future
#+begin_src bash :noweb yes
builder-debug
# emaint sync --repo tgbugs-overlay
# builder-arb pennsieve
./source.org tangle
./source.org build-image tgbugs/repos:latest
<<&re-local-repos-snap>>
./source.org build-image tgbugs/musl:sparcron
./source.org build-image tgbugs/musl:sparcron-user
#+end_src

#+name: &re-local-repos-snap
#+begin_src bash
docker container inspect local-repos-snap > /dev/null && { docker rm local-repos-snap; }
docker create -v /var/db/repos --name local-repos-snap tgbugs/repos:latest /bin/true || return $?;
# https://github.com/containers/podman/issues/10262
# FIXME TODO if podman then you must run the following for reasons I still do not entirely understand
if [ -n "${_use_podman}" ]; then
[ -n "$(podman container ls -q --filter name=local-repos-snap --filter status=initialized)" ] || \
podman container init local-repos-snap
fi
#+end_src

*** build
# FIXME the --no-cache option here means that setting --repos forces a
# rebuild of _everything_ downstream even if repos didn't change
#+name: &repos-build-repos
#+begin_src screen
docker build \
--no-cache \
--build-arg SYNC_GENTOO=$SYNC_GENTOO \
--tag tgbugs/repos:latest \
--file repos/Dockerfile repos
#+end_src
*** file
#+begin_src dockerfile :tangle ./repos/Dockerfile
ARG IMG_REPOS_BUILDER=tgbugs/musl:eselect-repo

FROM ${IMG_REPOS_BUILDER} AS builder

COPY --from=docker.io/gentoo/portage:latest /var/db/repos/gentoo /var/db/repos/gentoo

RUN \
   emaint sync --repo musl \
&& emaint sync --repo haskell \
&& emaint sync --repo racket-overlay \
&& emaint sync --repo tgbugs-overlay

# manual sync in cases where there is a showstopper blocking progress
ARG SYNC_GENTOO

# FIXME if you ever have to fix a broken profile and sync at the same time
# this is horribly inefficient, and we should probably add a separate image
RUN \
   test -z $SYNC_GENTOO \
# WOULD SOMEONE CARE TO EXPLAIN TO ME HOW THIS SOLVES THE ISSUE !??!?!
|| { mv /var/db/repos /var/db/repos-wat; cp -a /var/db/repos-wat /var/db/repos; rm -r /var/db/repos-wat; emaint sync --repo gentoo; }
# why this by itself hangs forever but a simple mv and cp -a resolves the issue we may never know
# || emaint sync --repo gentoo

FROM docker.io/library/busybox:latest

WORKDIR /
COPY --from=builder /var/db/repos /var/db/repos
CMD ["/bin/true"]
VOLUME /var/db/repos
#+end_src
* common
Functionality shared in common across arch, libc, etc.
Usually built on a specific arch, libc, etc. but output should be reusable on any combination.
** user
#+name: &build-user
#+begin_src screen
docker build \
--tag tgbugs/common:user \
--file common/user/Dockerfile common/user
#+end_src

# FIXME this is sufficient to create the default set of files and directories for the user
# however it is not able to deal with the fact that groupadd and useradd still must be run
# on the host system, which leads me to think that the only composability we are going to
# get here is via noweb :/ the primary issue is /etc/groups and other similar things

#+name: &run-user-noskel
#+begin_src bash :eval never
groupadd -g ${UID} ${USER_NAME} \
&& useradd -M -u ${UID} -g ${UID} ${USER_NAME}
#+end_src


Block to be nowebbed for the user creation portion of the images.
Should be +followed+ preceded? by a =COPY --from= that was built by
layering on top of the image we build below.

#+name: &musl-file-user-base
#+begin_src dockerfile
ARG UID=1000
ARG USER_NAME=user

RUN \
<<&run-user-noskel>>

USER ${USER_NAME}

WORKDIR /home/${USER_NAME}

RUN \
{ command -v xdg-user-dirs-update && xdg-user-dirs-update; rmdir Desktop Pictures Documents Public Downloads Templates Music Videos > /dev/null 2>&1;} || true

ENV PATH="/home/${USER_NAME}/.local/bin:${PATH}"
#+end_src

=groupadd= and =useradd= mean that this needs to be built from a gentoo image.
#+name: &user-skel-common
#+begin_src dockerfile
ARG UID=1000
ARG USER_NAME=user

RUN \
groupadd -g ${UID} ${USER_NAME} \
&& useradd -m -k /etc/skel -u ${UID} -g ${UID} -d $(pwd)/home/${USER_NAME} ${USER_NAME}

RUN \
mkdir -p home/${USER_NAME}/.local/bin

RUN \
chown -R ${UID}:${UID} home/${USER_NAME}
#+end_src

On the off chance that we don't have a musl source image around make it possible to use a different builder image.
#+begin_src dockerfile :tangle ./common/user/Dockerfile
ARG IMG_USER_BUILDER=docker.io/gentoo/stage3:amd64-musl-hardened

FROM ${IMG_USER_BUILDER} AS builder

WORKDIR /build

<<&user-skel-common>>

FROM scratch

WORKDIR /
COPY --from=builder /build /
#+end_src
** testing

#+name: &bash-abs-path
#+begin_src bash
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve all symlinks
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # resolve relative symlinks
done
ABS_PATH="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
#+end_src

#+name: &bash-at-abs-path
#+begin_src bash :noweb yes
<<&bash-abs-path>>
pushd "${ABS_PATH}" > /dev/null 2>&1
# end setup
#+end_src

#+begin_src bash
# isn't the actual thing to test
#pushd /var/db/repos/gentoo/dev-python/pypy3
#ebuild $(basename /var/db/pkg/dev-python/pypy3-*/*.ebuild) test

# this doesn't work for pypy3 in particular because it requires python2 and we just need some stdlib tests
ebuild_name=$(basename /var/db/pkg/dev-python/pypy3_10-*/*.ebuild)
package_name="${ebuild_name%.*}"
ebuild ${ebuild_name} clean unpack
#touch /var/tmp/portage/dev-python/${package_name}/.setuped
#touch /var/tmp/portage/dev-python/${package_name}/.unpacked
touch /var/tmp/portage/dev-python/${package_name}/.prepared
touch /var/tmp/portage/dev-python/${package_name}/.configured
touch /var/tmp/portage/dev-python/${package_name}/.compiled
pushd /var/db/repos/gentoo/dev-python/pypy3_10
# unfortunately testing is restricted?
ebuild ${ebuild_name} test
#+end_src

#+name: &image-testing-common
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v ${_path_dockerfiles}:/dockerfiles \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
--entrypoint /bin/bash \
#+end_src

#+name: &image-testing
#+begin_src bash :noweb yes
function test-image () {
local POSITIONAL=()
while [[ $# -gt 0 ]]
do
key="$1"
case $key in
    --debug)              local DEBUG=YES; shift ;;
    ,*)                    local POSITIONAL+=("$1"); shift ;;
esac
done

image=${POSITIONAL[0]}

if [[ -n ${DEBUG} ]]; then
<<&image-testing-common>>
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
-it ${image}
else
<<&image-testing-common>>
--rm \
${image} \
/dockerfiles/bin/test/${image//://}/test
fi

}

function test-images () {
for i in ${@}; do
# TODO also need a negative test
test-image ${i} > /dev/null 2>&1;
OK=$?
if [ ${OK} -eq 0 ]; then
echo "${i} [ OK ]";  # TODO alignment
else
echo "${i} [ FAIL ]";  # TODO alignment
fi
done
}

#+end_src

** portage-maven
:PROPERTIES:
:CUSTOM_ID: portage-maven
:END:
Hack to make it possible to install from maven using portage.
*** build
#+name: &build-portage-maven
#+begin_src screen
docker build \
--tag tgbugs/common:portage-maven \
--file common/portage-maven/Dockerfile common/portage-maven
#+end_src

*** file
The UID for portage is static so it is ok to hard code it [fn::
https://api.gentoo.org/uid-gid.txt
https://wiki.gentoo.org/wiki/Project:Quality_Assurance/UID_GID_Assignment].

#+name: &portage-maven-settings
#+begin_src xml :tangle ./common/portage-maven/settings.xml :mkdirp yes
<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd">
  <localRepository>/var/tmp/portage/.m2/repository</localRepository>
</settings>
#+end_src

#+name: &run-portage-maven-1
#+begin_src bash :eval never :noweb yes
# mkdir -p var/lib/portage/home/.m2 \
chown -R 250:250 var/lib/portage \
&& mkdir -p var/tmp/portage/.m2/repository \
&& chown -R 250:250 var/tmp/portage
#+end_src

#+begin_src dockerfile :tangle ./common/portage-maven/Dockerfile
FROM docker.io/library/busybox:latest AS builder

WORKDIR /build

ADD settings.xml var/lib/portage/home/.m2/settings.xml

RUN \
<<&run-portage-maven-1>>

FROM scratch

WORKDIR /
COPY --from=builder /build /
#+end_src

* musl
Pushes to https://hub.docker.com/r/tgbugs/musl. \\
Derived from [[https://hub.docker.com/r/gentoo/stage3/tags?page=1&ordering=last_updated&name=musl-hardened][gentoo/stage3:amd64-musl-hardened]] \\
Further derived from https://ftp-osl.osuosl.org/pub/gentoo/releases/amd64/autobuilds/current-stage3-amd64-musl-hardened/ \\
and from https://gitweb.gentoo.org/proj/releng.git/tree/releases/specs/amd64/musl/stage3-hardened.spec
** eselect-repo
This is where everything starts. The profile has to be set here etc.
*** run
#+begin_src screen
docker run \
--volumes-from local-portage-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-it tgbugs/musl:eselect-repo
#+end_src

# debug
#+begin_src screen :exports none
docker run -it tgbugs/musl:eselect-repo
#+end_src

# debug tgbugs/repos:latest
#+begin_src screen
docker run \
--volumes-from local-repos-snap \
-it tgbugs/musl:eselect-repo
#+end_src

*** build
#+name: &musl-build-eselect-repo
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:eselect-repo \
--file musl/eselect-repo/Dockerfile musl/eselect-repo
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/eselect-repo/Dockerfile
FROM docker.io/gentoo/stage3:amd64-musl-hardened

<<&gentoo-file-eselect-repo-common-1>>

<<&profile-musl>>

<<&gentoo-file-eselect-repo-common-2>>

RUN \
eselect profile set docker-profile:tgbugs/musl \
&& env-update

<<&gentoo-file-eselect-repo-common-3>>

RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
eselect repository enable musl
#+end_src

#+name: &gentoo-file-eselect-repo-common-1
#+begin_src dockerfile
ARG ARCHIVE

# fix for bad defaults in stage3 images
# remove any default repos as gpg failures cause none type errors
RUN \
   rm /etc/portage/binrepos.conf/*.conf || true
#+end_src

# fix for bad defaults in stage3 images
# RUN \
#    rm /etc/portage/binrepos.conf \
# && mkdir /etc/portage/binrepos.conf

#+name: &gentoo-file-eselect-repo-common-2
#+begin_src dockerfile
RUN \
# FIXME tgbugs-overlay symlinks
ln -s /var/db/repos/gentoo /usr/portage

RUN \
eselect news read all \
&& eselect news purge

# XXX these are retained to avoid crossdev and other issues where
# portage needs these to be folders and are expected to error if
# the profile in question creates a ./profile file in these folders
RUN \
   mkdir /etc/portage/package.accept_keywords > /dev/null 2>&1 \
;  mkdir /etc/portage/package.env             > /dev/null 2>&1 \
;  mkdir /etc/portage/package.license         > /dev/null 2>&1 \
;  mkdir /etc/portage/package.mask            > /dev/null 2>&1 \
;  mkdir /etc/portage/package.unmask          > /dev/null 2>&1 \
;  mkdir /etc/portage/package.use             > /dev/null 2>&1 \
;  mkdir /etc/portage/repos.conf              > /dev/null 2>&1 \
|| true
#+end_src

FIXME python version update issue is another problem with using/having
eselect repo in the direct line rather than just as a helper to set up
the repos overlay is a problem, though I'm not sure there is really
any way to work around the issue, we just accept that every once in
awhile we have slightly larger images because for 1 release cycle we
had to update python since upstream gets the images out of sync

#+name: &gentoo-file-eselect-repo-common-3
#+begin_src dockerfile
# FIXME MAKEOPTS_LOCAL
RUN \
echo "MAKEOPTS=\"-j$(nproc)\"" >> /etc/portage/make.conf
# XXX setting PORTAGE_BINHOSTS has to come later? maybe as an envar?

RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
emerge --info 2>&1 | { grep Invalid\ atom && exit 1; exit 0; }

RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
let diff=( $(date +%s) - $(awk '{ print $1 }' /var/db/repos/gentoo/metadata/timestamp.x) ) \
;  let days=( ${diff} / 86400 ) \
;  [ ${days} -lt 14 ] \
|| { echo 'portage is too old ${days} > 14 days' 1>&2; exit 1; }

# handle updates for python version changes that block further calls to emerge
# these happen periodically when portage and stage3 images have mismached PYTHON_TARGETS
RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
[ $(emerge --info | grep -o 'PYTHON_TARGETS="[^"]\+"') = \
  $(bzgrep -o 'PYTHON_TARGETS="[^"]\+"' /var/db/pkg/dev-python/wheel-*/environment.bz2) ] \
|| emerge -uDN -q --getbinpkg @world \
<<&archive-or-rm>>

# FIXME shouldn't we be using binhosts for this step as well?
RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
emerge -q \
   --getbinpkg \
   dev-vcs/git \
   app-eselect/eselect-repository \
<<&archive-or-rm>>

# if you need to build from a fork change remote for tgbugs-overlay here (ref:tgbugs-overlay-fork)
RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
   eselect repository add tgbugs-overlay git https://github.com/tgbugs/tgbugs-overlay.git \
&& eselect repository add racket-overlay git https://gitlab.com/tgbugs/gentoo-racket-overlay.git \
&& eselect repository enable haskell
#+end_src
# TODO should we be adding the mount points here as well or is that not necessary?

** updated
*** file
Produce an up-to-date base image for =amd64-hardened-musl= from the
latest stage3 image including the
[[https://github.com/gentoo/musl][musl overlay]] as noted on the
[[https://wiki.gentoo.org/wiki/Project:Hardened_musl#Working_with_musl][wiki]].

At the moment the docker images are generated far more frequently than
the underlying stage3 tarballs are updated, so there are two docker
files, one for building the first time and another for running routine
emerge updates until a new stage3 is released.

Alternately, one way to avoid rebuilds is to build packages and store
them across rebuilds. This will take more work, but ultimately might
be a bit more reproducible since we would avoid the issues with having
an image building =FROM= a prior version of itself.

Sometimes this can fail if we try to run updates during the window in
time between tgbugs-overlay updating a Manifest and the gentoo portage
snapshot image catching up with the main tree. I really do not want to
add a sync step when creating the repos image, but it seems like that
is by far the safest and most general solution. Which is exactly what
the =--sync-gentoo= option is for!

# FIXME I think we need to add buildpkg here to limit rebuilds during
# bootstrap in the event of errors? ah except that we can't mount that
# with docker build because of the docker design flaws ... I think we
# just accept that first bootstrap takes longer unless we switch to
# podman or something like that

#+name: &musl/updated
#+begin_src dockerfile :tangle ./musl/updated/Dockerfile
FROM tgbugs/musl:eselect-repo
<<&updated-common>>
#+end_src
#+name: &updated-common
#+begin_src dockerfile
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q -uDN @system @world \
   --getbinpkg \
   --keep-going \
   --exclude sys-process/procps \
|| echo $? > /emerge-fail \
<<&archive-or-rm>>

# fail if emerge fails but for buildkit ensure that we do it in such a
# way that we can truncate further steps and create a debug image
RUN \
test ! -e /emerge-fail

RUN \
eselect gcc set $(eselect gcc list | tail -n 1 | awk '{ print $2 }')
#+end_src

*** build
#+name: &musl-build-updated
#+begin_src screen
docker build \
--tag tgbugs/musl:updated \
--network host \
--add-host local.binhost:127.0.0.1 \
--file musl/updated/Dockerfile musl/updated
#+end_src

*** rebuild
#+begin_src bash
docker build \
--tag tgbugs/musl:updated-remerge \
--file musl/updated/remerge.Dockerfile musl/updated

# check that everything works as expected (and that there were changes at all)
docker run -it tgbugs/musl:updated-remerge

# rename the image
docker image tag tgbugs/musl:updated-remerge tgbugs/musl:updated
#+end_src

*** run
#+name: &musl-run-updated
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
-it tgbugs/musl:updated
#+end_src
** updated-user
An example of how to compose user images to minimize size.
*** run
#+begin_src bash
docker run -it tgbugs/musl:updated-user
#+end_src

*** build
#+name: &musl-build-updated-user
#+begin_src screen
docker build \
--tag tgbugs/musl:updated-user \
--build-arg UID=<<&UID>> \
--file musl/updated-user/Dockerfile musl/updated-user
#+end_src

*** file
#+begin_src dockerfile yes :tangle ./musl/updated-user/Dockerfile
FROM tgbugs/musl:updated

# change this line to copy from whatever user image you need
COPY --from=tgbugs/common:user / /

<<&musl-file-user-base>>
#+end_src

** package-builder-musl
*** notes
there are a number of packages that we need to build variants of
before we get our fully configured builder(s) up and running this
should simplify many of the bootstrapping circularity issues that we
have, particularly with pypy3, harfbuzz/freetype, openjdk, and
anything else that has circular dependencies or long build times, it
does mean that flags on the target profile must be known in advance

but those can be calculated quickly by hopping through =eselect
profile docker-profile:*= and running =emerge --info= and then
taking the set intersection between those and the flags on our
troublesome packages ... in some cases we might be able to move
quite a bit of package building to use this process, except for
cases where

we will actually have to create an image that has access to all the
profiles for this because individual profiles do set use flags per
package (the obvious example being nox setting headless-awt on
specific packages) in theory we could try to avoid setting per package
use flags in derived profiles, but that is a very strong constraint
that it is not clear we actually need to follow

this early in the process the cost of having to rebuild due to changes
in the unified profile is low because literally no work has been done
beyond the base update and we would have to build anyway, the good news
is that by doing this after updated we don't have to rebuild updated
so updates to the early builder become more or less free
*** run
#+name: &musl-run-free-harf-nonsense
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v "${_path_binpkgs}":/var/cache/binpkgs \
-v "${_path_dockerfiles}"/bin/quickpkg-new:/tmp/quickpkg-new \
-v "${_path_dockerfiles}"/bin/harfbuzz-freetype-sigh.sh:/etc/entrypoint.sh \
--entrypoint /etc/entrypoint.sh \
-it tgbugs/musl:updated
#+end_src

TODO mount =/tmp/= and =/var/tmp/= to ramdisk to avoid blasting the ssd more than necessary
#+begin_src screen
docker run \
--volumes-from local-repos-snap \
-v "${_path_binpkgs}":/var/cache/binpkgs \
-v "${_path_dockerfiles}"/bin/quickpkg-new:/tmp/quickpkg-new \
-it tgbugs/musl:package-builder-musl
#+end_src
# --volumes-from cross-sbcl \
*** build
#+name: &musl-build-package-builder-musl
#+begin_src screen
docker build \
--tag tgbugs/musl:package-builder-musl \
--file musl/package-builder/musl.Dockerfile musl/package-builder
#+end_src
*** file
1. all profiles
2. emerge settings for building binary packages
#+begin_src dockerfile :tangle ./musl/package-builder/musl.Dockerfile
FROM tgbugs/musl:updated

COPY --from=tgbugs/common:portage-maven / /

<<&profile-all-not-libc>>

ADD entrypoints /etc/entrypoints
ADD repo_name /var/db/crossdev/profiles/repo_name
ADD layout.conf /var/db/crossdev/metadata/layout.conf
ADD crossdev.conf /etc/portage/repos.conf/crossdev.conf
# ADD sbcl.env /etc/portage/env/dev-lisp/sbcl
# TODO need a full combo world file for the docker set here, iirc missing static right now
# FIXME actually can't quite do this, there are packages that must be excluded for base and nox i.e. xorg-server (sigh sigh sigh)
ADD world /etc/portage/sets/docker
ADD sets/license /etc/portage/sets/license

RUN \
echo 'FEATURES="${FEATURES} buildpkg"' >> /etc/portage/make.conf \
&& echo 'EMERGE_DEFAULT_OPTS="${EMERGE_DEFAULT_OPTS} --binpkg-changed-deps=y --usepkg"' >> /etc/portage/make.conf
#+end_src

*** entrypoints
TODO only build if the package has not already been built
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./bin/harfbuzz-freetype-sigh.sh :mkdirp yes
function doqp () {
local _qpvar
emerge --info 2>&1 | grep "Unable to parse profile" && { echo "bad profile, if using podman did you container init?"; return 1; };
_qpvar=$(/tmp/quickpkg-new); [ -z "${_qpvar}" ] || quickpkg ${_qpvar}
}

USE="-harfbuzz" emerge -1q --usepkg media-libs/freetype
doqp

# have to set -cairo here otherwise harfbuzz -truetype will fail to build
USE="-cairo -truetype" emerge -1q --usepkg media-libs/harfbuzz
doqp

USE="harfbuzz" emerge -1q --usepkg media-libs/freetype
doqp

USE="cairo truetype" emerge -1q --usepkg media-libs/harfbuzz
doqp

# the static-libs variants of freetype
USE="static-libs -harfbuzz" emerge -1q --usepkg media-libs/freetype
doqp

USE="static-libs harfbuzz" emerge -1q --usepkg media-libs/freetype
doqp
#+end_src

- axes
  - package
  - use flag
    - global (not clear we actually need this even for the static-libs case?, but probably desirable?)
    - package specific
    - python targets (this one is particularly annoying)
  - profile (alternative to use flags)
    should be done separately from the =--onlydeps --only-deps-with-rdeps=n= run

#+begin_src bash
{media-libs/freetype,metia-libs/harfbuzz}
{"harfbuzz"}
{"cairo truetype"}
#+end_src

#+begin_src bash :tangle ./musl/package-builder/entrypoints/builder.sh :mkdirp yes
function build-thing () {
local profile_base variants profiles variant prof profile
profile_base=docker-profile:tgbugs/musl
# TODO can we generate the variants and profiles?
declare -a xorgs=('' '/x' '/nox')  # XXX atm x/nox are the only mutually exclusive use flag cases that break portage (it seems)
declare -a statics=('' '/static')
declare -a pythons=('' 'pypy3')
# TODO likely need to restrict build patterns, or do the composition manually and skip cases we don't deal with/want ???
# or just build them all and don't worry about it?
declare -a profiles=(
''
/x
/nox
/static/x
)
for variant in "${statics[@]}"; do  # have to quote the array unpacking so that empty string isn't skipped
  for prof in "${xorgs[@]}"; do
    profile=${profile_base}${variant}${prof}
    eselect profile set ${profile} # || return $?
    env-update
    source /etc/profile
    echo build package ${@} for ${profile}
    # TODO tweak the options here XXX NOTE usually going to want to use image meta ebuilds or sets to avoid switching profiles between package builds
    # FIXME --buildpkgonly means that --onlydeps --onlydeps-with-rdeps=n probably needs to be run first?
    # ok, this more or less seems to work, it does unpack the binary before checking to see that it came from a binary
    # so it is not 100% efficient, but seems ok?
    emerge -uDNq --keep-going --onlydeps --onlydeps-with-rdeps=n ${@} # || return $?
    emerge -uDNq --keep-going --buildpkgonly ${@} # || return $?
  done
done
eselect profile set ${profile_base}
env-update
source /etc/profile
}
#+end_src

TODO deal with needing to exclude whole packages from certain builds, e.g. xorg-server whenever -X is set

** pypy3
*** run
#+name: &musl-run-pypy3
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
-it tgbugs/musl:pypy3
#+end_src
*** test
#+name: &test-musl-pypy3
#+begin_src screen
test-image tgbugs/musl:pypy3
#+end_src

#+header: :shebang "#!/usr/bin/env sh\nset -e"
#+begin_src bash :tangle ./bin/test/tgbugs/musl/pypy3/test :mkdirp yes :noweb yes
<<&bash-at-abs-path>>
pypy3 test.py
#+end_src

#+begin_src python :tangle ./bin/test/tgbugs/musl/pypy3/test.py :mkdirp yes
import os, sys, json, importlib, functools, collections, pathlib
print(sys.version_info)
print(sys.pypy_version_info)
print('ok')
#raise NotImplementedError('expected to fail')  # FIXME a hack to ensure tests can fail
#+end_src

*** build
#+name: &musl-build-pypy3
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:pypy3 \
--file musl/pypy3/Dockerfile musl/pypy3
#+end_src
*** file
#+name: &python-targets-common
#+begin_src dockerfile
ARG USE_PYTHON_TARGETS  # use if there are issues with mismatched python targets
# can't use PYTHON_TARGETS directly because ARG PYTHON_TARGETS is the same
# as export PYTHON_TARGETS= which means that portageq results will be affected

# we defer changing python targets until after eselect-repo to avoid
# issues bootstrapping pypy3
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
sh -c '\
USE_PYTHON_TARGETS=${USE_PYTHON_TARGETS:-"$(portageq envvar PYTHON_TARGETS) pypy3_11"} \
&& [[ -z ${USE_PYTHON_TARGETS} ]] || \
   echo "*/* PYTHON_TARGETS: -* ${USE_PYTHON_TARGETS}" >> /etc/portage/package.use/00-base\
'
#+end_src

#+begin_src dockerfile :tangle ./musl/pypy3/Dockerfile
FROM tgbugs/musl:updated
# FIXME python targets to include pypy3 needs to be in its own derived environment
# starting from package builder or something like that
<<&python-targets-common>>

RUN \
ln -s pypy3_x /etc/portage/patches/dev-python/pypy3_10 \
&& ln -s pypy3_x-exe /etc/portage/patches/dev-python/pypy3_10-exe \
&& ln -s ../dev-python/pypy3_x /etc/portage/patches/dev-lang/pypy:3.10

# FIXME /emerge-fail and &archive-or-rm have a very bad interaction
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q -uDN @system @world eselect-python \
   --getbinpkg \
   --keep-going \
   --exclude sys-process/procps \
|| echo $? > /emerge-fail \
<<&archive-or-rm>>

# fail if emerge fails but for buildkit ensure that we do it in such a
# way that we can truncate further steps and create a debug image
RUN \
test ! -e /emerge-fail

# XXX temporary until upstream fixes missing symlink/eselect pypy
RUN \
ln -s pypy3.11 /usr/bin/pypy3
#+end_src

#+begin_comment
Trying to bootstrap pypy3 at this phase of the build when you don't have
a pypy3 free environment yet can be a massive pain to debug. The solution
is to set =USE_PYTHYON_TARGETS= explicitly to override the default pypy3,
bootstrap everything until you can run =builder-debug= and then build the
binary pypy3 in an environment where you can debug.
#+end_comment

#+begin_comment
FIXME I think that we really have to make the pypy3 environment its
own branch because bootstrapping it is beyond terrifying when you have
to use the same images and the builder can deposit pyp3, otherwise you
wind up building pypy3 in an environment where binpkgs are not being
generated until the whole image is repackaged (!!!!)  SIGH

so here is the issue that we run into, we have multiple different
profiles that we construct for each base image and we keep those
profiles separate so that we don't impact all images when only a
derived layer changes, basically we want to have a single profile, but
we don't want docker to stupidly rebuild everything just becuase some
file that is irrelevant to the current image lineage changed,
unfortunately we need our base profile in order to build a pypy3
environment correctly so the base profile has to be added directly
after updated in order for this to work, maybe we can get it to go

XXX ignore the rant above, eselect-profile already sets our base
profile!  so we don't have to worry about any of this and can just add
an interposting layer
#+end_comment

*** patches
If you load up =builder-debug= you can emerge pypy3 by adding patches
manually, the right thing to do is to update the musl overlay build,
but for now, if you can manage to manually build in the builder you
will wind up with a binpkg that can be reused.

A number of relevant issues
- https://bugs.python.org/issue21622
- https://github.com/python/cpython/pull/18380
- https://bugs.python.org/issue43112
- https://github.com/gentoo/musl/issues/451#issuecomment-1017102775

**** cpython 2.7 patch
Sourced from https://git.alpinelinux.org/aports/plain/main/python3/musl-find_library.patch
If in =builder-debug= rebuild the python:2.7 binpkg =emerge -g n -k n python:2.7=.
#+begin_src diff :tangle ./docker-profile/base/musl-find_library.patch
diff -ru Python-2.7.12.orig/Lib/ctypes/util.py Python-2.7.12/Lib/ctypes/util.py
--- Python-2.7.12.orig/Lib/ctypes/util.py	2016-06-26 00:49:30.000000000 +0300
+++ Python-2.7.12/Lib/ctypes/util.py	2016-11-03 16:05:46.954665040 +0200
@@ -204,6 +204,41 @@
         def find_library(name, is64 = False):
             return _get_soname(_findLib_crle(name, is64) or _findLib_gcc(name))
 
+    elif True:
+
+        # Patched for Alpine Linux / musl - search manually system paths
+        def _is_elf(filepath):
+            try:
+                with open(filepath, 'rb') as fh:
+                    return fh.read(4) == b'\x7fELF'
+            except:
+                return False
+
+        def find_library(name):
+            from glob import glob
+            # absolute name?
+            if os.path.isabs(name):
+                return name
+            # special case for libm, libcrypt and libpthread and musl
+            if name in ['m', 'crypt', 'pthread']:
+                name = 'c'
+            elif name in ['libm.so', 'libcrypt.so', 'libpthread.so']:
+                name = 'libc.so'
+            # search in standard locations (musl order)
+            paths = ['/lib', '/usr/local/lib', '/usr/lib']
+            if 'LD_LIBRARY_PATH' in os.environ:
+                paths = os.environ['LD_LIBRARY_PATH'].split(':') + paths
+            for d in paths:
+                f = os.path.join(d, name)
+                if _is_elf(f):
+                    return os.path.basename(f)
+
+                prefix = os.path.join(d, 'lib'+name)
+                for suffix in ['.so', '.so.*']:
+                    for f in glob('{0}{1}'.format(prefix, suffix)):
+                        if _is_elf(f):
+                            return os.path.basename(f)
+
     else:
 
         def _findSoname_ldconfig(name):
#+end_src

**** pypy3-exe sys time patch
The patch is a version of the below patch that will apply correctly to later versions of pypy3.
<https://raw.githubusercontent.com/gentoo/musl/master/dev-python/
pypy3-exe/files/pypy3-exe-7.3.0-musl-compat-include-sys-time.patch>

#+begin_src diff :tangle ./docker-profile/base/musl-include-sys-time.patch :mkdirp yes
diff -r 9ef55f6fc369 pypy/module/cpyext/include/pytime.h
--- a/pypy/module/cpyext/include/pytime.h
+++ b/pypy/module/cpyext/include/pytime.h
@@ -2,6 +2,10 @@
 #ifndef Py_PYTIME_H
 #define Py_PYTIME_H
 
+#ifndef MS_WINDOWS
+#include <sys/time.h>
+#endif
+
 #include <pyconfig.h> /* include for defines */
 #include "object.h"
 
#+end_src

**** pypy3-exe stdio patch
The patch is a version of the below patch that will apply correctly to later versions of pypy3.
<https://raw.githubusercontent.com/gentoo/musl/master/dev-python/
pypy3-exe/files/pypy3-exe-7.3.0-musl-compat-fix-stdio-defs.patch>

#+begin_src diff :tangle ./docker-profile/base/musl-fix-stdio-defs.patch :mkdirp yes
--- a/rpython/rlib/rfile.py
+++ b/rpython/rlib/rfile.py
@@ -123,11 +123,11 @@
 c_ferror = llexternal('ferror', [FILEP], rffi.INT)
 c_clearerr = llexternal('clearerr', [FILEP], lltype.Void)
 
-c_stdin = rffi.CExternVariable(FILEP, 'stdin', eci, c_type='FILE*',
+c_stdin = rffi.CExternVariable(FILEP, 'stdin', eci, c_type='FILE* const',
                                getter_only=True, declare_as_extern=False)
-c_stdout = rffi.CExternVariable(FILEP, 'stdout', eci, c_type='FILE*',
+c_stdout = rffi.CExternVariable(FILEP, 'stdout', eci, c_type='FILE* const',
                                 getter_only=True, declare_as_extern=False)
-c_stderr = rffi.CExternVariable(FILEP, 'stderr', eci, c_type='FILE*',
+c_stderr = rffi.CExternVariable(FILEP, 'stderr', eci, c_type='FILE* const',
                                 getter_only=True, declare_as_extern=False)
 
 
#+end_src

**** pypy3-exe trashcan patch
https://foss.heptapod.net/pypy/pypy/-/issues/3959
this does not work, it allows pycurl to build but running the code results in a fatal error in pypy at runtime
note that pycurl is only a =test= dependency for kombu and tornado so the is not strictly necessary right now
#+begin_src diff :tangle ./docker-profile/base/pypy3-trashcan.patch :mkdirp yes
--- a/pypy/module/cpyext/include/object.h
+++ b/pypy/module/cpyext/include/object.h
@@ -395,6 +395,8 @@
 #define Py_TRASHCAN_SAFE_END(pyObj)   ; } while(0);
 /* note: the ";" at the start of Py_TRASHCAN_SAFE_END is needed
    if the code has a label in front of the macro call */
+#define Py_TRASHCAN_BEGIN(op, dealloc) do {
+#define Py_TRASHCAN_END   ; } while(0);
 
 /* Copied from CPython ----------------------------- */
#+end_src
**** pypy3 json string patch
Not all instance of string have =__radd__= methods that make uncasted
string concatenation safe. This results in divergent behavior compared
to the cypthon json implementation.
#+begin_src diff :tangle ./docker-profile/base/pypy3-json-str-subclass-safety.patch :mkdirp yes
diff -r 05fbe3aa5b08 lib-python/3/json/encoder.py
--- a/lib-python/3/json/encoder.py	Tue Mar 29 08:15:20 2022 +0300
+++ b/lib-python/3/json/encoder.py	Fri Apr 29 15:19:41 2022 -0700
@@ -371,8 +371,10 @@
                 first = False
             else:
                 buf = separator
-            if isinstance(value, str):
+            if type(value) == str:
                 yield buf + '"' + self.__encoder(value) + '"'
+            elif isinstance(value, str):
+                yield buf + '"' + str(self.__encoder(value)) + '"'
             elif value is None:
                 yield buf + 'null'
             elif value is True:
@@ -448,8 +450,10 @@
                 yield item_separator
             yield '"' + self.__encoder(key) + '"'
             yield self.key_separator
-            if isinstance(value, str):
+            if type(value) == str:
                 yield '"' + self.__encoder(value) + '"'
+            elif isinstance(value, str):
+                yield '"' + str(self.__encoder(value)) + '"'
             elif value is None:
                 yield 'null'
             elif value is True:
#+end_src

**** DONE patchelf musl patch
Temporary until main tree is updated. See https://bugs.gentoo.org/860888.
No longer needed since it was fixed by upstream.
#+begin_src diff :tangle ./docker-profile/base/patchelf-musl-no-dt-mips-xhash.patch :mkdirp yes :tangle no
diff --git a/src/patchelf.cc b/src/patchelf.cc
index 5dd320d..3a4fd50 100644
--- a/src/patchelf.cc
+++ b/src/patchelf.cc
@@ -1184,10 +1184,12 @@ void ElfFile<ElfFileParamNames>::rewriteHeaders(Elf_Addr phdrAddress)
                 // some binaries might this section stripped
                 // in which case we just ignore the value.
                 if (shdr) dyn->d_un.d_ptr = (*shdr).get().sh_addr;
+            #ifdef __GLIBC__
             } else if (d_tag == DT_MIPS_XHASH) {
                 // the .MIPS.xhash section was added to the glibc-ABI
                 // in commit 23c1c256ae7b0f010d0fcaff60682b620887b164
                 dyn->d_un.d_ptr = findSectionHeader(".MIPS.xhash").sh_addr;
+            #endif
             } else if (d_tag == DT_JMPREL) {
                 auto shdr = tryFindSectionHeader(".rel.plt");
                 if (!shdr) shdr = tryFindSectionHeader(".rela.plt");
#+end_src
** xorg
# FIXME why is this not being built from binpkg only? is it for layer
# efficiency?
*** run
#+name: &musl-run-xorg
#+begin_src screen
# -v ~/files/binpkgs/musl:/var/cache/binpkgs \
docker run \
--volumes-from local-repos-snap \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
-it tgbugs/musl:xorg
#+end_src
debug
#+begin_src screen
docker run \
--net host \
--add-host local.binhost:127.0.0.1 \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
--rm \
-it \
tgbugs/musl:xorg
#+end_src
*** build
#+name: &musl-build-xorg
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:xorg \
--file musl/xorg/Dockerfile musl/xorg
#+end_src

*** file
The really good news here is that portage ignores packages that were
built with mismatched use flags, so at the end of the day what we will
wind up with is a case where only packages with mismatched flags will
be built and deposited into musl-x. The less good news is that this is
not fully implemented yet as noted in <https://wiki.gentoo.org/wiki/
Binary_package_guide#Pulling_packages_from_a_binary_package_host>.

If you need to work around a broken ~USE="-harfbuzz -truetype"~ case
build them in a container rather than a builder and then quickpkg them
and then run the builder again.

#+name: &xorg-nox-common-1
#+begin_src dockerfile
# ARG PROFILE_IMAGE=tgbugs/musl:profile-x
# FIXME switch to use pypy3 profile instead probably ??? or do we do that in the pypy3 image probably?
ARG START_IMAGE=tgbugs/musl:pypy3

#FROM ${PROFILE_IMAGE} AS profile_image

FROM ${START_IMAGE}

# XXX noweb COPY --from= AFTER this point!
#+end_src

#+name: &xorg-nox-common-2
#+begin_src dockerfile
# XXX noweb COPY --from= BEFORE this point!

ARG PROFILE=docker-profile:tgbugs/musl/x

# reminder: you will see scary errors here because /var/db/repos is not mounted
RUN \
eselect profile set $PROFILE \
&& env-update

# cut here if you need to quickpkg to avoid some brokeness
#+end_src

#+name: &xorg-nox-common-3
#+begin_src dockerfile
# FIXME I think we have to update binhosts here

# FIXME this rebuild is bad because it results in duplication of
# rebuilt packages between layers, probably need updated-x
# XXX install freetype without harfbuzz first to avoid the circular dependency (sigh)
# also have to install harfbuzz -freetype as well https://bugs.gentoo.org/830966#c5
# XXX NOTE when harfbuzz and freetype are installed from binpkgs sometimes fontconfig
# will scream about missing libs, this is because the good harfbuzz is installed after
# fontconfig, confusing and scary, but apparently not fatal

# FIXME may need to break this out into a binpkg builder ala what we now do for openjdk
# that can be use by any profile that modifies useflags on these and thus needs a rebuild
# e.g. openjdk-nox pulls these all in, and only by sheer luck have they already been built
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -1q \
   --getbinpkgonly \
   media-libs/freetype \
   media-libs/harfbuzz \
# have to set -cairo here otherwise harfbuzz -truetype will fail to build
|| USE="-cairo -harfbuzz -truetype" emerge -1q \
   --getbinpkg \
   media-libs/freetype \
   media-libs/harfbuzz \
# FIXME need to quickpkg the minimal harfbuzz here, re: note above
# remind me again why were we using -j1 here? old gcc issues?
&& emerge -q -uDN @world \
   --getbinpkg \
   --exclude sys-process/procps \
   --keep-going \
|| echo $? > /emerge-fail \
<<&archive-or-rm>>

RUN \
test ! -e /emerge-fail

RUN \
eselect fontconfig disable 10-hinting-slight.conf \
&& eselect fontconfig enable \
   10-no-sub-pixel.conf \
   57-dejavu-sans.conf \
   57-dejavu-sans-mono.conf
#+end_src

#+begin_src dockerfile :tangle ./musl/xorg/Dockerfile
<<&xorg-nox-common-1>>
<<&profile-x>>
<<&xorg-nox-common-2>>
<<&xorg-nox-common-3>>
#+end_src

# FIXME 10-hinting-slight.conf no longer exists now ???

The issues with freetype hinting are partially dealt with in the
profile because so many packages pull in freetype, we have to deal
with the issue globally. We deal with some lingering issues here.

Only enabling dejavu sans and disabling any and all hinting matters.
There isn't a way to disable antialiasing using the gentoo fontconfig
and even if you do the disabled hinting engine has different and ugly
behavior compared to =-cleartype-hinting= so not sure what is going on
for even further insanity if you enable =10-hinting-none.conf= OR
=10-unhinted.conf= *YOU WILL GET HINTING !?!?!??! WAT!?* or at least
maybe AA is enabled which does not maybe ANY sense. Probably there is
some logic which is that in order to disable some feature there is
some default that is enabled so there winds up being a difference
between there being no reference to a feature and a reference to it to
explicitly disable it. Sigh.
** nox
*** run
#+name: &musl-run-nox
#+begin_src screen
docker run \
--volumes-from local-repos-snap \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
-it tgbugs/musl:nox
#+end_src
debug
#+begin_src screen
docker run \
--net host \
--add-host local.binhost:127.0.0.1 \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
--rm \
-it \
tgbugs/musl:nox
#+end_src
*** build
TODO we start from updated here not from pypy3, there is a pypy3/nox profile, but this image is not the pypy3/nox image
#+name: &musl-build-nox
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:nox \
--build-arg PROFILE='docker-profile:tgbugs/musl/nox' \
--build-arg START_IMAGE='tgbugs/musl:updated' \
--file musl/nox/Dockerfile musl/nox
#+end_src
# --build-arg PROFILE_IMAGE='tgbugs/musl:profile-nox' \
*** file
#+begin_src dockerfile :tangle ./musl/nox/Dockerfile
<<&xorg-nox-common-1>>
<<&profile-nox>>
<<&xorg-nox-common-2>>
<<&xorg-nox-common-3>>

# FIXME make this modification correctly in the profile images
# either via unmask or by confining cgc/bc/cs racket to another lineage
RUN \
sed -i '/racket/d' /etc/portage/package.mask/profile

RUN \
sed -i '/racket/d' /etc/portage/package.accept_keywords/profile

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q -uDN @world \
   --getbinpkg \
   --exclude sys-process/procps \
   --keep-going \
|| echo $? > /emerge-fail \
<<&archive-or-rm>>

RUN \
test ! -e /emerge-fail
#+end_src
** openjdk
*** run
debug
#+begin_src screen
docker run \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-it tgbugs/musl:openjdk-nox
#+end_src
*** build
#+name: &musl-build-openjdk
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:openjdk \
--file musl/openjdk/Dockerfile musl/openjdk
#+end_src

#+name: &musl-build-openjdk-nox
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:openjdk-nox \
--build-arg START_IMAGE='tgbugs/musl:nox' \
--file musl/openjdk/Dockerfile musl/openjdk
#+end_src

*** file
inherits from tgbugs/musl:xorg
TODO handle the nox case
#+begin_src dockerfile :tangle ./musl/openjdk/Dockerfile
ARG START_IMAGE=tgbugs/musl:xorg

FROM ${START_IMAGE}

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -1q \
   --getbinpkgonly \
   dev-java/openjdk \
# remove openjdk-bin mask for just this bootstrap if openjdk is not present
|| { sed -i "/openjdk-bin/d" /etc/portage/package.mask/profile \
;  emerge -q -uDN dev-java/openjdk-bin \
   --getbinpkg \
   --keep-going;} \
&& emerge -q -uDN openjdk \
   --getbinpkg \
   --keep-going \
|| echo $? > /emerge-fail \
<<&archive-or-rm>>

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -1q \
   --getbinpkg \
   --nodeps \
   virtual/jdk virtual/jre \
|| echo $? > /emerge-fail \
<<&archive-or-rm>>

RUN \
test ! -e /emerge-fail
#+end_src
** package-builder
*** populate 0
Yes it is kind of annoying to fully split the packages here when many of them don't actually
change, but I don't have an easy way to detect when it is safe to symlink a nox build into
the X build, though I think we can create a processes that would check the packages and to
see whether they have identical metadata and then remove one and symlink the other ....


A brief note on various =bindist= warnings that may appear during this step.

For =openssh= and =openssl=, the issue is related to various patents on ECC and RC5.
As far as I can tell from https://en.wikipedia.org/wiki/ECC_patents and the reference
in https://en.wikipedia.org/wiki/RC5, these patents have all expired, so redistribution
of packages compiled with =-bindist= is not an issue.

For =freetype= it seems that most of the patents https://freetype.org/patents.html
have expired as well. The latest ebuild in the tree has removed bindist entirely.

# these two are technically x/nox agnostic
#+name: &musl-run-updated-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:updated
#+end_src

#+name: &musl-run-pypy3-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:pypy3
#+end_src


# it is safe to use --include-config here becauseit is done before any modifications are made
# FIXME TODO need a way to ignore existing exact matches unless we override
#+name: &musl-run-xorg-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:xorg
#+end_src

#+name: &musl-run-openjdk-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:openjdk
#+end_src

**** quickpkg dedupe
This more or less works to avoid duplicate packages in a binhost multi
instance setup.
#+header: :shebang "#!/usr/bin/env python" :tangle-mode (or #o0755)
#+begin_src jupyter-python :session pys :tangle ./bin/quickpkg-new :mkdirp yes
import portage
from portage.versions import _pkg_str
from portage.gpkg import gpkg

def contents_csums(contents):
    return sorted([l.split()[2] for l in contents.split('\n') if l.startswith('obj')])

def pkg_csums(bintree, atom):
    # XXX can use from portage.binpkg import get_binpkg_format
    # to displatch on package tyupe if we really need it
    pkg = gpkg(settings=bintree.settings, gpkg_file=bintree.pkgdir + '/' + atom._metadata['PATH'])
    _contents = pkg.get_metadata('CONTENTS')
    if _contents:
        contents = _contents.decode()
        return contents_csums(contents)
    else:
        object()  # don't match anything

def atoms_to_package():
    eroot = portage.settings['EROOT']
    trees = portage.db[eroot]
    bintree = trees['bintree']
    vartree = trees['vartree']
    vardb = vartree.dbapi

    installed = vartree.dbapi.cpv_all()
    packaged, not_packaged, misses = [], [], {}
    for i in installed:
        bt, bi, use, contents = vardb.aux_get(i, ['BUILD_TIME', 'BUILD_ID', 'USE', 'CONTENTS'])
        bt = int(bt) if bt else -1
        bi = int(bi) if bi else -1
        csums = contents_csums(contents)
        # yes build id has issues for some reason that I don't entirely understand
        matches = [a for a in bintree.dbapi.match(i) if
                   (a._metadata['USE'] == use and
                    (a.build_time == bt or
                     #a.build_id == bi and
                     csums == pkg_csums(bintree, a)))]
        if matches:
            packaged.append(i)
        else:
            misses[i] = [a for a in bintree.dbapi.match(i) if not
                         (a._metadata['USE'] == use and
                          (a.build_time == bt or
                           #a.build_id == bi and
                           csums == pkg_csums(bintree, a)))]
            not_packaged.append(i)
            #[[l.build_id for l in [k] + v] for k, v in misses.items()]
            #[[l.build_time for l in [k] + v] for k, v in misses.items()]
            #[[l._metadata['USE'] if hasattr(l, '_metadata') else None
              #for l in [k] + v] for k, v in misses.items()]

    return sorted([f'={a}' for a in not_packaged])

if __name__ == '__main__':
    atp = atoms_to_package()
    if atp:
        print(*atoms_to_package())
    # else don't print anything because we check if output is the empty
    # string and then avoid running quickpkg to stop the logspam
#+end_src

*** run
#+begin_src bash
function build_package () {
echo docker run \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
--rm \
tgbugs/musl:package-builder \
$@
}
#+end_src


#+begin_src bash
build_package sh -c "USE=-harfbuzz emerge -1q freetype"
# and here we see why I keep harfbuzz out of the nox profile
build_package sh -c "emerge -1q freetype"
#+end_src

# TODO it is almost certainly worth keeping these containers around
# and stashing them because they can be used to build more packages
# without having to do a full reinstall, which still takes awhile

# TODO figure out how to properly archive distfiles and binpkgs

# FIXME there is a nasty issue here with composability for use flag
# changes in the profile, in all likelihood we would be better off
# maintaining a stack layers on the builder to update the use flags
# independent of the profile until we we know that we have to do a
# full rebuild, simply because rebuilding build images from scratch
# every time is still slow and adding new packages will almost
# inevitably reveal issues that require such use changes many should
# go in the profile because we know that we are always going to need
# those in the future, it should be fairly straight forward to create
# a /var/db/docker-profile -> /etc/portage translator for the builder

I suggest adding all the =_path_= variables below (and the repo name)
to your shell rc file if you use any of the docker run commands during
development so that you can do so in a new shell. These values are
usually stable per system.
# watch out with the use of a=${a:-b} the value of a will persist
# in the environment when you change the default value
# mitigated in part by prefixing all input variables with _in
# that doesn't quite get all the way to the solution we want
# but it prevents bad variables lingering
#+name: &builder-vars
#+begin_src bash :noweb yes
_path_dockerfiles=${_in_path_dockerfiles:-<<&host-dockerfiles-path()>>}
_path_binpkgs_root=${_in_path_binpkgs_root:-<<&host-binpkgs-root-path()>>}
_binpkgs_repo_name=${_in_binpkgs_repo_name:-<<&host-binpkgs-repo-name()>>}
_path_binpkgs=${_in_path_binpkgs:-${_path_binpkgs_root}/${_binpkgs_repo_name}}
_path_distfiles=${_in_path_distfiles:-<<&host-distfiles-path()>>}
_path_distcc_hosts=${_in_path_distcc_hosts:-<<&host-distcc-hosts-path()>>}
_path_ssh=${_in_path_ssh:-<<&host-ssh-path()>>}

_tm_pb=${_in_tm_pb:-tgbugs/musl:package-builder}
_tm_s_pb=${_in_tm_s_pb:-tgbugs/musl:static-package-builder}
_tm_n_pb=${_in_tm_n_pb:-tgbugs/musl:package-builder-nox}

_tm_pbs=${_in_tm_pbs:-${_tm_pb}-snap}
_tm_s_pbs=${_in_tm_s_pbs:-${_tm_s_pb}-snap}
_tm_n_pbs=${_in_tm_n_pbs:-${_tm_n_pb}-snap}

_tg_pb=${_in_tg_pb:-tgbugs/gnu:package-builder}

_tg_pbs=${_in_tg_pbs:-${_tg_pb}-snap}

_host_features="$({ command -v portageq > /dev/null && { portageq envvar FEATURES | grep -o distcc; }; } || true)"
_host_makeopts="$({ command -v portageq > /dev/null && portageq envvar MAKEOPTS; } || { command -v nproc > /dev/null && echo "-j$(nproc)"; } || echo ${NUMBER_OF_PROCESSORS};)"
#+end_src

#+name: &builder-resnap
#+begin_src bash
function builder-resnap () {
docker run ${_tm_pb}
docker commit $(docker ps -lqf ancestor=${_tm_pb}) ${_tm_pbs}
}
# FIXME SIGH SIGH SIGH why is this easier than doing the right thing
function static-builder-resnap () {
docker run ${_tm_s_pb}
docker commit $(docker ps -lqf ancestor=${_tm_s_pb}) ${_tm_s_pbs}
}
function nox-builder-resnap () {
docker run ${_tm_n_pb}
docker commit $(docker ps -lqf ancestor=${_tm_n_pb}) ${_tm_n_pbs}
}
function gnu-builder-resnap () {
docker run ${_tg_pb}
docker commit $(docker ps -lqf ancestor=${_tg_pb}) ${_tg_pbs}
}
#+end_src

#+name: &container-check
#+begin_src bash
function gnu-container-check () {
docker container inspect local-repos-snap > /dev/null || \
docker create -v /var/db/repos --name local-repos-snap tgbugs/repos:latest /bin/true
if [ -n "${_use_podman}" ]; then
[ -n "$(podman container ls -q --filter name=local-repos-snap --filter status=initialized)" ] || \
podman container init local-repos-snap
fi
}

function container-check () {
gnu-container-check

# FIXME need to check that the cross image exists sigh make
#docker container inspect cross-sbcl > /dev/null || \
#docker create -v /sbcl --name cross-sbcl tgbugs/musl:cross-sbcl /bin/true
}
#+end_src

# FIXME currently the host sets /etc/distcc/hosts and mounts it
# host discovery for distcc will take more work ... in particular
# port mapping via ssh, the other possibility is that we just
# add a utility that can configure a container image to update
# itself with current settings or something
# FIXME FEATURES and MAKEOPTS are also set assuming the host is gentoo

# FIXME need to mount /var/tmp/portage as a ramdisk most times
# FIXME need a pre-test to ensure that the distcc config points to hosts that
# are actually accessible and/or that we are running with network host
# the issue was actually that the ssh connections timed out and reset
# so there were no hosts and distcc was segfaulting as a result, fun bug
# --volumes-from cross-sbcl \
#+name: &builder-args
#+begin_src bash :noweb yes
<<&builder-args-gnu>>
#+end_src

# FIXME cross build repos missing proper configuration for compression and archive type (even though there aren't very many packages)
#+name: &builder-args-gnu
#+begin_src bash
--volumes-from local-repos-snap \
-v ${_path_dockerfiles}:/dockerfiles \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-v ${_path_ssh}:/var/lib/portage/home/.ssh \
-v ${_path_distcc_hosts}:/etc/distcc/hosts \
-v ${_path_binpkgs_root}/cross/gnu/x86_64-pc-linux-musl:/usr/x86_64-pc-linux-musl/var/cache/binpkgs \
-v ${_path_binpkgs_root}/cross/gnu/x86_64-gentoo-linux-musl:/usr/x86_64-gentoo-linux-musl/var/cache/binpkgs \
-v ${_path_dockerfiles}/bin:/usr/local/bin \
-v ${_path_dockerfiles}/musl/package-builder/accept_keywords:/etc/portage/package.accept_keywords/tweaks \
--env FEATURES="${_host_features}" \
--env MAKEOPTS="${_host_makeopts}" \
#+end_src

# --network host \
# network host is required to get distcc working on here due to the port forwarding
# hilariously or depressingly network host is still the only sane solution
# https://stackoverflow.com/q/17770902

#+name: &builder-bootstrap
#+begin_src bash :noweb yes
function builder-bootstrap () {
local _builder CODE CHOST
_builder=${_tm_pbs}
CHOST=$(docker run --volumes-from local-repos-snap --rm ${_builder} portageq envvar CHOST)
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y -q --keep-going --usepkg @builder
<<&docker-run-suffix>>

for target in {x86_64-pc-linux-gnu,x86_64-pc-linux-musl}; do
if [ ${target} != ${CHOST} ]; then
<<&docker-run-prefix>>
crossdev --stage4 --stable --portage --usepkg --target ${target}
<<&docker-run-suffix>>
fi
done

}

# FIXME SIGH copy paste
function static-builder-bootstrap () {
local _builder CODE
_builder=${_tm_s_pbs}
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y -q --keep-going --usepkg @builder
<<&docker-run-suffix>>
}

# FIXME SIGH copy paste
function nox-builder-bootstrap () {
local _builder CODE
_builder=${_tm_n_pbs}
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y -q --keep-going --usepkg @builder
<<&docker-run-suffix>>
}

# FIXME SIGH copy paste
function gnu-builder-bootstrap () {
local _builder CODE CHOST
_builder=${_tg_pbs}
CHOST=$(docker run --volumes-from local-repos-snap --rm ${_builder} portageq envvar CHOST)
gnu-container-check
docker run \
<<&builder-args-gnu>>
${_builder} \
emerge --color=y --with-bdeps=y -q --keep-going --usepkg @builder
<<&docker-run-suffix>>

for target in {x86_64-pc-linux-gnu,x86_64-pc-linux-musl}; do
if [ ${target} != ${CHOST} ]; then
gnu-container-check
docker run \
<<&builder-args-gnu>>
${_builder} \
crossdev --stage4 --stable --portage --usepkg --target ${target}
<<&docker-run-suffix>>
fi
done

}
#+end_src

# TODO distcc test
#+begin_src bash
function test_distcc () {
local test_host
for test_host in $(distcc --show-hosts); do
echo ${test_host}
DISTCC_HOSTS=${test_host} distcc x86_64-gentoo-linux-musl-gcc -c test.c -o test.o -v
echo
done
}
#+end_src

#+name: distcc-test-file
#+begin_src c
#include <stdio.h>
int main(void)
{
	printf("Hello world\n");
	return 0;
}
#+end_src

#+name: &docker-run-prefix
#+begin_src bash :noweb yes
container-check
docker run \
<<&builder-args>>
${_builder} \
#+end_src

#+name: &docker-run-suffix
#+begin_src bash :noweb yes
CODE=$?
docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=${_builder}) ${_builder}
[ ${CODE} -eq 0 ] || return ${CODE}
#+end_src

#+name: &builder-axes
#+begin_src bash
local _builder _axis

_axis=${1}

if [ "${_axis}" = "static" ]; then
_builder=${_tm_s_pbs}
elif [ "${_axis}" = "nox" ]; then
_builder=${_tm_n_pbs}
elif [ "${_axis}" = "gnu" ]; then
_builder=${_tg_pbs}
else
_builder=${_tm_pbs}
fi

echo builder ${_builder}
#+end_src

# TODO crossdev aarch64-unknown-linux-gnu-emerge
FIXME sometimes this fails to update a package with slotted deps or something
#+name: &builder-world
#+begin_src bash :noweb yes
function builder-world () {
<<&builder-axes>>
local CODE

echo get targets
# get targets
container-check
targets=$(\
docker run \
--rm \
<<&builder-args>>
${_builder} \
sh -c 'set -o pipefail; emerge --color=n --with-bdeps=y -q -uDN -p @docker | { grep "\(^\[ebuild\|acct-\)" || true; } | cut -b17- | awk '"'"'{ print "="$1 }'"'")
# XXX acct- is a workaround for https://bugs.gentoo.org/890777
# FIXME it seems that sometimes this will fail on packages that need to be rebuilt and then
# the binpkg only will fail due to incorrect slot deps or something until we reemerge ???

CODE=$?
[ ${CODE} -eq 0 ] || return ${CODE}

[ "${targets}" ] || return 0  # we're done here

targets_ghc=$(echo ${targets} | awk -v RS='\\s' '/^=dev-lang\/ghc/')
targets_user=$(echo ${targets} | awk -v RS='\\s' '/^=acct-user/')
targets_group=$(echo ${targets} | awk -v RS='\\s' '/^=acct-group/')
targets_nogruser=$(echo ${targets} | awk -v RS='\\s' '!/^=acct-(group|user)/')

[ "${targets_ghc}" ] && {
echo ghc targets
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n -1 -q --keep-going ${targets_ghc}
<<&docker-run-suffix>>

<<&docker-run-prefix>>
emerge --color=y -q --keep-going ${targets_ghc}
<<&docker-run-suffix>>

# FIXME workaround for ghc PDEPEND !ghcbootstrap? haskell-updater
<<&docker-run-prefix>>
emerge --color=y -q --keep-going haskell-updater
<<&docker-run-suffix>>

}

[ "${targets_group}" ] && {
echo only acct-group targets  # XXX avoid --nodeps order issues
# FIXME this is currently broken if a dependency and a dependent are in the same list
# because of the behavior described in https://bugs.gentoo.org/902207
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y --nodeps -1 -q --keep-going -u ${targets_group}
<<&docker-run-suffix>>

}

[ "${targets_user}" ] && {
echo only acct-user targets  # XXX avoid --nodeps order issues
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y --nodeps -1 -q --keep-going -u ${targets_user}
<<&docker-run-suffix>>

}

[ "${targets_nogruser}" ] && {
echo only targets_nogruser bdeps
# FIXME for some reason there are sporadic package rebuilds that are being missed here
# e.g. due to slot depends changes
# FIXME for some reason this fails to pull in the user packages needed by e.g. redict thus the manual merge above
<<&docker-run-prefix>>
emerge --load-average 2.5 --color=y --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n --onlydeps-with-ideps=y -q --keep-going -uDN ${targets_nogruser}
<<&docker-run-suffix>>

echo only targets not acct-group
<<&docker-run-prefix>>
bash -c "source /dockerfiles/bin/workflow-funs.sh; batch-emerge-nodeps '${targets_nogruser}'"
<<&docker-run-suffix>>

}

# if we get to the even if there are no targets
# then we should not return the value of [ "${targets_nogruser}" ]
return 0

}

function batch-emerge-nodeps () {
    local to_run n one_failed
    one_failed=0
    to_run=()
    for atom in ${@}; do
        to_run+=(${atom})
        ((n++))
        if [ $((n % <<&emerge-jobs()>>)) -eq 0 ]; then
            emerge --color=y --with-bdeps=y --nodeps -1 -q --keep-going -u ${to_run[@]}
            CODE=$?
            if [ ${CODE} -ne 0 ]; then one_failed=${CODE}; fi
            to_run=()
        fi
    done

    if [ ${#to_run[@]} -ne 0 ]; then
        emerge --color=y --with-bdeps=y --nodeps -1 -q --keep-going -u ${to_run[@]}
        CODE=$?
        if [ ${CODE} -ne 0 ]; then one_failed=${CODE}; fi
    fi
    return ${one_failed}
}

# FIXME SIGH code dupe
# TODO install sbcl for system bootstrap if it is missing
function static-builder-world () {
   builder-world static
}

function nox-builder-world () {
   builder-world nox
}

function gnu-builder-world () {
   # FIXME incorrectly pulls in the cross-sbcl volume
   builder-world gnu
}
#+end_src

#+name: &builder-arb
#+begin_src bash :noweb yes
function builder-run () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tm_pbs}
<<&docker-run-prefix>>
"${@}"
<<&docker-run-suffix>>
}

function gnu-builder-run () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tg_pbs}
<<&docker-run-prefix>>
"${@}"
<<&docker-run-suffix>>
}

function static-builder-run () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tm_s_pbs}
<<&docker-run-prefix>>
"${@}"
<<&docker-run-suffix>>
}

function nox-builder-run () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tm_n_pbs}
<<&docker-run-prefix>>
"${@}"
<<&docker-run-suffix>>
}

function builder-smart-live-rebuild () {
local _builder CODE
_builder=${_tm_pbs}
<<&docker-run-prefix>>
sh -c 'rebuilds=$(live-changed); if [ -n "${rebuilds}" ]; then { emerge --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n --onlydeps-with-ideps=y --color=y -q --keep-going ${rebuilds} && emerge --color=y -q --keep-going --usepkg=n --nodeps ${rebuilds}; } fi'
<<&docker-run-suffix>>
}

function builder-smart-also-live-rebuild () {
local _builder CODE
_builder=${_tm_pbs}
<<&docker-run-prefix>>
sh -c 'rebuilds=$(live-changed also); if [ -n "${rebuilds}" ]; then { { echo ${rebuilds} | sed "s/ /\n/g" | sed "s/$/ **/" > /etc/portage/package.accept_keywords/99-live-tmp; } && emerge --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n --onlydeps-with-ideps=y --color=y -q --keep-going ${rebuilds} && emerge --color=y -q --keep-going --usepkg=n --nodeps --buildpkgonly ${rebuilds}; } fi; rm /etc/portage/package.accept_keywords/99-live-tmp'
<<&docker-run-suffix>>
}

function static-builder-smart-live-rebuild () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tm_s_pbs}
<<&docker-run-prefix>>
sh -c 'rebuilds=$(live-changed); if [ -n "${rebuilds}" ]; then { emerge --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n --onlydeps-with-ideps=y --color=y -q --keep-going ${rebuilds} && emerge --color=y -q --keep-going --usepkg=n --nodeps ${rebuilds}; } fi'
<<&docker-run-suffix>>
}

function builder-arb () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tm_pbs}
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y -q --keep-going --usepkg=n \
"${@}"
<<&docker-run-suffix>>
}
# XXX FIXME code dupe
function static-builder-arb () {
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
local _builder CODE
_builder=${_tm_s_pbs}
<<&docker-run-prefix>>
emerge --color=y --with-bdeps=y -q --keep-going --usepkg=n \
"${@}"
<<&docker-run-suffix>>
}

function builder-license () {
<<&builder-axes>>

local CODE
# FIXME don't actually need to mount sets/license since it is in the builder image now since sets are safe, but might be useful
# what we really need is to mount the accepted license file
# XXX don't forget to rebuild portage manually to get the patch applied so we don't have to mount the accpeted license file at this stage XXX sigh, not actually fixed yet so we do have to mount at this step
container-check
docker run \
<<&builder-args>>
-v ${_path_dockerfiles}/musl/package-builder/sets/license:/etc/portage/sets/license \
-v ${_path_dockerfiles}/musl/package-builder/package.license:/etc/portage/package.license \
${_builder} \
emerge --color=y --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n -q --keep-going -uDN @license
<<&docker-run-suffix>>

container-check
docker run \
<<&builder-args>>
-v ${_path_dockerfiles}/musl/package-builder/sets/license:/etc/portage/sets/license \
-v ${_path_dockerfiles}/musl/package-builder/package.license:/etc/portage/package.license \
${_builder} \
emerge --color=y -q --keep-going --buildpkgonly -uDN @license
<<&docker-run-suffix>>
}
#+end_src

# XXX seems like cross emerge really does not work at all as desired ???
# well this is somewhat annoying https://wiki.gentoo.org/wiki/Cross_build_environment
# grrr the system should be able to use the host packages, yes I know that there are
# surely many build tools that have bad assumptions baked in about the host environment
# matching the target environment ... but sigh
# XXX LOL the answer was simple, --nodeps DUH!
# the other part of the solution is to add ** to ACCEPT_KEYWORDS
# but it may need to be se in the cross env make.conf
# yeah ... unfortunately for things like sbcl ... there is no way
# because the ebuild authors would have had to anticipate this
# and it is insanely hard to test stuff like this, I make the modification
# in sbcl.env that is needed to get it to work, but still ...
# just symlink it to the cross env portage/env, and then we are to our
# usual determine-endianness issues with host vs target
#+name: &cross-builder-arb
#+begin_src bash :noweb yes
function cross-aarch64-gnu-builder-arb () {
container-check
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
docker run \
<<&builder-args>>
--env USE='-*' \
--env ACCEPT_KEYWORDS='**' \
${_tm_pbs} \
aarch64-unknown-linux-gnu-emerge --color=y --with-bdeps=y -q --keep-going \
--usepkg=n --nodeps --buildpkgonly \
${@}

}
#+end_src

There are some packages such as =dev-lang/go= and some cross compiles
that require elevated privs in order to build otherwise they try to
call =process_vm_readv= then die.

See https://github.com/gentoo/gentoo-docker-images/issues/98 and
https://github.com/moby/moby/issues/1916.

#+name: &builder-arb-priv
#+begin_src bash :noweb yes
function builder-arb-priv () {
local _builder CODE
_builder=${_tm_pbs}
container-check
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
docker run \
--security-opt seccomp=unconfined \
<<&builder-args>>
${_builder} \
emerge --color=y --with-bdeps=y -q --keep-going --usepkg=n \
${@}
<<&docker-run-suffix>>
}
# FIXME code dupe
function static-builder-arb-priv () {
local _builder CODE
_builder=${_tm_s_pbs}
container-check
# rebuild packages modified without revbump e.g. due to changing /etc/portage/patches
docker run \
--security-opt seccomp=unconfined \
<<&builder-args>>
${_builder} \
emerge --color=y --with-bdeps=y -q --keep-going --usepkg=n \
${@}
<<&docker-run-suffix>>
}
#+end_src
# dev-lisp/sbcl cross compile

# ah the irony of docker dependencies needing ISE:do_peekstr:process_vm_readv
# due to using go-md2man and thus being unsafe to build in a sandboxed docker container
#+name: &musl-run-build-need-priv
#+begin_src bash
builder-arb-priv -1 -uN --usepkg \
dev-lang/go \
dev-go/go-md2man \
app-containers/runc \
app-containers/containerd \
app-containers/docker-cli
#+end_src

# --network host \
#+name: &builder-debug
#+begin_src bash :noweb yes
function builder-debug () {
container-check
docker run \
--privileged \
<<&builder-args>>
"${@}" \
-it ${_tm_pbs}

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=${_tm_pbs}) ${_tm_pbs}
}
function builder-debug-nowrite () {
container-check
docker run \
--privileged \
<<&builder-args>>
"${@}" \
-it ${_tm_pbs}
}
# XXX FIXME code dupe
function static-builder-debug () {
container-check
docker run \
--privileged \
<<&builder-args>>
"${@}" \
-it ${_tm_s_pbs}

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=${_tm_s_pbs}) ${_tm_s_pbs}
}
function nox-builder-debug () {
container-check
docker run \
--privileged \
<<&builder-args>>
"${@}" \
-it ${_tm_n_pbs}

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=${_tm_n_pbs}) ${_tm_n_pbs}
}
# SIGH
function gnu-builder-debug () {
gnu-container-check
docker run \
--privileged \
<<&builder-args-gnu>>
"${@}" \
-it ${_tg_pbs}

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=${_tg_pbs}) ${_tg_pbs}
}
#+end_src

#+begin_src bash
# --nodeps # potentially useful

@live-rebuild

app-misc/screen
dev-lisp/sbcl


# to debug issues
docker run \
--volumes-from local-repos-snap \
--rm \
-it \
tgbugs/musl:package-builder-snap

# too many issues, just merge and get on with it
# the lack of separation between build time dependencies and runtime is quite annoying
# that or the dependency trees are even worse than I thought
# emerge --color=y -q --keep-going --onlydeps
# emerge --color=y -q --keep-going --buildpkgonly
#+end_src

TODO gnu builder cross

*** build
# FIXME cp -r is a hack for the time being, patches should be source more sanely
# cp -r patches/ musl/package-builder/
#+name: &musl-build-package-builder
#+begin_src screen
docker build \
--tag tgbugs/musl:package-builder \
--file musl/package-builder/Dockerfile musl/package-builder
#+end_src

*** file
# TODO distcc
# COPY patches /etc/portage/patches
binpkg-changed-deps=y causes way too many big things to rebuild
way too frequently, however, if it is not enabled there are certain
hard to debug failures that are the result of the fact that there is
not currently a way to indicate that certain packages must be rebuild
when a given dependency changes ... portage does have the ability to
approximate this behavior  but I don't think it has the exact feature?
#+name: &musl-package-builder-common
#+begin_src dockerfile
COPY --from=tgbugs/common:portage-maven / /

ADD repo_name /var/db/crossdev/profiles/repo_name
ADD layout.conf /var/db/crossdev/metadata/layout.conf
ADD crossdev.conf /etc/portage/repos.conf/crossdev.conf
# ADD sbcl.env /etc/portage/env/dev-lisp/sbcl
# FIXME need a better way to update the world file/docker set i.e. mount at runtime
ADD world /etc/portage/sets/docker
ADD sets/builder /etc/portage/sets/builder
ADD sets/license /etc/portage/sets/license
ADD sets/live /etc/portage/sets/live
ADD sets/also-live /etc/portage/sets/also-live
# ADD accept_keywords.temp /etc/portage/package.accept_keywords/temp  # XXX mount in docker run not here

RUN \
echo 'FEATURES="${FEATURES} buildpkg"' >> /etc/portage/make.conf \
&& echo 'EMERGE_DEFAULT_OPTS="${EMERGE_DEFAULT_OPTS} --binpkg-changed-deps=y --usepkg"' >> /etc/portage/make.conf

#+end_src
REMINDER if you need something installed in a builder add it to the
=@builder= set for that builder e.g. [[set-builder-musl]].  Doing it
there avoids 1st/nth issues with trapped packages needing quickpkg.

We don't add distcc until we get to the builder here to avoid issues
during bootstrap. Usually we don't have too many packages to rebuild
to get to a sane world state.

#+begin_src dockerfile :tangle ./musl/package-builder/Dockerfile
FROM tgbugs/musl:xorg

<<&musl-package-builder-common>>
#+end_src

*** ghc
**** fetch
Bootstrapping ghc is too hard for regular use right now.
The workaround on new systems is to fetch existing binpkgs.

#+name: &musl-run-ghc-fetch-etc
#+begin_src bash
ghc-fetch-flow-host
#+end_src

#+name: &ghc-funs
#+begin_src bash :tangle ./bin/ghc-funs.sh
function ghc-fetch-flow-host () {
./source.org fetch-ghc || return $?
docker run \
--volumes-from local-repos-snap \
-v ${_path_dockerfiles}:/dockerfiles \
-v ${_path_binpkgs}:/var/cache/binpkgs \
--rm \
tgbugs/musl:updated \
bash -c "source /dockerfiles/bin/ghc-funs.sh; ghc-fetch-flow-container;"
}

function ghc-fetch-flow-container () {
# FIXME TODO fail if not in container
local f fn fixhost
[ -d /var/cache/binpkgs/dev-lang/ghc ] || mkdir -p /var/cache/binpkgs/dev-lang/ghc
for f in $(ls /dockerfiles/retfile/cache/*/*-ghc*); do
fn=$(basename "${f}")
[ -f "/var/cache/binpkgs/dev-lang/ghc/${fn:65}" ] || { cp "${f}" /var/cache/binpkgs/dev-lang/ghc/"${fn:65}"; fixhost=1; };
done
[ -z "${fixhost}" ] || emaint binhost --fix
}
#+end_src
**** bootstrap
FIXME issue with needing to install the prior version to bootstrap the next
ideally we would be able to test for that
FIXME when ghc gets updated we have to reinstall all haskell packages and rebuild them
what a nightmare, fortunately just pandoc and dot2graphml for now, but that is 163 packages
that we have to rebuild, sometimes haskell-updater will miss some packages and you will
have to rebuild them manually
#+begin_src bash
emerge -q -j12 app-text/pandoc-cli dev-haskell/dot2graphml shellcheck
emerge -q --usepkg=n haskell-updater
haskell-updater --all --no-deep -- --usepkg=n
#+end_src
# must emerge -1q =dev-lang/ghc-{previous-revision} prior to bootstrap
# emerge ghc --onlydeps into builder, and then use the builder image as a starting point
# start from the builder snapshot but do not overwrite the snapshot when we're done i.e. use builder-debug-nowrite
# check here for latest version: https://pkgs.alpinelinux.org/package/edge/community/x86_64/ghc
# FIXME except ... that 9.4.4 seeminly can't bootstrap 9.0.2 ... what utter nonsense
#+begin_src bash :noweb yes
container-check
docker run \
--privileged \
<<&builder-args>>
--rm -it ${_tm_pb}
#+end_src
see https://pkgs.alpinelinux.org/packages?name=ghc&repo=&arch=&maintainer=
#+begin_src bash
# curl -L -o ghc-9.0.2-r1.tar.bz http://dl-cdn.alpinelinux.org/alpine/edge/community/x86_64/ghc-9.0.2-r1.apk
# tar xvzf ghc-9.0.2-r1.tar.bz  # yes this is crazy, throw the container away when when are done
#curl -L -o ghc-9.4.4-r1.tar.bz http://dl-cdn.alpinelinux.org/alpine/edge/community/x86_64/ghc-9.4.4-r1.apk
#curl -L -o ghc-${ghc_version}.tar.bz http://dl-cdn.alpinelinux.org/alpine/v3.19/community/x86_64/ghc-${ghc_version}.apk
#ghc_version=9.4.7-rl;alpine_branch=v3.19  # 9.0.2-r4 fail
#ghc_version=9.4.4-r1;alpine_branch=v3.18  # 9.0.2-r4 fail
ghc_version=9.0.2-r1;alpine_branch=v3.17  # 9.0.2-r4 ok
curl -L -o ghc-${ghc_version}.tar.bz http://dl-cdn.alpinelinux.org/alpine/${alpine_branch}/community/x86_64/ghc-${ghc_version}.apk
tar xvzf ghc-${ghc_version}.tar.bz  # yes this is crazy, throw the container away when when are done

# have to force the fix by setting chost so we can use our old package (don't lose it!) XXX this doesn't work EITHER
# CHOST=x86_64-gentoo-linux-musl emerge -1q dev-lang/ghc --usepkg
# unset CHOST

# finally biuld the package
FEATURES=-distcc emerge -1q dev-lang/ghc
#+end_src

since we are lucky enough to have an existing ghc xpak file from another system we can use that sigh
technically can skip the emerge dev-lang/ghc bit if the versions match ...
#+begin_src screen
docker run \
-v ~/files/binpkgs/ghc-9.0.2-r4-1.xpak:/tmp/multi/dev-lang/ghc/ghc-9.0.2-r4-1.xpak \
-e PKGDIR=/tmp/multi \
--privileged \
<<&builder-args>>
"${@}" \
-it ${_tm_pbs} \
sh -c "emerge -K =dev-lang/ghc-9.0.2-r4; unset PKGDIR; emerge dev-lang/ghc; quickpkg dev-lang/ghc"
#+end_src

+and then we have to modify all the top level entry point scripts to adjust their install location+
give up and for the bootstrap just extract the alpine build straight into the existing file system
because we can't set the prefix

but even here the Winline errors are showing up during the build maybe
they aren't fatal in non-crossbuild settings?

haha! this works! we are good to go, lots of file collisions of
course, but at least we have the binpkg now this means we aren't going
to try to get ghc-cross working for the time being because kicking off
from alpline seems to be working well enough for one auxillary package
**** old
https://github.com/redneb/ghc-alt-libc/releases/download/ghc-9.0.2-musl/ghc-9.0.2-x86_64-unknown-linux-musl.tar.xz
Have to roll with =USE=ghcbootstrap= but mercifully this makes it extremely easy to just plob the binaries on
the system and get to work.
also no distcc
TODO follow sbcl pattern here and stick the stuff we need in =/ghc= probably and provide as a separate image?
#+begin_src bash
pushd /tmp/
PV=9.0.2
curl -OL https://github.com/redneb/ghc-alt-libc/releases/download/ghc-${PV}-musl/ghc-${PV}-x86_64-unknown-linux-musl.tar.xz
tar xvJf ghc-${PV}-x86_64-unknown-linux-musl.tar.xz
pushd ghc-${PV}
./configure --prefix=/ghc
make install
popd
popd
#+end_src

#+begin_src ebuild :tangle ./musl/package-builder/ghc.env
pre_pkg_setup() { export PATH="${PATH}:/ghc/bin"; }
#+end_src

using this on musl fails because ghc-cabal is segfaulting for some reason
https://github.com/NixOS/nixpkgs/issues/118731, the solution is to use a
version of ghc that can target musl 1.2.3, i.e. the one from alpine

*** sbcl bootstrap :old:
Since the addition of the =system-bootstrap= use flag we cross compile
from gnu to musl and emerge the cross compiled binpkg once and then we
are good to go, though may need to emerge the old version of sbcl when
we have to update to a new version.

The gentoo ebuilds for sbcl retrieve an existing binary for bootstrapping.
Due to the fact that the current EAPI (?) is not libc aware for precompiled
binaries we would have to create and maintain a binary for the musl overlay.
Modifying =src_unpack= is a more expedient solution.
#+name: &sbcl-env
#+begin_src ebuild :tangle ./musl/package-builder/sbcl.env
src_unpack() {
	unpack ${A}
	[ -d /sbcl ] && {
		einfo "Using /sbcl for bootstrap"
		cp -r /sbcl sbcl-binary || die;
		cp -a ${S}/run-sbcl.sh sbcl-binary/ || die;
	} || {
	command -v sbcl && {
		einfo "Using local sbcl found at $(command -v sbcl) for bootstrap"
		local bin_core_home;
		IFS=',' read -r -a bin_core_home <<< $(sbcl --noinform --no-sysinit --no-userinit --eval \
		'(progn (format t "~a,~a,~a" sb-ext:*runtime-pathname* sb-ext:*core-pathname* (sb-int:sbcl-homedir-pathname)))' --quit) || die;
		mkdir -p sbcl-binary/src/runtime || die;
		mkdir -p sbcl-binary/output || die;
		mkdir -p sbcl-binary/obj/sbcl-home || die;
		cp -a ${bin_core_home[0]} sbcl-binary/src/runtime/ || die;
		cp -a ${bin_core_home[1]} sbcl-binary/output/ || die;
		cp -a ${bin_core_home[2]}/contrib sbcl-binary/obj/sbcl-home/contrib || die;
		cp -a ${S}/run-sbcl.sh sbcl-binary/ || die;
	} } ||
	mv sbcl-*-* sbcl-binary || die
	cd "${S}"
}
#+end_src
*** crossdev
In order to fix
#+begin_example
 * Missing digest for '/var/db/docker-profile/cross-x86_64-pc-linux-gnu/binutils/binutils-2.34-r2.ebuild'
 * Missing digest for '/var/db/docker-profile/cross-x86_64-pc-linux-gnu/binutils/binutils-2.33.1-r1.ebuild'
#+end_example

This works around the fact that musl uses thin manifests.  See
https://wiki.gentoo.org/wiki/Custom_ebuild_repository#Crossdev.
#+name: &crossdev-repo_name
#+begin_src conf :tangle ./musl/package-builder/repo_name
crossdev
#+end_src

#+name: &crossdev-layout
#+begin_src conf :tangle ./musl/package-builder/layout.conf
masters = gentoo
thin-manifests = true
#+end_src

#+name: &crossdev-conf
#+begin_src conf :tangle ./musl/package-builder/crossdev.conf
[crossdev]
location = /var/db/crossdev
priority = 10
masters = gentoo
auto-sync = no
#+end_src

But even with that fix there is an issue with linking the core runtime libs.
#+begin_example
/usr/libexec/gcc/x86_64-pc-linux-gnu/ld: cannot find crti.o: No such file or directory
#+end_example

For reasons I do not fully understand we have to use the gentoo repo
as the source for the gcc ebuild, the two are virtually identical, so
maybe the toolchain eclass is silently different? Unknown.
#+begin_src bash
crossdev --stage4 --stable --target x86_64-pc-linux-gnu --ov-gcc /var/db/repos/gentoo
#+end_src

At this point we can attempt to emerge sbcl, but =src_config= will fail.
#+begin_src bash
x86_64-pc-linux-gnu-emerge -q sbcl
#+end_src

As a result, I reworked the profile so that it can support whatever
libc we want and do the cross build from gnu to musl since there are
distributed sbcl-binaries for gnu but not for musl. The way that
multiple libcs are implemented in gentoo right now seems to add
significant maintenance overhead due to ebuild duplication.
*** sets
**** builder
#+name: set-builder-common
#+begin_src conf
app-portage/gentoopm
#+end_src

#+name: set-builder-musl
#+begin_src conf :tangle ./musl/package-builder/sets/builder
<<set-builder-common>>
sys-devel/distcc
sys-devel/crossdev
#+end_src

*** world
#+name: world-package-builder
#+begin_src conf :tangle ./musl/package-builder/world
tgbugs-meta/package-builder-meta
#+end_src

#+name: world-package-builder-old
#+begin_src conf :tangle ./musl/package-builder/world
<<ident((dedupe-lines "world-package-builder-dupes"))>>
#+end_src

#+name: world-package-builder-dupes
#+begin_src conf
<<world-package-builder-nox>>
<<world-kg-release>>
<<world-kg-dev>>
<<world-tgbugs-dev>>
<<world-docker>>
<<world-interlex>>
<<world-sparcron>>
<<world-dynapad-base>>
<<world-package-builder-common-x>>
x11-base/xorg-server
x11-libs/gtk+
#+end_src

#+name: world-package-builder-common
#+begin_src conf
#+end_src

#+name: world-package-builder-common-x
#+begin_src conf
<<world-package-builder-common>>
app-editors/gvim
#+end_src

#+begin_src conf
media-libs/freetype
media-libs/fontconfig
media-fonts/dejavu
#+end_src

*** sets
**** license
#+name: set-license-package-builder-musl
#+begin_src conf :tangle ./musl/package-builder/sets/license
<<set-license-kg-dev>>
#+end_src

**** live
Always live (right now).
#+begin_src conf :tangle ./musl/package-builder/sets/live
dev-java/scigraph-bin
dev-java/robot-bin
dev-node/apinat-converter
dev-python/sparcur
dev-python/interlex
#+end_src

Also live.
#+begin_src conf :tangle ./musl/package-builder/sets/also-live
dev-python/augpathlib
dev-python/clifn
dev-python/htmlfn
dev-python/hyputils
dev-python/idlib
#dev-python/interlex
dev-python/neurondm
dev-python/nifstd-tools
dev-python/ontquery
dev-python/orthauth
dev-python/protcur
dev-python/pyontutils
dev-python/pysercomb
dev-python/scibot
#dev-python/sparcur
dev-python/sxpyr
dev-python/ttlser
#+end_src

Meta live.
#+begin_src conf :tangle ./musl/package-builder/sets/meta-live
tgbugs-meta/sparcron-meta
#+end_src

FIXME TODO check the ebuild in the package meta for changes against the tree
#+begin_src bash
builder-run sh -c 'ACCEPT_KEYWORDS="**" emerge --buildpkgonly --nodeps @meta-live'
#+end_src

FIXME not clear where this should go, but need a way to toggle
maybe do it by mounting the file to =/etc/portage/package.accept_keywords/00-live= when run?
emerge from package and then run smart live rebuild?
#+begin_src conf :tangle ./musl/package-builder/00-live
dev-python/*::tgbugs-overlay **
#+end_src

#+begin_src bash :noweb yes
function builder-live () {
<<&builder-axes>>

docker run \
<<&builder-args>>
-v ${_path_dockerfiles}/musl/package-builder/00-live:/etc/portage/package.accept_keywords/00-live \
-v ${_path_dockerfiles}/musl/package-builder/sets/live:/etc/portage/sets/live \
${_builder} \
emerge --color=y --with-bdeps=y --onlydeps --onlydeps-with-rdeps=n -q --keep-going -uDN @live
# NOTE if members of @live are also dependencies they will be pulled in, this is ok
# because they will be built only the first time and be binpkgs thereafter

docker run \
<<&builder-args>>
${_builder} \
emerge --color=y --nodeps -q --keep-going @live  # TODO use live-changed

}
#+end_src

#+header: :shebang "#!/usr/bin/env python" :tangle-mode (or #o0755)
#+begin_src python :tangle ./bin/live-changed :mkdirp yes
import sys
from _emerge.actions import load_emerge_config
ec = load_emerge_config()
if 'also' in sys.argv:
    lives = ec.target_config.sets['also-live'].getAtoms()
    sys.argv.remove('also')
else:
    lives = ec.target_config.sets['live'].getAtoms()

# FIXME better to do it from packages in general or from @docker
# and infer ... smart-live-rebuild doesn't help us here because it only
# works on packages that are already installed fortunately the package builder is the only
# on that needs to know which ebuilds are actually live

#from gentoopm import get_package_manager
#pm = get_package_manager()
#filt = 'dev-python/ttlser'
#pkgs = [pkg for pkg in pm.installed.filter(filt)]  # FIXME except we don't want installed we want in binpkgs
#if pkgs:
#    pkgs[0].environ

import bz2
from gentoopm import bash
import portage
from portage.gpkg import gpkg
import tempfile

bp = bash.get_any_bashparser()

eroot = portage.settings['EROOT']
trees = portage.db[eroot]
bintree = trees['bintree']
porttree = trees['porttree']

def load_byte_string(bp, byte_string):
    # see gentoopm.bash.bashserver
    with tempfile.NamedTemporaryFile('w+b') as f:
         f.write(byte_string)
         f.flush()
         f.file.seek(0)
         bp.load_file(f)

cp_data = {}
ebuilds = {}
maybe_rebuild = []
not_live = set()
for live in sorted(lives, reverse=True):
    atoms = list(bintree.dbapi.match(live))

    if not atoms:
        # no binpkg exists at all, but that doesn't mean that live is
        # actually newer than latest release
        maybe_rebuild.append(live)
        continue

    _9999_done = False
    _nl_done = False
    for atom in sorted(atoms, key=(lambda a: (a, a.build_id)), reverse=True):
        #p_atom, *_ = porttree.dbapi.match(atom)  # XXX might not need this if settings iuse effective is always live?
        # p_atom._settings._iuse_effective_match()  # may need to get this from porttree atoms instead of binpkg atoms?
        _cat, name, version, _rev = atom.cpv_split
        if atom.version == '9999' and not _9999_done:
            for uf in atom._metadata['USE'].split():
                if not atom._settings._iuse_effective_match(uf):
                    #print(f'USE mismatch? {atom} {uf}')  # TODO
                    pass

            _9999_done = True  # FIXME need to match USE flags so can skip mismatched packages
            atom.build_id
            key_ebuild = atom.cpv.split('/')[-1] + '.ebuild'

            pkg = gpkg(settings=bintree.settings, gpkg_file=bintree.pkgdir + '/' + atom._metadata['PATH'])

            _envbz2 = pkg.get_metadata('environment.bz2')
            env = bz2.decompress(_envbz2)
            #load_byte_string(bp, env)
            #with tempfile.NamedTemporaryFile('w+b') as f:
                #f.write(env)
                #f.flush()
                #f.file.seek(0)
                #bp.load_file(f)

            # XXX doesn't quite work because things like PV PN S defaults are missing due to ebuild lacking a name
            # but we can probably construct that from some of the other _metadata vars maybe?
            #bp('declare -x PV=9999')  # doesn't quite seem to work
            #bp(f'export PV={version}')
            # ebuild some.ebuild help ... very handy entrypoint
            #bp('PORTAGE_BIN_PATH=/usr/lib/portage/python3.11 source "${PORTAGE_BIN_PATH}"/ebuild.sh')
            #bp(f'function inherit () { :; }')
            #bp(f'export P={name}-{version} PN={name} PV={version} PR={_rev}')
            ebuild = pkg.get_metadata(key_ebuild)
            funs = b'\nfunction inherit () { :; }\nfunction pypi_sdist_url () { :; }\n'
            ff = env + b'\n' + funs + b'\n' + ebuild
            load_byte_string(bp, ff)  # very much does not work due to missing names right now
            #with tempfile.NamedTemporaryFile('w+b') as f:
                #f.write(ebuild)
                #f.flush()
                #f.file.seek(0)
                #bp.load_file(f)  # XXX somehow this severly breaks the bashproc ... ah well yeah bunch of undefined functions

            slot = bp['SLOT']
            egit_dir = bp['EGIT_DIR']
            egit_version = bp['EGIT_VERSION']
            egit_min_clone_type = bp['EGIT_MIN_CLONE_TYPE']
            egit_repo_uri = bp['EGIT_REPO_URI']
            egit_branch = bp['EGIT_BRANCH']

            S = bp['S']
            ebuilds[atom.cp] = key_ebuild, ebuild, slot, egit_dir, egit_version, egit_branch,
            cp_data[atom.cp] = (
                egit_dir,
                egit_version,
                #egit_min_clone_type,
                egit_repo_uri,
                egit_branch,
                S,
            )

            #egits = [l.strip() for l in env.decode().split("\n") if l.startswith('decl') and 'EGIT' in l]  # old way
        elif _9999_done:
            pass
        else:
            # collect to find the latest version(s) so we can check tags
            _nl_done = True
            not_live.add((atom, version, name))

    if not _nl_done:
        # case where no not live package has been built
        maybe_rebuild.append(atom)

    if not _9999_done:
        # no 9999 ebuild was found in binpkgs so check the tree for a live
        # ebuild since we need to build it (not even rebuild)
        maybe_rebuild.append(live)

rebuild = []
for mr_atom in maybe_rebuild:
    for p_atom in porttree.dbapi.match(mr_atom.cp):
        _cat, name, version, _rev = p_atom.cpv_split
        if version != '9999':
            not_live.add((p_atom, version, name))

    if mr_atom.cp not in cp_data:
        # we should go ahead and built it at least once
        # to simplify information retrieval in the future
        # the determination of which packages actually need
        # a live build is independent of whether we have actually
        # built one
        rebuild.append(mr_atom.cp)

condition = 'TODO'  # S is not default (currently this is always true), watch out for dev vs pre
cp_tag = {}
for natom, version, name in sorted(not_live, reverse=True):  # FIXME TODO make sure the sorting on the versions is newest to oldest
    condition = natom.cp in cp_data and cp_data[natom.cp][-1]  # XXX this will miss if we just haven't built before
    tag = version if condition else (name + '-' + version)
    if natom.cp not in cp_tag:
        cp_tag[natom.cp] = tag
    # git rev-list -n 1 ${TAG}
    # better, see, commits_since_last_release in release-next ... really need to move that somewhere augpathlib seems reasonable?

# a deep clone is required for all of these
# it probably makes sense to keep those repos somewhere else (i.e. not in distfiles) to keep things simple and separate from portage
# actually, it seems ok to keep these with portage and they can be used as a local source to clone user image dev repos

import shutil
import pathlib
import subprocess

ebuild_path = pathlib.Path('/tmp/rebuilds')
try:
    ebuild_path.mkdir(exist_ok=True)
    _em = ebuild_path / 'metadata'
    _em.mkdir(exist_ok=True)
    _lo = '''masters = gentoo
    thin_manifests = true
    '''
    with open((_em / 'layout.conf'), 'wt') as f:
        f.write(_lo)

    simple_unpack = b'\nfunction src_unpack () { git-r3_src_unpack; }\nfunction pkg_setup () { :; }\n'

    for cp, (efn, ebuild, slot, egit_dir, egit_version, egit_branch) in ebuilds.items():
        cpp = ebuild_path / cp
        cpp.mkdir(parents=True, exist_ok=True)
        nep = cpp / efn
        with open(nep, 'wb') as f:
            f.write(ebuild + simple_unpack)

        # use portage to sync the git-r3 repository, avoiding ssh key issues
        # the look at FETCH_HEAD to get the latest commit (tracks overrides)
        argv = 'ebuild', nep.as_posix(), 'digest', 'unpack', 'clean'
        p = subprocess.Popen(argv, stderr=sys.stderr, stdout=sys.stderr)
        out = p.communicate()

        egit_path = pathlib.Path(egit_dir)
        # g3_main from git-r3_fetch local_ref in git-r3.eclass
        g3_main = egit_path / 'refs/git-r3' / cp / slot / '__main__'
        with open(g3_main, 'rt') as f:
            # FIXME may need to use git symbolic-ref --quiet if not ref: refs/a/b
            g3_ref = f.read().split()[1]

        ref_path = egit_path / g3_ref
        with open(ref_path, 'rt') as f:
            new_commit = f.read().strip().split()[0]

        # XXX the way things work now it is not clear we need to do this
        # because if we are building live ebuilds then we don't care if
        # there is a non-live that is equivalent because there is no easy
        # way to say "only use live if live is newer than non-live"
        # there are other reasons we might want to use this though
        #tag = cp_tag[cp] if cp in cp_tag else None
        #if tag is not None:
        #    tag_path = egit_path / 'refs/tags' / tag
        #    if tag_path.exists():
        #        with open(tag_path, 'rt') as f:
        #            tag_commit = f.read().strip().split()[0]

        #        if tag_commit == new_commit:
        #            pass
        #        elif tag_commit == egit_version:
        #            pass
        #        else:
        #            # TODO git merge-base --is-ancestor
        #            pass

        # FIXME TODO use release-next to check that only the files
        # that are actually included in the package have changed
        if new_commit != egit_version:
            rebuild.append(cp)

finally:
    shutil.rmtree(ebuild_path)

print(' '.join(rebuild))
#+end_src

old approach that we no longer need since we use ebuild to fetch via git-r3
#+begin_src python
import augpathlib as aug

class PackagePath(aug.PackagePath):

    _tag = None

    @property
    def tag(self):
        return self._tag


rpp = aug.RepoPath('/tmp/git-repos')
rpp.mkdir(exist_ok=True)
for cp, (egit_dir, egit_version, egit_repo_uri, S) in cp_data.items():
    clp = rpp.clone_path(egit_repo_uri)
    if clp.exists():
        rp = clp
        #rp.pull()  # XXX TODO
    else:
        rp = rpp.clone_from(egit_repo_uri)  # XXX watch out for interlex cloning issues should be ok in principle if we run as portage maybe?

    pp = aug.PackagePath(rp / S.strip('/'))
    pp._tag = cp_tag[cp]  # FIXME handle case where there is only a live build and cp won't be in cp_tag
    if pp.commits_since_last_release():
        # TODO and then we have to check egit_version against as well
        if list(pp.commits(rev=f'HEAD...{egit_version}')):
            rebuild.append(cp)

print('\n'.join(rebuild))
#+end_src

** package-builder-nox
*** populate 0
#+name: &quickpkg-image
#+begin_src bash
function quickpkg-image () {
container-check  # FIXME what if we want to snap when we only have local-portage-snap?
docker run \
--volumes-from local-repos-snap \
-v "${_path_binpkgs}":/var/cache/binpkgs \
-v "${_path_dockerfiles}"/bin/quickpkg-new:/tmp/quickpkg-new \
--rm \
${1} \
sh -c 'set -o pipefail; { emerge --info 2>&1 | grep "Unable to parse profile" && { echo "bad profile, if using podman did you container init?"; exit 1; } } || _qpvar=$(/tmp/quickpkg-new); [ -z "${_qpvar}" ] || quickpkg ${_qpvar}'
}
#+end_src
Use =sh= without prefix to match other calls and
avoid issues with git bash on windows rewriting
=/bin/sh= -> =C:/Program Files/Git/bin/sh.exe=
and failing. Hacks, hacks all the way down. See
https://stackoverflow.com/a/68795519.

#+name: &musl-run-nox-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:nox
#+end_src

#+name: &musl-run-openjdk-nox-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:openjdk-nox
#+end_src

*** run
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
--rm \
tgbugs/musl:package-builder-nox \
emerge --color=y -q --keep-going @docker
#+end_src

*** sets
**** builder
#+name: set-builder-musl-nox
#+begin_src conf :tangle ./musl/package-builder/sets/nox.builder
<<set-builder-common>>
#+end_src

*** world
If there is a new package that one of your images needs add it here.
Yes, there are going to be issues with keywording that are likely going
to require updates to the profile followed by a rebuild here. I can't quite
remember whether binpkgs check use flags.
#+name: world-package-builder-nox
#+begin_src conf :tangle ./musl/package-builder/nox.world
tgbugs-meta/package-builder-meta
#+end_src

#+name: world-package-builder-nox-old
#+begin_src conf
<<world-debug>>
<<world-emacs>>
<<world-python>>
<<world-schemes>>
<<world-blazegraph>>
<<world-scigraph>>
<<world-package-builder-common>>
dev-lisp/sbcl
dev-scheme/racket
#+end_src
# <<world-dynapad-base>>
# dynapad definitely requires racket, also would have to go minimal racket for this

# requires a crossdev environment for this to work
#+name: world-lisp
#+begin_src conf
dev-lisp/sbcl
dev-lisp/clozurecl
dev-lisp/clisp
#+end_src

#+name: world-schemes
#+begin_src conf
dev-scheme/chicken
dev-scheme/guile
dev-scheme/gambit
#+end_src
# TODO build Chez from the Racket repo for unencumbered boot files
#+name: world-xemacs
#+begin_src conf
app-editors/xemacs
app-xemacs/xemacs-packages-all
#+end_src

*** build
#+name: &musl-build-package-builder-nox
#+begin_src screen
docker build \
--tag tgbugs/musl:package-builder-nox \
--file musl/package-builder/nox.Dockerfile musl/package-builder
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/package-builder/nox.Dockerfile
FROM tgbugs/musl:nox

<<&musl-package-builder-common>>

# overwrite the regular world file
ADD nox.world /etc/portage/sets/docker
ADD sets/nox.builder /etc/portage/sets/builder
#+end_src

** package-binhost
** distcc
*** run
#+begin_src screen
docker run \
--detach \
--add-host=host.docker.internal:host-gateway \
-v ${_path_dockerfiles}/musl/distcc/distccd.confd:/etc/conf.d/distccd \
-v ${_path_dockerfiles}/musl/distcc/entrypoints:/etc/entrypoints \
--entrypoint /etc/entrypoints/distcc-start \
-p 3632:3632 \
--rm \
tgbugs/musl:distcc-snap
#+end_src

#+begin_src screen
docker exec -it $(docker ps -lqf ancestor=tgbugs/musl:distcc-snap) tail -f /var/log/distcc/distccd.log
#+end_src

*** build
#+begin_src screen
function build-distcc-snap () {
container-check

docker run ${_tm_pb}
docker commit $(docker ps -lqf ancestor=${_tm_pb}) tgbugs/musl:distcc-snap

docker run \
<<&builder-args>>
tgbugs/musl:distcc-snap \
emerge  --color=y --with-bdeps=y -q --keep-going --usepkg \
sys-devel/distcc \
sys-devel/crossdev

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=tgbugs/musl:distcc-snap) tgbugs/musl:distcc-snap

for target in {x86_64-pc-linux-gnu,x86_64-pc-linux-musl}; do
docker run \
<<&builder-args>>
tgbugs/musl:distcc-snap \
crossdev --stage4 --stable --portage --usepkg --target ${target}

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=tgbugs/musl:distcc-snap) tgbugs/musl:distcc-snap
done
}
#+end_src
*** config
#+begin_src conf :tangle ./musl/distcc/distccd.confd
DISTCCD_EXEC="/usr/bin/distccd"
DISTCCD_PIDFILE="/var/run/distccd/distccd.pid"
DISTCCD_OPTS="--port 3632 --log-level notice"
DISTCCD_OPTS="${DISTCCD_OPTS} --log-file /var/log/distcc/distccd.log -N 19"
DISTCCD_OPTS="${DISTCCD_OPTS} --allow host.docker.internal/24"
DISTCCD_OPTS="${DISTCCD_OPTS} --allow 172.17.0.1/24"
DISTCCD_OPTS="${DISTCCD_OPTS} --allow 127.0.0.1/24"
DISTCCD_OPTS="${DISTCCD_OPTS} --allow 192.168.1.0/24"
#+end_src

*** entrypoints
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/distcc/entrypoints/distcc-start :mkdirp yes
mkdir /var/log/distcc
chown distcc:distcc /var/log/distcc  # FIXME move this to image creation also explains why this was always an issue
rc-status
touch /run/openrc/softlevel
/etc/init.d/distccd start
tail -f /dev/null
#+end_src

** binpkg-only
*** run
debug
#+begin_src screen
docker run \
--net host \
--add-host local.binhost:127.0.0.1 \
--volumes-from local-repos-snap \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
--rm \
-it tgbugs/musl:binpkg-only
#+end_src

*** build
#+name: &musl-build-binpkg-only
#+begin_src bash
docker build \
--tag tgbugs/musl:binpkg-only \
--file musl/binpkg-only/Dockerfile musl/binpkg-only
#+end_src

*** file
# wow parallel-install -ebuild-locks speeds things up quite a bit
# unfortunately they break acct-group and acct-user packages
# due to /etc/gshadow.lock contention which I think happends due
# to the -ebuild-locks feature because setting that allows
# unsandboxed steps to install concurrently
#+name: &musl-binpkg-only-common
#+begin_src dockerfile

RUN \
echo 'EMERGE_DEFAULT_OPTS="${EMERGE_DEFAULT_OPTS} --usepkgonly --getbinpkgonly"' >> /etc/portage/make.conf \
&& echo 'FEATURES="${FEATURES} parallel-install -ebuild-locks"' >> /etc/portage/make.conf
#+end_src

#+begin_src dockerfile :tangle ./musl/binpkg-only/Dockerfile
FROM tgbugs/musl:xorg
<<&musl-binpkg-only-common>>
#+end_src

** binpkg-only-nox
*** run
debug
#+begin_src screen
docker run \
--net host \
--add-host local.binhost:127.0.0.1 \
--volumes-from local-repos-snap \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
--rm \
-it tgbugs/musl:binpkg-only-nox
#+end_src

*** build
#+name: &musl-build-binpkg-only-nox
#+begin_src screen
docker build \
--tag tgbugs/musl:binpkg-only-nox \
--file musl/binpkg-only/nox.Dockerfile musl/binpkg-only
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/binpkg-only/nox.Dockerfile
FROM tgbugs/musl:nox
<<&musl-binpkg-only-common>>
#+end_src

** debug
*** world
#+name: world-debug
#+begin_src conf
app-editors/vim
app-portage/eix
dev-debug/gdb
#+end_src

** docker :bootstrap:
This image provides one route to bootstrap an environment that can
execute this file. Other routes also exist.

This route requires the following dependencies.
1. This =source.org= file.
2. Emacs 27 or later (earlier might work but not tested)
3. A posix shell
4. docker version 20 or later
5. Access to a gentoo stage 3 musl image.

It does not require git to be installed on the host so =source.org=
could be retrieved via curl or from a backup or similar.

With a bit of wrangling the bootstrap might also be able to drop the
Emacs dependency.

A second phase bootstrap is used to provide a stable starting point
for the rest of the process. This second phase does use git.

*** run
Reminder that this gives access to the host docker system.
#+begin_src bash
docker run \
-v /var/run/docker.sock:/var/run/docker.sock \
-it tgbugs/musl:docker
#+end_src

Version that works with existing package host folders.

#+begin_src bash
docker run \
-v /var/run/docker.sock:/var/run/docker.sock \
-v ~/files/binpkgs:/binpkgs \
-it tgbugs/musl:docker
#+end_src

After quite a bit of exploration it seems that passing =docker.sock=
is the sanest way to achieve what we want, though it does create a
strange warping of perspective because all containers run on the host
docker server. This is unfortunate because it causes the semantics to
differ between running the bootstrap outside of docker or trying to
run it "inside" of docker. See the dind heading above for more.
*** build
#+name: &musl-build-docker
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:docker \
--file musl/docker/Dockerfile musl/docker
#+end_src

*** entrypoints
0th
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/docker/entrypoint-0.sh :mkdirp yes
pushd dockerfiles
./source.org ${@}
#+end_src

1st
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/docker/entrypoint-1.sh :mkdirp yes
git clone https://github.com/tgbugs/dockerfiles.git
pushd dockerfiles
./source.org ${@}
#+end_src

*** file
# TODO daemon.json
#+begin_src dockerfile :tangle ./musl/docker/Dockerfile
<<&build-world>>
ADD source.org /dockerfiles/source.org
ADD entrypoint-0.sh /etc/entrypoint-0.sh
ENTRYPOINT ["/etc/entrypoint-0.sh"]
#+end_src

#+begin_src dockerfile :tangle ./musl/docker/nox.Dockerfile
<<&build-world-nox>>
ADD entrypoint.sh /etc/entrypoint.sh
ENTRYPOINT ["/etc/entrypoint.sh"]
#+end_src

*** world
NOTE =dev-lang/go= is pulled in by =app-emulation/docker= and must be
built using ref:&builder-arb-priv to avoid =process_vm_readv= being
blocked by the container.

#+name: world-docker
#+begin_src conf :tangle ./musl/docker/world
tgbugs-meta/docker-meta
#+end_src

#+begin_src conf
dev-vcs/git
app-misc/screen
app-editors/emacs
app-containers/docker
app-containers/docker-cli
app-containers/docker-buildx
#+end_src

Additional dependencies that are already present on a gentoo system
but might not be present in some other bootstrapping environments.
Ideally, only emacs and docker would be required, or even less.
However, we aren't quite there yet. Some processes are easier to
move into docker containers e.g. the sbcl patches, the work just
has to be done.
#+begin_src conf
app-alternatives/awk
app-shells/bash
dev-lang/python
net-misc/curl
sys-apps/coreutils
sys-apps/grep
sys-apps/portage
sys-apps/sed
#+end_src
- app-alternatives/awk
  - sbcl
  - targets
- sys-apps/coreutils
  - comm
    - sbcl
  - echo
    - top level
    - pull
  - cp
    - kg-release-user
  - rm
    - tangle
    - kg-release-user
  - mkdir
    - sbcl
    - kg-release-user
  - date
    - pull
  - cut
    - pull
- dev-vcs/git
  - sbcl (clone repo TODO FIXME by mounting helper-repos in a docker image)
  - clone this repo (not strictly necessary)
- dev-lang/python
  - package-server
- net-misc/curl
  - pull (stage3 latest)
  - package-server (check if running)
  - kg-release-user (sckan files)
- sys-apps/grep
  - pull
  - builder-bootstrap (for FEATURES)
  - targets
- sys-apps/portage
  - builder-bootstrap (for FEATURES)
  - targets
- sys-apps/sed
  - pull
  - sbcl

** testing-python
Python testing.
*** world
# gentoo no longer has 3.{6,7} in the main tree, going to be a pain test back there
# dev-lang/python:3.6
# dev-lang/python:3.7

# dev-lang/python:3.13t
# dev-lang/python:3.14t
# dev-lang/python:3.14
#+name: world-python
#+begin_src conf :tangle ./musl/testing-python/world
tgbugs-meta/python-testing-meta
#+end_src

#+begin_src conf
dev-lang/python:3.8
dev-lang/python:3.9
dev-lang/python:3.10
dev-lang/python:3.11
dev-lang/python:3.12
dev-lang/python:3.13
dev-lang/pypy:2.7
dev-lang/pypy:3.10
dev-lang/pypy:3.11
dev-python/pip
dev-python/pipenv
#+end_src
# XXX pipenv continues to be a toxic waste dump of insanity and brokeness
# https://bugs.gentoo.org/717666 really really bad call on my part for
# picking it back in 2018 because Pipfile seemed useful
** testing-emacs
Emacs testing.
*** world
# app-editors/emacs:23
# app-editors/emacs:24
# app-editors/emacs:25
#+begin_src conf :tangle ./musl/testing-emacs/world
tgbugs-meta/emacs-testing-meta
#+end_src

#+begin_src conf
app-editors/emacs:18
app-editors/emacs:26
app-editors/emacs:27
app-editors/emacs:28
app-editors/emacs:29
app-editors/emacs:30
#+end_src
** emacs
Emacs using the athena 3d toolkit to avoid pulling in gtk.
*** bugs
If you quickpkg emacs and then try to install it you can encounter
#+begin_example
mv: cannot stat '/var/tmp/portage/app-editors/emacs-27.2-r5/image/usr/share/info/emacs-27/dir.orig': No such file or directory
#+end_example
This is somewhat concerning since the failure is during preinst and it
definitely should not be looking in /var/tmp/portage for that orig
file. It seems that forcing a rebuild with builder-arb fixes the issue.
*** run
#+begin_src screen
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:emacs
#+end_src

debug run
#+begin_src screen
docker run \
--net host \
--add-host local.binhost:127.0.0.1 \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
--rm \
-it \
tgbugs/musl:emacs
#+end_src

If you see the following error you somehow forgot/are missing the musl overlay.
#+begin_example
Error loading shared library libbsd.so.0: No such file or directory (needed by /usr/lib/libICE.so.6)
Error loading shared library libbsd.so.0: No such file or directory (needed by /usr/lib/libXdmcp.so.6)
Error relocating /usr/lib/libICE.so.6: arc4random_buf: symbol not found
Error relocating /usr/lib/libXdmcp.so.6: arc4random_buf: symbol not found
#+end_example
*** test
#+name: &test-musl-emacs
#+begin_src screen
test-image tgbugs/musl:emacs
#+end_src

#+header: :shebang "#!/usr/bin/env sh\nset -e"
#+begin_src bash :tangle ./bin/test/tgbugs/musl/emacs/test :mkdirp yes :noweb yes
<<&bash-at-abs-path>>
HOME=/tmp/test-home
INIT_URL=https://raw.githubusercontent.com/tgbugs/orgstrap/master/init-simple.el

emacs --batch --quick --load test.el

/dockerfiles/source.org

if $(false); then  # extended testing
emacs --batch --quick --eval \
"(progn (setq ow-site-packages '(vterm zmq)) (url-handler-mode 1) (find-file (pop argv)) (eval-buffer))" \
"${INIT_URL}"
fi

#+end_src

#+begin_src elisp :tangle ./bin/test/tgbugs/musl/emacs/test.el :mkdirp yes :noweb yes
(require 'org)
(message "%s" emacs-version)
(message "ok")
(flush-standard-output)
;(error "expected to fail") ; FIXME a hack to ensure tests can fail
#+end_src

*** build
#+name: &musl-build-emacs
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:emacs \
--file musl/emacs/Dockerfile musl/emacs
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/emacs/Dockerfile
<<&build-world>>
#+end_src

#+begin_src dockerfile :tangle ./musl/emacs/nox.Dockerfile
<<&build-world-nox>>
#+end_src

*** world
# FIXME I think something in the emacs ebuild is broken because sometimes it fails to pull in libbsd???
#+name: world-emacs
#+begin_src conf :tangle ./musl/emacs/world
tgbugs-meta/emacs-meta
#+end_src

#+begin_src conf
app-emacs/vterm
app-emacs/zmq
app-editors/emacs
#+end_src

** icedtea
*** build
#+name: &musl-build-icedtea
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:icedtea \
--file musl/icedtea/Dockerfile musl/icedtea
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/icedtea/Dockerfile
<<&build-world>>
#+end_src
*** world
# FIXME BROKEN
#+name: world-icedtea-broken
#+begin_src conf :tangle ./musl/icedtea/world :tangle no
dev-java/icedtea-bin::musl
#+end_src

Backup.
#+name: world-icedtea
#+begin_src conf :tangle ./musl/icedtea/world
dev-libs/nss
x11-libs/libXcomposite
x11-libs/libXtst
dev-java/icedtea-bin::tgbugs-overlay
#+end_src
# back to musl since somehow my local setup is broken for the package builder
# and the musl repo is fixed again and I managed to pull everything down this time
# dev-java/icedtea-bin::local

# note the lack of tangle
#+name: world-icedtea-nox
#+begin_src conf
dev-libs/nss
dev-java/icedtea-bin::local
#+end_src

*** legacy
The musl overlay installs icedtea-bin correctly now so this is
+thankfully no longer needed+ only needed periodically.
#+name: &musl/icedtea/legacy
#+begin_src dockerfile :tangle ./musl/icedtea/legacy.Dockerfile
FROM tgbugs/musl:xorg

ARG ARCHIVE

ARG BASE="https://github.com/tgbugs/musl/releases/download/icedtea-bin-3.18.0-alpine-helper-0/"

ARG TMCH=34581ad0f14b5898abfb8d0a7ad89d560270a2e5

RUN \
eselect repository create local /usr/local/portage

# FIXME this is an evil hack that WILL expire
RUN \
mkdir -p /usr/local/portage/dev-java/icedtea-bin \
&& pushd /usr/local/portage/dev-java/icedtea-bin \
&& ln -s /var/db/repos/musl/dev-java/icedtea-bin/files \
&& curl -L -O "https://raw.githubusercontent.com/tgbugs/musl/${TMCH}/dev-java/icedtea-bin/icedtea-bin-3.18.0.ebuild" \
&& curl -L -O "https://raw.githubusercontent.com/tgbugs/musl/${TMCH}/dev-java/icedtea-bin/Manifest"

RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
emerge -q nss \
<<&archive-or-rm>>

RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
emerge -q dev-java/icedtea-bin::local --onlydeps \
<<&archive-or-rm>>

ARG SIGH="icedtea-bin-3.18.0-x86_64-musl.tar.gz \
icedtea-bin-3.18.0-dbg-x86_64-musl.tar.gz \
icedtea-bin-3.18.0-doc-x86_64-musl.tar.gz \
icedtea-bin-3.18.0-jre-base-x86_64-musl.tar.gz \
icedtea-bin-3.18.0-jre-lib-x86_64-musl.tar.gz \
icedtea-bin-3.18.0-jre-x86_64-musl.tar.gz \
icedtea-bin-3.18.0-libjpeg-x86_64-musl.tar.gz"

RUN --mount=from=docker.io/gentoo/portage:latest,source=/var/db/repos/gentoo,target=/var/db/repos/gentoo,rw \
pushd /var/cache/distfiles \
&& for SI in ${SIGH}; do curl -L -o "${SI}" "${BASE}${SI/-musl/}"; done \
&& popd \
&& emerge -q dev-java/icedtea-bin::local \
<<&archive-or-rm>>
#+end_src

# export failure=$(docker ps -lq)
# docker start $failure
# docker attach $failure

** protege
*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:protege
#+end_src

*** build
#+name: &musl-build-protege
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:protege \
--build-arg UID=<<&UID>> \
--file musl/protege/Dockerfile musl/protege
#+end_src

Due to the fact that protege needs X11 running in order to create
config files.  Run the following command, change the default reasoner
to ELK, make any other changes that are needed, and then quit protege.
The second command will run automatically and commit the changes.

NOTE you must run the =protege= command manually to prevent the commit
from changing the default behavior of the container from changing its
entry point to run =protege=.

#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:protege && \
docker commit $(docker ps -lq) tgbugs/musl:protege
#+end_src

*** world
#+name: world-protege
#+begin_src conf :tangle ./musl/protege/world
app-misc/protege-bin
dev-python/pip
x11-misc/xdg-user-dirs
#+end_src
*** file
We install pip during this step because any builds that =FROM
tgbugs/musl:protege= default to =protegeuser=.

#+name: &musl/protege
#+begin_src dockerfile :tangle ./musl/protege/Dockerfile
FROM tgbugs/musl:xorg AS builder

ARG ARCHIVE

WORKDIR /build

<<&user-skel-common>>

USER ${USER_NAME}

ARG HOME=/build/home/${USER_NAME}

WORKDIR $HOME

# paths to preferences files
ARG PATH_CFU_1=_\!\&\!\!\`g\"\>\!\&@\!\[@\"\(\!%\`\!\|w\"@\!\&\)\!\[@\"\'\!%\`\!\`g\"\&\!%4\!@w\"\&\!\&:=
ARG PATH_CFU_2=_\!\'%\!c\!\"w\!\'w\!a@\"j\!\'%\!d\!\"p\!\'8\!bg\"f\!\(\!\!cg\"l\!\'\}\!~@\"y\!\'\`\!bg\"j\!\'\`\!cw==
ARG PATH_CFU_3=_\!\'8\!cg\"n\!#4\!c\!\"y\!\'8\!d\!\"l\!\'c\!~@\!u\!\'\`\!~\!\"p\!\(@\!bw\"y\!#4\!\}w\"v\!\(\)\!~@\!u\!\(\`\!c\!\"k\!\'%\!d\!\"l\!#4\!\`\!\"s\!\(\`\!~w\"p\!\'4\!\^@\"h\!\'4\!\}@\"n\!\'\`\!cg==
ARG PATH_CFU="${PATH_CFU_1}/${PATH_CFU_2}/${PATH_CFU_3}"

# set preferences so that protege starts in the right state the first time
# protege doesn't create this prefs file by default so we would have to do this regardless
# this helps because it prevents the search for plugins on first run so that goes faster
RUN \
pushd ~/ \
&& mkdir -p ".java/.userPrefs/${PATH_DRI_1}" \
&& chmod 0700 ".java/.userPrefs" \
&& mkdir -p ".java/.userPrefs/${PATH_CFU}" \
&& echo '<?xml version="1.0" encoding="UTF-8" standalone="no"?>' > ".java/.userPrefs/${PATH_CFU}/prefs.xml" \
&& echo '<!DOCTYPE map SYSTEM "http://java.sun.com/dtd/preferences.dtd">' >> ".java/.userPrefs/${PATH_CFU}/prefs.xml" \
&& echo '<map MAP_XML_VERSION="1.0">' >> ".java/.userPrefs/${PATH_CFU}/prefs.xml" \
&& echo '  <entry key="CheckForUpdates" value="false"/>' >> ".java/.userPrefs/${PATH_CFU}/prefs.xml" \
&& echo '</map>' >> ".java/.userPrefs/${PATH_CFU}/prefs.xml" \
&& popd

FROM tgbugs/musl:binpkg-only

<<&build-world-common>>

COPY --from=builder /build /

<<&musl-file-user-base>>
#+end_src

Sadly this approach does not work because protege dies before the
reasoner prefs file is written.  Therefore we have to run the image
manually and commit before release. Sigh.
#+begin_src dockerfile
# start protege to generate settings files, have to sleep becuase the
# protege sh wrapper breaks $!
RUN \
protege \
& sleep 6 \
&& kill $(ps | grep java | awk '{ printf $1 }')

# on first run protege doesn't check to see if there is already
# something in this prefs.xml file and appends to it automatically
RUN \
find ~/.java/.userPrefs -name 'prefs.xml' -exec grep -q DEFAULT_REASONER_ID {} \; \
-exec sed -i 's/org.protege.editor.owl.NoOpReasoner/org.semanticweb.elk.elk.reasoner.factory/' {} \;

# must use absolute path otherwise command form won't work
WORKDIR /home/${USER_NAME}
#+end_src

In order to get paths that point to the prefs.xml files that we can
embed in the docker file you need the following commands.
#+begin_src bash
printf '%q' $(find ~/.java/.userPrefs -name 'prefs.xml' -exec grep -q CheckForUpdates {} \; -print0)
#+end_src

A useful find command for debugging whether the correct reasoner has been set.
#+begin_src bash
find ~/.java/.userPrefs -name 'prefs.xml' -exec grep -q DEFAULT_REASONER_ID {} \; -exec cat {} \;
#+end_src

** NIF-Ontology
*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:NIF-Ontology
#+end_src

*** build
# TODO progress prints to stderr
#+name: &musl-build-NIF-ontology
#+begin_src screen
docker build \
--tag tgbugs/musl:NIF-Ontology \
--file musl/NIF-Ontology/Dockerfile musl/NIF-Ontology
#+end_src

*** file
# FIXME composition with protege user issues I think the right way to
# do this is to move to having a single container user image that we
# build and then use COPY --from on that?
#+name: &musl/NIF-Ontology
#+begin_src dockerfile :tangle ./musl/NIF-Ontology/Dockerfile
FROM tgbugs/musl:protege

# phase three ontology
RUN \
pushd ~/ \
;   mkdir git \
;   pushd git \
;       git clone https://github.com/SciCrunch/NIF-Ontology.git \
;       pushd NIF-Ontology \
;           pushd ttl \
;           cp catalog-v001.xml.example catalog-v001.xml \
;       popd \
;   popd
#+end_src

** neurondm
*** run
#+begin_src bash
# to allow the container access to the local x session you have to run the following
xhost local:docker
# use xhost -local:docker to remove

docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:neurondm

docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--workdir /home/protegeuser/git/NIF-Ontology/ttl \
tgbugs/musl:neurondm \
protege
#+end_src

*** build
#+begin_src bash
docker build \
--tag tgbugs/musl:neurondm \
--build-arg ONTOLOGY_GITREF=neurons \
--file musl/neurondm/Dockerfile musl/neurondm
#+end_src

*** file
#+name: &musl/neurondm
#+begin_src dockerfile :tangle ./musl/neurondm/Dockerfile
FROM tgbugs/musl:NIF-Ontology

ARG ONTOLOGY_GITREF=neurons

# phase three ontology
RUN \
pushd ~/git/NIF-Ontology \
;   git checkout ${ONTOLOGY_GITREF} \
;   popd

# phase four python tools
RUN \
pushd ~/ \
;   pushd git \
;       git clone https://github.com/tgbugs/pyontutils.git \
;       pushd pyontutils \
;           pip install --user --break-system-packages -e . \
;           pushd neurondm \
;               pip install --user --break-system-packages -e . \
;           popd \
;       popd \
;   popd
#+end_src

** npo-1.0
*** run
#+begin_src bash
xhost local:docker

docker pull tgbugs/musl:npo-1.0

docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--workdir /home/protegeuser/git/NIF-Ontology/ttl \
tgbugs/musl:npo-1.0 \
sh -c 'protege ~/git/NIF-Ontology/ttl/npo.ttl'
#+end_src
**** macos notes
#+begin_src bash
brew install virtualbox  # there are some system level persmissions that you will need to set
brew install --cask docker
open -a Docker\ Desktop
# You will need to go to Docker Desktop > Preferences > Resources
# and increase the memory limit to 8 gigs
# otherwise oom killer will end Protege while trying to load npo.ttl

brew install xquartz
open -a XQuartz
# You will need to go to XQuartz > Preferences > Security
# and enable Allow connections from network clients
xhost +localhost
export DISPLAY=:0
# test to make sure everything still works e.g. by running xeyes

docker pull tgbugs/musl:npo-1.0
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=host.docker.internal$DISPLAY \
--workdir /home/protegeuser/git/NIF-Ontology/ttl \
tgbugs/musl:npo-1.0 \
sh -c 'protege ~/git/NIF-Ontology/ttl/npo.ttl'
#+end_src

Run the block above and once protege starts type =Control R= to run
the reasoner. The docker image is running the Linux version of Protege
so the key bindings use Control instead of Command. You can then run
OWL DL queries in the tab. Note that if you are using the ELK reasoner
(enabled by default in the image) then you will have to click through
a number of warning dialogues, this is normal.

*** build
#+begin_src bash
docker build \
--tag tgbugs/musl:npo-1.0 \
--build-arg ONTOLOGY_GITREF=npo-1.0 \
--file musl/neurondm/Dockerfile musl/neurondm
#+end_src

** npo-1.0-neurondm-build
*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--workdir /home/protegeuser/git/NIF-Ontology/ttl \
tgbugs/musl:npo-1.0-neurondm-build \
sh -c 'git stash && protege ~/git/NIF-Ontology/ttl/npo.ttl'
#+end_src
*** build
Build using the SciCrunch SciGraph API endpoint.
#+begin_src screen
# XXX note that NUID does nothing right now
docker build \
--tag tgbugs/musl:npo-1.0-neurondm-build \
--build-arg NEURONS_BRANCH=npo-1.0 \
--build-arg NUID=<<&UID>> \
--secret id=scigraph-api-key,src=<(echo export SCIGRAPH_API_KEY=$(python -c 'from pyontutils.config import auth; print(auth.get("scigraph-api-key"))')) \
--file musl/npo-1.0-neurondm-build/Dockerfile musl/npo-1.0-neurondm-build
#+end_src

Build using an alternate SciGraph API endpoint.
#+begin_src screen
# XXX note that NUID does nothing right now
docker build \
--tag tgbugs/musl:npo-1.0-neurondm-build \
--build-arg NEURONS_BRANCH=npo-1.0 \
--build-arg NUID=<<&UID>> \
--build-arg SCIGRAPH_API=$(python -c 'from pyontutils.config import auth; print(auth.get("scigraph-api"))') \
--secret id=scigraph-api-key,src=<(echo) \
--file musl/npo-1.0-neurondm-build/Dockerfile musl/npo-1.0-neurondm-build
#+end_src
# --build-arg SCIGRAPH_API=http://192.168.1.207:9000/scigraph \

*** file
# FIXME should probably be using a multi source file here instead of
# noweb but I'm not sure we can really do that because the output
# depends on the state of the ontology repo
#+name: &musl/neurondm-build
#+begin_src dockerfile :tangle ./musl/npo-1.0-neurondm-build/Dockerfile
FROM tgbugs/musl:npo-1.0
<<&-base-musl/neurondm-build>>
#+end_src

*** save
This is the image that will be archived to Zenodo for the paper. Note
that the dl queries will not run as expected on this unless you first
stash the changes in =~/git/NIF-Ontology=.

#+begin_src bash
docker save tgbugs/musl:npo-1.0-neurondm-build | gzip > /tmp/npo-1.0-neurondm-build.tar.gz
#+end_src

To restore from the archive run
#+begin_src bash
docker load --input npo-1.0-neurondm-build.tar.gz
#+end_src

The sha256 checksum for npo-1.0-neurondm-build.tar.gz on Zenodo at
doi:10.5281/zenodo.5033493 is
=8e0bb1c684ca8a28f1abeb01ef7aa2597388b8011244f097a92bdd2a523db102=.

** neurondm-build
This image runs the neurondm build process.
*** run
*** build
#+begin_src screen
# XXX note that NUID does nothing right now
docker build \
--tag tgbugs/musl:neurondm-build \
--build-arg NUID=<<&UID>> \
--secret id=scigraph-api-key,src=<(echo export SCIGRAPH_API_KEY=$(python -c 'from pyontutils.config import auth; print(auth.get("scigraph-api-key"))')) \
--file musl/neurondm-build/Dockerfile musl/neurondm-build
#+end_src

Build using an alternate SciGraph API endpoint.
#+begin_src screen
# XXX note that NUID does nothing right now
docker build \
--tag tgbugs/musl:neurondm-build \
--build-arg NUID=<<&UID>> \
--build-arg SCIGRAPH_API=$(python -c 'from pyontutils.config import auth; print(auth.get("scigraph-api"))') \
--secret id=scigraph-api-key,src=<(echo) \
--file musl/neurondm-build/Dockerfile musl/neurondm-build
#+end_src

*** file
#+name: &musl/neurondm-build
#+begin_src dockerfile :tangle ./musl/neurondm-build/Dockerfile
FROM tgbugs/musl:neurondm
<<&-base-musl/neurondm-build>>
#+end_src

#+name: &-base-musl/neurondm-build
#+begin_src dockerfile
# phase five build
# XXX FIXME we can't run this for the demonstrator because the lack of
# npokb identifiers causes the queries to fail we probably want two
# separate images for this
ARG SCIGRAPH_API
ARG NEURONS_BRANCH
ARG NUID=11741
# FIXME waiting on https://github.com/moby/buildkit/issues/815
#RUN --mount=type=secret,id=scigraph-api-key,uid=${NUID} \
RUN --mount=type=secret,id=scigraph-api-key,uid=1000 source /run/secrets/scigraph-api-key \
; python -m neurondm.models.allen_cell_types \
; python -m neurondm.models.huang2017 \
; python -m neurondm.models.ma2015 \
; git -C ~/git/NIF-Ontology status
#+end_src

** postgresql
*** run
- configure
- migrate
- etc.

#+begin_src screen
docker run \
--volumes-from local-repos-snap \
-it tgbugs/musl:postgresql

# -v ~/files/docker-postgres/interlex-dev:/var/lib/postgresql \
# -v ~/.pgpass-interlex:/var/lib/interlex/.pgpass \
# -v ~/.mypass-interlex:/var/lib/interlex/.mypass \
# -v /tmp/.X11-unix:/tmp/.X11-unix \
# -e DISPLAY=$DISPLAY \

#+end_src

configure the database cluster
#+name: &config-pg
#+begin_src bash
# XXX DO NOT USE THIS OUTSIDE A musl docker image !!!
# changes here deviate from gentoo standard practice
# because PGDATA and DATA_DIR must be the same because
# /etc/postgresql-${PG_SLOT}/ is not persisted to the image
# when the database cluster is created, we may consider
# running this during the initial image build or in an interlex-user image build
# but this seems like a reasonable solution for now
# one issue is that if emerge --config was called without these
# changes the whole cluster must be re-initialized so watch out

function fix-pg-config () {
# FIXME this should almost certainly be run during image creation
# because otherwise we have to run it every single time we restart the image
local PG_SLOT PG_CONFD
PG_SLOT=$(eselect postgresql show)
PG_CONFD=/etc/conf.d/postgresql-${PG_SLOT}
# --locale determines LC_COLLATE and it needs to match
grep -q '^PG_INITDB_OPTS.\+locale=en_US.UTF-8' ${PG_CONFD} || \
echo 'PG_INITDB_OPTS="${PG_INITDB_OPTS} --locale=en_US.UTF-8"' >> ${PG_CONFD}
grep -q '^PGDATA="\${DATA_DIR}"' ${PG_CONFD} || \
echo 'PGDATA="${DATA_DIR}"  # overwrite to simplify image reuse' >> ${PG_CONFD}
}

function config-pg () {
fix-pg-config
local PG_SLOT
PG_SLOT=$(eselect postgresql show)
emerge --config dev-db/postgresql:${PG_SLOT}
}

#+end_src
*** build
#+name: &musl-build-postgresql
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:postgresql \
--file musl/postgresql/Dockerfile musl/postgresql
#+end_src
*** file
#+begin_src dockerfile :tangle ./musl/postgresql/Dockerfile :mkdirp yes
FROM tgbugs/musl:binpkg-only

<<&build-world-common>>
#+end_src
*** world
#+begin_src conf :tangle ./musl/postgresql/world :mkdirp yes
dev-db/postgresql
dev-db/pguri
#+end_src
** interlex
*** test
#+begin_src screen
test-image tgbugs/musl:interlex
#+end_src

#+header: :shebang "#!/usr/bin/env sh\nset -e"
#+begin_src bash :tangle ./bin/test/tgbugs/musl/interlex/test :mkdirp yes :noweb yes
<<&bash-at-abs-path>>

<<&config-interlex-db>>

pypy3 -c 'from pyontutils.config import auth; from interlex.config import auth'  # side effect create user config files
cp /dockerfiles/musl/interlex/test-config.yaml ~/.config/interlex/config.yaml
cp /dockerfiles/musl/interlex/test-config-pyontutils.yaml ~/.config/pyontutils/config.yaml

mkdir ~/.config/orthauth
chmod 0700 ~/.config/orthauth
cp /dockerfiles/musl/interlex/test-secrets.sxpr ~/.config/orthauth/secrets.sxpr
chmod 0600 ~/.config/orthauth/secrets.sxpr

touch ~/.mypass
mkdir ~/git
pushd ~/git
git clone --depth=1 https://github.com/tgbugs/pyontutils.git
git clone --depth=1 --branch=dev https://github.com/SciCrunch/NIF-Ontology.git
popd

pypy3 -m interlex.cli ops api-key
PYTHONPATH=~/git/pyontutils INTERLEX_DATABASE=$(python -c 'from interlex.config import auth; print(auth.get("test-database"))') \
pypy3 -m pytest --verbose --color=yes -W ignore /usr/share/interlex/test

#+end_src

#+begin_src yaml :tangle ./musl/interlex/test-config.yaml :mkdirp yes
auth-stores:
  secrets:
    path: ~/.config/orthauth/secrets.sxpr
auth-variables:
  email-verify: false
  interlex-api-key:
    path: interlex api tgbugs test
  test-api-user: tgbugs
  interlex-test-api-key:
    path: interlex api tgbugs test
  alt-db-host: localhost
  fl-session-secret-key:
    path: interlex flask session-secret-key
#+end_src

#+begin_src yaml :tangle ./musl/interlex/test-config-pyontutils.yaml
auth-stores:
  secrets:
    path: ~/.config/orthauth/secrets.sxpr
auth-variables:
  git-local-base: ~/git
  ontology-local-repo: ~/git/NIF-Ontology
#+end_src

#+name: ilx-test-key
#+begin_src python
# py-safe-block
from interlex.auth import gen_key
return gen_key()
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/interlex/test-secrets.sxpr
(
 :interlex
 (
  :flask
  (
   :session-secret-key "secret"
   )
  :api
  (:tgbugs ; FIXME we use tgbugs here becuase i insert it during setup (which should not be the case)
   (
    :test <<ilx-test-key()>>
    ))))
#+end_src

#+begin_src conf
localhost:3306:*:nif_eelg_secure:password
#+end_src

*** run
WARNING! If you mount your postgres data directory like this make sure
the host system is NOT also running postgres on top of that directory
otherwise you will have a BAD TIME.
**** setup
FIXME file ownership issues on pgpass and mypass AS IS TRADITION
see https://www.postgresql.org/docs/current/libpq-envars.html for possible alternate approaches
but for now we just have entrypoints do the very stupid thing of copying the =.*pass= files from
=/root/= to the actual location SIGH.
#+begin_src screen
# container-check
docker run \
--volumes-from local-repos-snap \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v ~/files/docker-postgres/interlex-dev:/var/lib/postgresql \
-v ~/.pgpass-interlex:/root/.pgpass-interlex \
-v ~/.mypass-interlex:/root/.mypass-interlex \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:interlex
#+end_src

**** ops
#+begin_src screen
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v ~/files/docker-postgres/interlex-dev:/var/lib/postgresql \
-v ~/.pgpass-interlex:/root/.pgpass-interlex \
-v ~/.mypass-interlex:/root/.mypass-interlex \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:interlex
#+end_src

*** build
#+name: &musl-build-interlex
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:interlex \
--file musl/interlex/Dockerfile musl/interlex
#+end_src

rebuild for quick package changes
#+begin_src screen
builder-arb --usepkg=y --onlydeps --onlydeps-with-rdeps=n interlex
builder-arb --nodeps interlex
# increment RUN echo 0 below
# ./source.org build-image tgbugs/musl:interlex
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/interlex/Dockerfile
FROM tgbugs/musl:binpkg-only

<<&build-world-common>>

# quick rebuild
RUN echo 0
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q interlex

ADD entrypoints /etc/entrypoints
ENTRYPOINT ["/etc/entrypoints/default.sh"]
#+end_src
*** world
# app-misc/elasticsearch  # XXX license issues, likely must handle separately
# so that we don't taint the profile, probably want a license builder image
# or something, this would allow us to build license tainted software and
# then individual images that want to use it could set the license before
# emerging the binpkg
#+name: world-interlex
#+begin_src conf :tangle ./musl/interlex/world
dev-python/interlex
#+end_src
FIXME there has got to be a better way to do this ...

*** entrypoints
**** default
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src bash :noweb yes :tangle ./musl/interlex/entrypoints/default.sh :mkdirp yes
<<&config-pg>>
rc-status
touch /run/openrc/softlevel
fix-pg-config  # FIXME REMOVE once this is done when building the image
/bin/bash
#+end_src
**** common
SIGH FIXME HACK

FIXME this is beyond dumb/annoying because we always have
to start the container as root and then either use docker exec
to run something else as another user or who knows
#+header: :shebang "#!/usr/bin/env sh"
#+name: &interlex-entrypoint-common
#+begin_src bash :noweb yes :tangle ./musl/interlex/entrypoints/common.sh :mkdirp yes
_oum=$(umask)
umask 0077
cp /root/.pgpass-interlex /var/lib/interlex/.pgpass
cp /root/.mypass-interlex /var/lib/interlex/.mypass
umask ${_oum}
chown interlex:interlex /var/lib/interlex/.*pass
#+end_src

**** initial configuration
yeold run once stuff
- postgres
- bin/interlex-dbsetup

#+name: &config-interlex-db
#+begin_src bash :noweb yes
<<&config-pg>>
config-pg
# FIXME if the database is already initialized then it assumes that the config files will be in /etc/postgresql-${PG_SLOT}/
# which is not the case if the docker image is removed/recycled, need a solution for that
rc-status
touch /run/openrc/softlevel
PG_SLOT=$(eselect postgresql show)
/etc/init.d/postgresql-${PG_SLOT} start
interlex-dbsetup  # quite broken due to install path issues right now
# FIXME I think we need to set interlex-user and interlex-admin passwords after this step? or is that automated now?
# XXX it isn't checking likely because pg_hba.conf is set to trust for all which is probably bad???
#+end_src

=test/test_constraints.py= also missing (but is expected to be missing) since it is only needed when testing

**** mysql dump
**** sync
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src bash :noweb yes :tangle ./musl/interlex/entrypoints/sync.sh :mkdirp yes
<<&interlex-common>>
# FIXME this needs to run as the interlex user probably due to the way we provide config files
# FIXME if hitting prod this needs the port to change, if syncing from a dev copy dumped from prod --network=host probably needed
su interlex -c 'INTERLEX_ALT_DB_HOST=localhost INTERLEX_ALT_DB_PORT=3306 INTERLEX_DATABASE=__interlex_testing interlex sync'
#+end_src
**** run
#+name: &interlex-common
#+begin_src bash :noweb yes
# FIXME obviously move some of this stuff into the image etc.
<<&config-pg>>
fix-pg-config  # FIXME remove once we move this to image creation
PG_SLOT=$(eselect postgresql show)
rc-status
touch /run/openrc/softlevel
/etc/init.d/postgresql-${PG_SLOT} start  # FIXME symlink to version to avoid need to rewrite here?
/etc/entrypoints/common.sh
#+end_src

#+header: :shebang "#!/usr/bin/env bash"
#+begin_src bash :noweb yes :tangle ./musl/interlex/entrypoints/service.sh :mkdirp yes
<<&interlex-common>>
# XXX FIXME adjust conf.d for testing
# DATABASE=__interlex_testing
# FIXME runs off a unix socket right now
# /etc/init.d/interlex start

# FIXME TODO config file probably
su interlex -c 'INTERLEX_DATABASE=__interlex_testing interlex server uri'  # FIXME temp until fix socket and/or switch to uwsgi
tail -f /dev/null
#+end_src

** blazegraph
*** run
#+begin_src bash
docker run \
-v /var/lib/blazegraph:/var/lib/blazegraph \
-p 9999:9999 \
-it tgbugs/musl:blazegraph
#+end_src
**** services
If you are running on some random system that doesn't already have the images, run the following.
#+begin_src bash :eval never
_running_blazegraph=$(docker ps -lqf ancestor=tgbugs/musl:blazegraph)
docker pull tgbugs/sckan:latest
docker pull tgbugs/musl:blazegraph
#+end_src

#+begin_src screen :noweb yes :tangle ./bin/run-sckan-blazegraph.sh
<<&create-sckan-data-if-not-exists>>
docker run \
--detach \
--volumes-from sckan-data \
-p 127.0.0.1:9999:9999 \
-it tgbugs/musl:blazegraph
#+end_src

backup journal after simple sckan inserts
#+begin_src bash
docker cp $(docker ps -lqf ancestor=tgbugs/musl:blazegraph):/var/lib/blazegraph/blazegraph.jnl blazegraph-$(date -Idate).jnl
#+end_src

restart without losing insert queries
#+begin_src bash
docker exec -it $(docker ps -lqf ancestor=tgbugs/musl:blazegraph) /etc/init.d/blazegraph stop
docker exec -it $(docker ps -lqf ancestor=tgbugs/musl:blazegraph) /etc/init.d/blazegraph start
#+end_src

restart and update to latest version of sckan data
#+begin_src screen :noweb yes :tangle ./bin/restart-sckan-blazegraph.sh
<<&create-sckan-data>>

# FIXME sometimes this fails with nasty results because we don't start the new image ???
# ah it will always happen if you pull a new blazegraph first ... very annoying
docker kill ${_running_blazegraph}

docker run \
--detach \
--volumes-from sckan-data \
-p 127.0.0.1:9999:9999 \
-it tgbugs/musl:blazegraph

curl http://127.0.0.1:9999/blazegraph/sparql
#+end_src

run simple sckan inserts
see [[file:../sparc-curation/docs/queries.org::hackathon 2023]]

validate inserts (in particular =Example 7: Neurons with Processes in IMG=)
https://github.com/smtifahim/sckan-query-examples
https://nbviewer.org/github/smtifahim/sckan-query-examples/blob/main/example-queries/sckan-sparql-query-examples.ipynb

cleanup old images
#+begin_src bash
docker container prune -f
docker image prune -f
#+end_src

view logs
#+begin_src bash
docker exec -it $(docker ps -lqf ancestor=tgbugs/musl:blazegraph) tail -f /var/log/blazegraph/sysout.log
#+end_src

***** additional ops stuff
****** manually update the rc and conf files
#+begin_src bash
docker exec -it $(docker ps -lqf ancestor=tgbugs/musl:blazegraph) /bin/bash
#+end_src

#+begin_src bash
nano /var/lib/blazegraph/override-web.xml
chown blazegraph:blazegraph /var/lib/blazegraph/override-web.xml
nano /etc/conf.d/blazegraph
nano /etc/init.d/blazegraph
/etc/init.d/blazegraph restart
#+end_src

*** build
#+name: &musl-build-blazegraph
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:blazegraph \
--file musl/blazegraph/Dockerfile musl/blazegraph
#+end_src

*** file
#+name: &musl/blazegraph
#+begin_src dockerfile :tangle ./musl/blazegraph/Dockerfile
<<&build-world-nox>>
ADD entrypoint.sh /etc/entrypoint.sh
ENTRYPOINT ["/etc/entrypoint.sh"]
#+end_src

*** world
#+name: world-blazegraph
#+begin_src conf :tangle ./musl/blazegraph/world
dev-db/blazegraph-bin
#+end_src

*** entrypoint
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/blazegraph/entrypoint.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel
rc-service blazegraph start
tail -f /dev/null
#+end_src

** scigraph
*** run
#+begin_src bash
docker run \
-v /var/lib/scigraph:/var/lib/scigraph \
-p 9000:9000 \
tgbugs/musl:scigraph
#+end_src

*** build
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:scigraph \
--file musl/scigraph/Dockerfile musl/scigraph
#+end_src

*** file
#+name: &musl/scigraph
#+begin_src dockerfile :tangle ./musl/scigraph/Dockerfile
<<&build-world>>
ADD entrypoint.sh /etc/entrypoint.sh
ENTRYPOINT ["/etc/entrypoint.sh"]
#+end_src

*** world
#+name: world-scigraph
#+begin_src conf :tangle ./musl/scigraph/world
dev-java/scigraph-bin
#+end_src

*** entrypoint
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/scigraph/entrypoint.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel
rc-service scigraph start
#+end_src

** kg-release
Base environment for knowledge graph distribution and interaction.
Combines both server and client functionalities into a single image.
In principle this could be split into multiple images, but for the
sake of simplicity and reproducibility it is a single image.

*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:kg-release
#+end_src

*** build
#+name: &musl-build-kg-release
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:kg-release \
--file musl/kg-release/Dockerfile musl/kg-release
#+end_src

*** file
# when deriving from multiple parent worlds docker does not compose
# well at all, so we have to pick a primary world line so to speak
#+name: &musl/kg-release
#+begin_src dockerfile :tangle ./musl/kg-release/Dockerfile
FROM tgbugs/musl:emacs

<<&build-world-common>>
#+end_src

*** world
#+name: world-kg-release
#+begin_src conf :tangle ./musl/kg-release/world
tgbugs-meta/kg-release-meta
#+end_src

#+begin_src conf
<<world-emacs>>
dev-db/blazegraph-bin
dev-java/scigraph-bin
media-gfx/feh
media-gfx/graphviz
dev-python/ipykernel
dev-python/pip
dev-haskell/dot2graphml
x11-misc/xdg-user-dirs
#+end_src
# dev-python/jupyter_server is not needed, dev-python/ipykernel is sufficient for our limited emacs-jupyter use case

# TODO consider including for the python bits in queries
# dev-python/pyontutils-9999
# dev-python/nifstd-tools-9999

** kg-release-user
*** run
**** default
Quick link to [[&create-sckan-data]] useful if =sckan-data= has not
been created or was wiped in a purge.

#+begin_src screen
docker run \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:kg-release-user
#+end_src

with configuration for xdg-open forwarding
#+begin_src screen
docker run \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--add-host=host.docker.internal:host-gateway \
-e FORWARD_URL_HOST=host.docker.internal \
-e FORWARD_URL_PORT=59213 \
-it tgbugs/musl:kg-release-user
#+end_src

debug with network in bridge mode
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
--add-host local.binhost:127.0.0.1 \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--add-host=host.docker.internal:host-gateway \
-e FORWARD_URL_HOST=host.docker.internal \
-e FORWARD_URL_PORT=59213 \
-it tgbugs/musl:kg-release-user
#+end_src

debug with host network
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
--add-host local.binhost:127.0.0.1 \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--add-host=host.docker.internal:host-gateway \
--network=host \
-e FORWARD_URL_HOST=host.docker.internal \
-e FORWARD_URL_PORT=59213 \
-it tgbugs/musl:kg-release-user
#+end_src
**** services
***** blazegraph
#+begin_src screen :noweb yes
<<&create-sckan-data-if-not-exists>>
docker run \
--detach \
--volumes-from sckan-data \
-v ${_path_dockerfiles}/musl/kg-release-user/entrypoints/blazegraph.sh:/etc/entrypoints/blazegraph.sh \
--entrypoint /etc/entrypoints/blazegraph.sh \
-p 9999:9999 \
-it tgbugs/musl:kg-release-user
#+end_src

To follow the logs you can run the following.
#+begin_src bash
docker exec -it $(docker ps -lqf ancestor=tgbugs/musl:kg-release-user) tail -f /var/log/blazegraph/sysout.log
#+end_src

***** both
#+begin_src screen :noweb yes :tangle ./bin/run-sckan-services.sh
<<&create-sckan-data-if-not-exists>>
docker run \
--volumes-from sckan-data \
-v ${_path_dockerfiles}/musl/kg-release-user/entrypoints/services.sh:/etc/entrypoints/services.sh \
--entrypoint /etc/entrypoints/services.sh \
-p 9999:9999 \
-p 9000:9000 \
-it tgbugs/musl:kg-release-user
#+end_src
*** test
#+name: &test-kg-release-user
#+begin_src screen
<<&create-sckan-data-if-not-exists>>
docker run \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-v ${_path_dockerfiles}/musl/kg-release-user/entrypoints/test.sh:/etc/entrypoints/test.sh \
--entrypoint /etc/entrypoints/test.sh \
--rm \
tgbugs/musl:kg-release-user
#+end_src

*** build
# FIXME move this into a container
#+name: &musl-build-kg-release-user
#+begin_src screen
unset _devel _docd _sckand _docsfsc
_devel=
_docd=(queries.org)
_docsf="https://raw.githubusercontent.com/SciCrunch/sparc-curation/master/docs/"
_sckand=(welcome.org tutorial.org overview.org examples.org scratch.org README.org)
_docsfsc="${_docsf}sckan/"
pushd ./musl/kg-release-user
  [ -d sckan ] && rm -r sckan
  mkdir sckan
  pushd sckan
    mkdir images
    mkdir reports
    if [ -n "${_devel}" ]; then
      cp -aL ~/git/sparc-curation/docs/sckan/*.org . ;
    else
      for fn in ${_docd[@]};   do curl -O ${_docsf}${fn}  ; done
      for fn in ${_sckand[@]}; do curl -O ${_docsfsc}${fn}; done
    fi
    chmod +x ./queries.org
  popd
popd

<<&musl-build-kg-release-user-min>>
#+end_src

#+name: &musl-build-kg-release-user-min
#+begin_src screen
docker build \
--tag tgbugs/musl:kg-release-user \
--build-arg UID=<<&UID>> \
--file musl/kg-release-user/Dockerfile musl/kg-release-user
#+end_src

*** entrypoints
Default interactive entrypoint.
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-release-user/entrypoint.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel
/etc/init.d/scigraph start
/etc/init.d/blazegraph start
runuser -u user -- emacs -geometry 120x40 -eval "(find-file-noselect (pop argv))" ~/sckan/welcome.org
/etc/init.d/scigraph stop
/etc/init.d/blazegraph stop
#+end_src

Entrypoint to start blazegraph and run in the background.
# FIXME BUILDKIT_PROGRESS='plain' leaking through here somehow but now on orpheus when it tangles?
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-release-user/entrypoints/blazegraph.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel
/etc/init.d/blazegraph start
tail -f /dev/null
#+end_src

Entrypoint to start services and run in the background.
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-release-user/entrypoints/services.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel
/etc/init.d/scigraph start
/etc/init.d/blazegraph start
tail -f /dev/null
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-release-user/entrypoints/test.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel

# check that services are working as expected
/etc/init.d/scigraph start
SCIG_OOPS=$?
/etc/init.d/blazegraph start
BLAZ_OOPS=$?
[ ${SCIG_OOPS} -eq 0 ] && [ ${BLAZ_OOPS} -eq 0 ] || { echo scigraph ${SCIG_OOPS} and blazegraph ${BLAZ_OOPS} failed to start; exit 1;}
[ ${SCIG_OOPS} -eq 0 ] || { echo scigraph failed to start; exit 2;}
[ ${BLAZ_OOPS} -eq 0 ] || { echo blazegraph failed to start; exit 3;}

# check elisp config
runuser -u user -- sh -c 'sckan/queries.org test config' || { echo basic config test failed cannot continue; exit 4;}
# run data sanity check queries
runuser -u user -- sh -c 'sckan/queries.org test query neurons datasets protocols' || { echo at least one query test failed; exit 5;}
#+end_src

*** file
# FIXME we should be able to stash the builder in this image
# and reuse it without having to rerun over and over ...
#+name: kg-root-password
: sparcSCKAN-2021

#+name: &musl/kg-release-user
#+begin_src dockerfile :tangle ./musl/kg-release-user/Dockerfile
FROM tgbugs/musl:kg-release AS builder

WORKDIR /build

<<&user-skel-common>>

ARG HOME=/build/home/${USER_NAME}

WORKDIR $HOME

COPY --chown=${UID}:${UID} sckan sckan

USER ${USER_NAME}

RUN \
xdg-user-dirs-update \
;  rmdir Desktop Pictures Documents Public Downloads Templates Music Videos > /dev/null 2>&1

RUN \
sed -i 's/-no-site-file//' ./sckan/queries.org \
&& ./sckan/queries.org

RUN \
pushd ./.emacs.d/ \
&& ln -s reval/cache/*/*-ow.el ow.el

ADD --chown=1000:1000 early-init.el ./.emacs.d/early-init.el
ADD --chown=1000:1000 init.el ./.emacs.d/init.el

RUN \
emacs -batch -eval \
"(let ((user-init-file (pop argv))) (load (pop argv)) (load user-init-file) (while argv (let ((f (pop argv))) (orgstrap-whitelist-file f) (kill-buffer (find-file-noselect f)))) (ow-use-packages evil undo-tree))" \
$HOME/.emacs.d/init.el $HOME/.emacs.d/early-init.el sckan/{queries,welcome,examples,scratch,tutorial}.org

FROM tgbugs/musl:kg-release

COPY --from=builder /build /

ADD entrypoints /etc/entrypoints
ADD entrypoint.sh /etc/entrypoint.sh

<<&musl-file-user-base>>

<<&dev-user-common-3>>

# TODO when running this you will have to set the right mounts
# unless you bake a new kg-dev-with-data release
ENTRYPOINT ["/etc/entrypoint.sh"]
#+end_src

*** emacs init
Must disable early loading of =package.el= because it makes it
impossible for site-lisp to add packages which breaks the activation
of any dependent packages.
#+begin_src elisp :tangle ./musl/kg-release-user/early-init.el
(setq package-enable-at-startup nil)
#+end_src
Calling =ow-enable-use-package= makes it possible to load site-lisp
and call =package-initialize= with =no-activate= set so that site
dependencies can be loaded correctly. This is all a dance to work
around the fundamentally broken design of =normal-top-level= and
=early-init= with regard to site-lisp.
#+begin_src elisp :tangle ./musl/kg-release-user/init.el
(load (expand-file-name "ow.el" user-emacs-directory))
(ow-enable-use-package nil '(vterm zmq))
#+end_src
** kg-dev
:PROPERTIES:
:CUSTOM_ID: kg-dev
:END:
*** run
# scigraph-build-local
# scigraph-deploy-local
# TODO package ontree server so that the updated local scigraph can be seen
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:kg-dev
#+end_src

#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v /tmp/scigraph-build:/tmp/scigraph-build \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:kg-dev \
echo TODO secrets, apinat build and more!
#+end_src

debug
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
--network host \
--add-host local.binhost:127.0.0.1 \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:kg-dev
#+end_src

*** build
#+name: &musl-build-kg-dev
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:kg-dev \
--build-arg UID=<<&UID>> \
--file musl/kg-dev/Dockerfile musl/kg-dev
#+end_src

*** file
#+name: &musl/kg-dev
#+begin_src dockerfile :tangle ./musl/kg-dev/Dockerfile
FROM tgbugs/musl:kg-release
<<&build-world-common>>

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
eselect racket set cs
#+end_src

*** world
#+name: world-kg-dev
#+begin_src conf :tangle ./musl/kg-dev/world
tgbugs-meta/kg-dev-meta
#+end_src

#+begin_src conf
<<world-kg-release>>
<<world-sbcl-base>>
<<world-tex>>
app-arch/zip
app-misc/screen
app-misc/yq
app-text/pandoc-cli
dev-java/robot-bin
dev-node/apinat-converter
dev-python/nifstd-tools
dev-python/sparcur
dev-db/redict
net-misc/rabbitmq-server
dev-scheme/racket
dev-util/shellcheck
net-libs/nodejs
dev-python/seaborn
dev-libs/redland
media-gfx/inkscape
#+end_src

FIXME can't put license stuff in world because it gets pulled into the global package world
#+name: set-license-kg-dev
#+begin_src conf :tangle ./musl/kg-dev/sets/license
media-fonts/corefonts
#+end_src

# FIXME pandoc is a dev-python/nifstd-tools doc dep :/
# FIXME seaborn for generating figures, not clear whether this is the right place, but here for now

# FIXME and here we are having an annoying time
# with nifstd-tools and variant use flags sigh
# maybe build both versions of the package somehow
# without creating yet another profile variant

# FIXME issue with robot-bin building against openjdk-bin-17
# and then running against openjdk-17, that shouldn't matter
# but it seems that something got baked in by accident

# we include pennsieve in world here because
# it cannot be installed via pip due to a completely
# braindead handling of 2.7 vs 3.0 issues

#+name: world-tex
#+begin_src conf
app-text/texlive
dev-tex/biblatex
dev-tex/latexmk
dev-texlive/texlive-latexextra
dev-texlive/texlive-luatex
dev-texlive/texlive-music
#+end_src
We specify =texlive-latexextra= and =texlive-music= explicitly instead
of as use flags on texlive to avoid pulling in xetex as a dependency.

** kg-dev-user
:PROPERTIES:
:CUSTOM_ID: kg-dev-user
:END:
# FIXME somehow missing blazegraph user?
*** run
# TODO -v /var/lib/scigraph:/var/lib/scigraph \
reminder: localhost:9000 tells the image to use its own internal
scigraph not the host scigraph specifically it prevents it from using
the scicrunch production api endpoint
#+begin_src screen
# docker create -v /var/lib/blazegraph -v /var/lib/scigraph --name sckan-data tgbugs/sckan:latest /bin/true
docker run \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-e SCIGRAPH_API=http://localhost:9000/scigraph \
-it tgbugs/musl:kg-dev-user

#+end_src

Example of how to use the image for the SCKAN release workflow.
#+begin_src bash
mkdir /tmp/build  # must be made first by the running user otherwise permission issues
docker run \
--volumes-from sckan-data \
-v /tmp/build:/tmp/build \
-v /tmp/scigraph-build:/tmp/scigraph-build \
-v /home/tom/.ssh:/home/user/.ssh \
-v /home/tom/.ssh_tmp:/home/user/.ssh_tmp \
-v /home/tom/git/misc:/home/user/git/misc \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v $(dirname ${SSH_AUTH_SOCK}):$(dirname ${SSH_AUTH_SOCK}) \
-e SSH_AUTH_SOCK=${SSH_AUTH_SOCK} \
-e SSH_AGENT_PID=${SSH_AGENT_PID} \
-e DISPLAY=$DISPLAY \
-e SCIGRAPH_API=http://localhost:9000/scigraph \
-it tgbugs/musl:kg-dev-user
#+end_src

#+begin_src screen
docker run \
-v /var/lib/blazegraph:/var/lib/blazegraph \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:kg-dev-user
#+end_src

*** test
#+name: &test-musl-kg-dev-user
#+begin_src screen
test-image tgbugs/musl:kg-dev-user
#+end_src

#+header: :shebang "#!/usr/bin/env sh\nset -e"
#+begin_src bash :tangle ./bin/test/tgbugs/musl/kg-dev-user/test :mkdirp yes :noweb yes
# <<&bash-at-abs-path>>
#emerge --color=y -q -1q --usepkgonly pytest
su - user -c "sh /dockerfiles/bin/test/tgbugs/musl/kg-dev-user/test.sh"
#+end_src

#+begin_src bash :tangle ./bin/test/tgbugs/musl/kg-dev-user/test.sh :mkdirp yes :noweb yes
set -e
sh ~/git/pyontutils/nifstd/scigraph/README.org tangle
~/git/sparc-curation/docs/release.org # build --sckan --no-blaze --no-load
#+end_src

#+begin_src bash
set -e
export PYTHONBREAKPOINT=0
pushd ~/git/pyontutils
pypy3 -m pytest -x || true
popd
pushd ~/git/sparc-curation
pypy3 -m pytest -x || true
popd
#+end_src

**** with data
#+name: &test-kg-dev-user
#+begin_src screen
<<&create-sckan-data-if-not-exists>>
docker run \
--volumes-from sckan-data \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-e SCIGRAPH_API=http://localhost:9000/scigraph \
-v ${_path_dockerfiles}/musl/kg-dev-user/entrypoints/test.sh:/etc/entrypoints/test.sh \
--entrypoint /etc/entrypoints/test.sh \
--rm \
tgbugs/musl:kg-dev-user
#+end_src
*** build
#+name: &musl-build-kg-dev-user
#+begin_src screen
docker build \
--tag tgbugs/musl:kg-dev-user \
--build-arg UID=<<&UID>> \
--file musl/kg-dev-user/Dockerfile musl/kg-dev-user
#+end_src

*** entrypoint
1. sync
   1. sparc curation (done separately)
   2. NPO
   3. NIF-Ontology general
   4. ApiNATOMY
   5. prrequaestor stuff

2. build release
   1. scigraph
   2. blazegraph
   3. docker
3. test
4. deploy

#+begin_src bash
if [ -d /tmp/blazegraph ]; then
    cp /tmp/blazegraph/blazegraph.jnl /var/lib/blazegraph/
    cp /tmp/blazegraph/prefixes.conf /var/lib/blazegraph/
    chown -R blazegraph:blazegraph /var/lib/blazegraph
fi
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-dev-user/entrypoint.sh :mkdirp yes
rc-status
touch /run/openrc/softlevel
/etc/init.d/scigraph start
/etc/init.d/blazegraph start
#su user -c 'emacs -visit ~/git/sparc-curation/docs/queries.org'
su user -
/etc/init.d/scigraph stop
/etc/init.d/blazegraph stop
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-dev-user/entrypoints/sync.sh :mkdirp yes
/etc/init.d/scigraph start
# do prrequaestor things
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-dev-user/entrypoints/test.sh :mkdirp yes
echo TODO testing ...
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/kg-dev-user/entrypoints/sckan-release.sh :mkdirp yes
su user -c 'sh ~/git/sparc-curation/docs/developer-guide.org sckan-release'
#+end_src

*** file
# XXX note that using builder means that the command history
# that is used to create the /build/ directory are lost
# sigh bad trade offs for multi stage builds
# HOORAY pip install --user -e and /build/ are incompatable! >_<
#+name: &emacs-sanity
#+begin_src dockerfile
ARG INIT_URL=https://raw.githubusercontent.com/tgbugs/orgstrap/master/init-simple.el

RUN \
emacs --batch --quick --eval \
"(progn (setq ow-site-packages '(vterm zmq)) (url-handler-mode 1) (find-file (pop argv)) (eval-buffer))" \
"${INIT_URL}"

RUN \
pushd ~/.emacs.d \
&& ln -s reval/cache/*/*-ow.el ow.el \
&& ln -s reval/cache/*/*-reval.el reval.el \
&& ln -s reval/cache/*/*-init-content.el init-content.el \
&& echo "(setq package-enable-at-startup nil)" >> early-init.el \
&& echo "(defvar ow-site-packages '(vterm zmq))" >> init.el \
&& echo "(load (expand-file-name \"ow.el\" user-emacs-directory))" >> init.el \
&& echo "(load (expand-file-name \"reval.el\" user-emacs-directory))" >> init.el \
&& echo "(load (expand-file-name \"init-content.el\" user-emacs-directory))" >> init.el \
&& popd
#+end_src

#+name: &dev-user-common-1
#+begin_src dockerfile
WORKDIR /build

<<&user-skel-common>>

RUN \
usermod --move-home --home /home/user ${USER_NAME}
#mv /build/home/user /home/ \
#&& usermod -d /home/user -m

# FIXME TEMP needed for sbcl quicklisp https stuff
RUN \
chown ${USER_NAME} /build

USER ${USER_NAME}

#ARG HOME=/build/home/${USER_NAME}  # FIXME somehow /build/ lingers ??? do I have a stale file !?? !
# XXX the issue is &user-skel-common uses /build/home/user in useradd with the expectation that
# it will not affect the actual image only the builder image
# FIXME XXXXXXXXXXXXXXXXXXXXXX SO SO SO SO BROKEN ARGH this breaks emacs and everything else
# it looks like /etc/passwd still has /build/home/user as HOME so setting $HOME doesn't save us?
# at least during debug that is what it looks like
ARG HOME=/home/${USER_NAME}

WORKDIR $HOME

RUN \
xdg-user-dirs-update \
;  rmdir Desktop Pictures Documents Public Downloads Templates Music Videos > /dev/null 2>&1

<<&emacs-sanity>>

RUN \
mkdir ~/git \
&& mkdir ~/.ssh \
&& chmod 0700 ~/.ssh \
&& mkdir ~/.ssh_tmp \
&& chmod 0700 ~/.ssh_tmp
#+end_src

#+name: &dev-user-common-2
#+begin_src dockerfile
COPY --from=builder /home/ /home/

<<&musl-file-user-base>>
#+end_src

#+name: &dev-user-common-3
#+begin_src dockerfile
# root stuff must run after &musl-file-user-base because user must already exist

USER 0

RUN \
usermod -a -G blazegraph user \
;  usermod -a -G scigraph user \
;  usermod -a -G wheel user

RUN \
echo 'root:<<kg-root-password()>>' | chpasswd

# make it easier to use portage inside a container
RUN \
echo 'EMERGE_DEFAULT_OPTS=""' >> /etc/portage/make.conf \
&& echo 'FEATURES=""' >> /etc/portage/make.conf
#+end_src

#+begin_src dockerfile :tangle ./musl/kg-dev-user/Dockerfile
FROM tgbugs/musl:kg-dev AS builder

<<&dev-user-common-1>>

# FIXME that is going to need to go in .bashrc or something
ENV PYTHONPYCACHEPREFIX=${HOME}/.cache/pycache/

# FIXME break these into their own images to avoid serial dependencies

# FIXME pip install --user -e . fails when run in /build/ due to change in $HOME
# because it uses an expanded directory of course pip doesn't have any only-deps
# option

RUN \
pushd git \
&&     git clone https://github.com/tgbugs/pyontutils.git \
&&     git clone https://github.com/SciCrunch/sparc-curation.git \
&& popd

RUN \
pushd git \
&&     pushd pyontutils \
&&         touch pyproject.toml \
&&         python -m pip install --user --break-system-packages -e . \
&&         pypy3 -m pip install --user --break-system-packages -e . \
&&         pushd nifstd \
&&             touch pyproject.toml \
&&             python setup.py --release || true \
&&             python -m pip install --user --break-system-packages -e . \
&&             pypy3 -m pip install --user --break-system-packages -e . \
&&         popd \
&&     popd \
&&     pushd sparc-curation \
&&         touch pyproject.toml \
&&         python setup.py --release || true \
&&         python -m pip install --user --break-system-packages -e . \
&&         pypy3 -m pip install --user --break-system-packages -e . \
&&     popd \
&& popd \
&& ln -s ../../git/sparc-curation/docs/apinatomy.org ~/.local/bin/apinat-build \
&& rm -r ~/.cache/pip

RUN \
sh ./git/pyontutils/nifstd/scigraph/README.org --tangle

# curation viewer setup
RUN \
raco pkg install --no-docs --name breadcrumb --type git-url https://github.com/tgbugs/racket-breadcrumb.git \
&& raco pkg install --no-docs --name json-view --type git-url https://github.com/tgbugs/racket-json-view.git \
&& raco pkg install --no-docs --auto --batch git/sparc-curation/sparcur_internal/sparcur/

# protc setup
RUN \
pushd git \
&&     git clone https://github.com/tgbugs/rkdf.git \
&&     git clone https://github.com/tgbugs/protc.git \
&&     git clone https://github.com/SciCrunch/NIF-Ontology.git \
&& popd

RUN \
raco pkg install --no-docs --name barcode --type git-url https://github.com/zussitarze/qrcode.git \
&& pushd git \
&&     pushd rkdf \
&&         raco pkg install --no-docs --auto --batch rkdf-lib/ rkdf/ \
&&     popd \
&&     pushd NIF-Ontology \
&&         git checkout dev \
&&         PATH=../rkdf/bin:${PATH} rkdf-convert-all \
&&         git checkout master \
&&     popd \
&&     raco pkg install --no-docs --auto --batch NIF-Ontology/ \
&&     pushd protc \
&&         raco pkg install --no-docs --auto --batch protc-lib/ protc/ protc-tools-lib/ protc-tools/ \
&&     popd \
&& popd

<<&dev-sbcl-stuff-1>>

RUN \
./git/sparc-curation/docs/queries.org

RUN \
./git/sparc-curation/docs/apinatomy.org

RUN \
emacs -batch -eval \
"(let ((user-init-file (pop argv))) (package-initialize) (while argv (orgstrap-whitelist-file (pop argv))))" \
$HOME/.emacs.d/init.el \
./git/sparc-curation/docs/queries.org \
./git/sparc-curation/docs/apinatomy.org

FROM tgbugs/musl:kg-dev

<<&dev-user-common-2>>

ENV PATH="/home/${USER_NAME}/.local/bin:${PATH}"

# XXX somehow this induces ~10MB of new content !??!
#RUN \
#pip install --user --no-deps -e \
#git/pyontutils/nifstd \
#git/sparc-curation \
#&& rm -r ~/.cache/pip

<<&dev-user-common-3>>

ADD entrypoints /etc/entrypoints
ADD entrypoint.sh /etc/entrypoint.sh

# TODO when running this you will have to set the right mounts
# unless you bake a new kg-dev-with-data release
ENTRYPOINT ["/etc/entrypoint.sh"]
#+end_src

#+name: &dev-sbcl-stuff-1
#+begin_src dockerfile
# sbcl quicklisp https
RUN \
git clone --branch version-2021-02-13 --depth=1 https://github.com/quicklisp/quicklisp-client.git /build/quicklisp

RUN \
git clone --depth=1 https://github.com/rudolfochrist/ql-https.git /tmp/ql-https

USER 0

COPY quicklisp-https-always /build/quicklisp/local-projects/quicklisp-https-always
ADD bootstrap-1.lisp /tmp/bootstrap-1.lisp
ADD bootstrap-2.lisp /tmp/bootstrap-2.lisp
ADD bootstrap-3.lisp /tmp/bootstrap-3.lisp

RUN \
chown ${USER_NAME}:${USER_NAME} /tmp/bootstrap-*.lisp \
&& chown -R ${USER_NAME}:${USER_NAME} /build/quicklisp

USER ${USER_NAME}

RUN --network=none \
sbcl --no-userinit --non-interactive \
--load /tmp/bootstrap-1.lisp

RUN \
sbcl --core /tmp/continue.core --no-userinit --non-interactive \
--load /tmp/bootstrap-2.lisp

# FIXME shouldn't need to do this dance
RUN \
mv /build/quicklisp ${HOME}/quicklisp

RUN \
sbcl --no-userinit --non-interactive \
--load /tmp/bootstrap-3.lisp

# FIXME HACK workaround for bad prcl assumptions
RUN \
mkdir -p ${HOME}/code/lisp

RUN \
ln -s ../../quicklisp ${HOME}/code/lisp/quicklisp

ADD clrc ${HOME}/.sbclrc

# FIXME something around here is installing slime without checking to see if sly exists
# the issue is probably that the git share readme orgstrap doesn't enable packages to check
RUN \
pushd git \
&&     git clone https://github.com/tgbugs/git-share.git \
&&     sh git-share/README.org tangle \
&&     git clone https://github.com/tgbugs/prrequaestor.git \
&&     pushd ~/git/prrequaestor \
&&         ./prcl-build.lisp \
&&     popd \
&& popd
#+end_src

*** retangles
In general we should prefer to retangle rather than move the working
directory for docker build because this is explicit and we can trace
the links between =:tangle= paths and =<<noweb>>= references.

#+begin_src lisp :noweb yes :tangle ./musl/kg-dev-user/bootstrap-1.lisp :mkdirp yes
<<lisp-ql-https-bootstrap-1>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/kg-dev-user/bootstrap-2.lisp
<<lisp-ql-https-bootstrap-2>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/kg-dev-user/bootstrap-3.lisp
<<lisp-ql-https-bootstrap-3>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/kg-dev-user/clrc
<<lisp-ql-https-clrc>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/kg-dev-user/quicklisp-https-always/quicklisp-https-always.lisp :mkdirp yes
<<lisp-qlha-lisp>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/kg-dev-user/quicklisp-https-always/quicklisp-https-always.asd
<<lisp-qlha-asdf>>
#+end_src

** tgbugs-dev
*** build
#+name: &musl-build-tgbugs-dev
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:tgbugs-dev \
--file musl/tgbugs-dev/Dockerfile musl/tgbugs-dev
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/tgbugs-dev/Dockerfile :mkdirp yes
FROM tgbugs/musl:kg-dev
<<&build-world-common>>

# XXX temporary until upstream fixes missing symlink/eselect pypy
RUN \
unlink /usr/bin/pypy3 \
;  ln -s pypy3.11 /usr/bin/pypy3
#+end_src

*** TODO world
#+name: world-tgbugs-dev
#+begin_src conf :tangle ./musl/tgbugs-dev/world
tgbugs-meta/dev-meta
#+end_src

#+begin_src conf
<<world-kg-dev>>
dev-python/pytest
#+end_src

** tgbugs-dev-user
*** notes
Replication of the environment used to develop all the various projects used throughout this file.

This is distinct from kg-dev-user which is specifically for developing
the KG but NOT the development of the code to develop the KG.

This section is referenced in
[[file:~/git/sparc-curation/docs/setup.org::*Variables]] and
[[file:~/git/sparc-curation/sparcur_internal/sparcur/README.org::*python]]

and extend it to tangle the
envars we need to get all the git repos and those should be cloned into
the parent system in some sandboxed directory
see also the sparcur racket viewer readme which should converge there too
*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:tgbugs-dev-user
#+end_src

*** build
#+name: &musl-build-tgbugs-dev-user
#+begin_src screen
docker build \
--tag tgbugs/musl:tgbugs-dev-user \
--file musl/tgbugs-dev-user/Dockerfile musl/tgbugs-dev-user
#+end_src

iteration during devel could use
#+begin_src bash
./source.org tangle && ./source.org build-image tgbugs/musl:tgbugs-dev-user
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/tgbugs-dev-user/Dockerfile :mkdirp yes
FROM tgbugs/musl:tgbugs-dev AS builder

<<&dev-user-common-1>>

<<&dev-sbcl-stuff-1>>

ADD repo-setup.sh /tmp/repo-setup.sh

# TODO this is the point where we need to decide whether we do this in build or in run
RUN \
sh /tmp/repo-setup.sh

# TODO pip install --user --break-system-packages pudb
# TODO set PYTHONBREAKPOINT=pudb.set_trace etc. in bashrc

FROM tgbugs/musl:tgbugs-dev

<<&dev-user-common-2>>

<<&dev-user-common-3>>

USER ${USER_NAME}
#+end_src

*** retangles
#+begin_src lisp :noweb yes :tangle ./musl/tgbugs-dev-user/bootstrap-1.lisp :mkdirp yes
<<lisp-ql-https-bootstrap-1>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/tgbugs-dev-user/bootstrap-2.lisp
<<lisp-ql-https-bootstrap-2>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/tgbugs-dev-user/bootstrap-3.lisp
<<lisp-ql-https-bootstrap-3>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/tgbugs-dev-user/clrc
<<lisp-ql-https-clrc>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/tgbugs-dev-user/quicklisp-https-always/quicklisp-https-always.lisp :mkdirp yes
<<lisp-qlha-lisp>>
#+end_src

#+begin_src lisp :noweb yes :tangle ./musl/tgbugs-dev-user/quicklisp-https-always/quicklisp-https-always.asd
<<lisp-qlha-asdf>>
#+end_src

*** build helper script
TODO there is a question of whether to use =docker build= or =docker run= for this.
The advantage of using =docker run= is that

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :noweb yes :tangle ./musl/tgbugs-dev-user/repo-setup.sh
<<&dev-repo-setup()>>
#+end_src

#+name: &dev-group-repo
| tgbugs/augpathlib
| tgbugs/dynapad
| tgbugs/elasticsearch.rkt
| tgbugs/hyputils
| tgbugs/idlib
| tgbugs/interlex
| tgbugs/ontquery
| tgbugs/orgstrap
| tgbugs/orthauth
| tgbugs/parsercomb
| tgbugs/protc
| tgbugs/pyontutils
| tgbugs/racket-breadcrumb
| tgbugs/racket-json-view
| tgbugs/racket-mode
| tgbugs/rkdf
| tgbugs/rrid-metadata
| tgbugs/sxpyr
| SciCrunch/NIF-Ontology
| SciCrunch/scibot
| SciCrunch/sparc-curation
| Ophirr33/pda
| zussitarze/qrcode

#+name: &dev-py-install-order
| pyontutils/clifn
| sxpyr
| augpathlib
| idlib
| pyontutils/htmlfn
| pyontutils/ttlser
| hyputils
| orthauth
| ontquery
| parsercomb
| pyontutils
| protc/protcur
| sparc-curation
| scibot
| interlex
| pyontutils/nifstd
| pyontutils/neurondm

#+name: &dev-rkt-install-order-name
| qrcode/                                 | barcode     |
| pda/                                    |             |
| rkdf/rkdf-lib                           |             |
| rkdf/rkdf                               |             |
| NIF-Ontology/                           |             |
| protc/protc-lib                         |             |
| protc/protc                             |             |
| protc/protc-tools-lib                   |             |
| protc/protc-tools                       |             |
| elasticsearch.rkt/elasticsearch         |             |
| rrid-metadata/rrid                      |             |
| orthauth/racket/orthauth                |             |
| racket-breadcrumb/                      | breadcrumb  |
| racket-json-view/                       | json-view   |
| sparc-curation/sparcur_internal/sparcur |             |
| racket-mode/racket                      | racket-mode |
| dynapad/dynapad                         |             |
| dynapad/dynapad-collects                |             |

#+name: &dev-repo-setup
#+begin_src elisp :noweb yes :results drawer :lexical yes
; elisp-safe-block
(let ((clone-ssh nil)) ; TODO
  (let ((group-repo '<<&dev-group-repo()>>)
        (py-inst '<<&dev-py-install-order()>>)
        (rkt-inst (mapcar (lambda (l)
                            (cons (car l)
                                  (let ((cl (cadr l)))
                                    (if (string= cl "")
                                        nil
                                      cl))))
                          '<<&dev-rkt-install-order-name()>>))
        (rkt-known-fail '("rrid-metadata/rrid"))
        (ontology-repo-rkt-setup "
pushd NIF-Ontology
git checkout dev
PATH=../rkdf/bin:${PATH} rkdf-convert-all
git checkout master
popd
")
        (fgitstr (if clone-ssh "git@github.com:%s.git" "https://github.com/%s.git")))
    (pp-to-string (list group-repo py-inst rkt-inst))
    (string-join
     (flatten-list
      (list
       "set -e"
       "pushd ~/git"
       (cl-loop
        for (gr . _) in group-repo
        unless (string-match "dynapad\\|interlex" gr) ; TODO needs ssh
        collect
        (concat "git clone " (format fgitstr gr)))
       (cl-loop
        for (rp . name) in rkt-inst
        unless (string-match "dynapad" rp) ; TODO needs ssh
        collect
        (concat (when (string= rp "NIF-Ontology/") ontology-repo-rkt-setup)
                "raco pkg install --no-docs --auto --batch " (if name (concat "--name " name " ") "") rp
                (when (member rp rkt-known-fail)
                  (concat " || echo " rp " failed with $? ... continuing"))))
       (cl-loop
        for (pi . _) in py-inst
        unless (string-match "interlex" pi) ; TODO needs ssh
        collect ; FIXME didn't we wind up having to use PYTHONPATH ???
        (concat "pushd " pi "
touch pyproject.toml
python -m pip install --break-system-packages --user -e .
pypy3 -m pip install --break-system-packages --user -e .
popd"))
       "popd"
       ))
     "\n")
    )
  )
#+end_src

** racket
*** test
#+name: &test-musl-racket
#+begin_src screen
test-image tgbugs/musl:racket
#+end_src

#+header: :shebang "#!/usr/bin/env sh\nset -e"
#+begin_src bash :tangle ./bin/test/tgbugs/musl/racket/test :mkdirp yes :noweb yes
<<&bash-at-abs-path>>
racket test.rkt
#+end_src

#+begin_src racket :tangle ./bin/test/tgbugs/musl/racket/test.rkt :mkdirp yes :noweb yes
#lang racket/base
(require racket/string)
(displayln (version))
(displayln "ok")
(flush-output)
;(error "expected to fail") ; FIXME a hack to ensure tests can fail
#+end_src

*** build
#+name: &musl-build-racket
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:racket \
--file musl/racket/Dockerfile musl/racket
#+end_src

Build debug workflow.
#+begin_src bash
# if you have not done so already
docker create \
-v /var/db/repos/gentoo \
--name local-portage-snap \
docker.io/gentoo/portage:latest \
/bin/true

if [ -n "${_use_podman}" ]; then
[ -n "$(podman container ls -q --filter name=local-portage-snap --filter status=initialized)" ] || \
podman container init local-portage-snap
fi

# if you have you have to clear the container with
# docker rm local-portage-snap

# then
docker run \
--volumes-from local-portage-snap \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:racket
#+end_src

*** file
#+name: &musl/racket
#+begin_src dockerfile :tangle ./musl/racket/Dockerfile
FROM tgbugs/musl:emacs
<<&build-world-common>>

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
eselect racket set cs
#+end_src

*** world
#+name: world-racket
#+begin_src conf :tangle ./musl/racket/world
tgbugs-meta/racket-meta
#+end_src

#+begin_src conf
<<world-emacs>>
dev-scheme/racket
#+end_src
** racket-user
*** run
#+begin_src bash
# to allow the container access to the local x session you have to run the following
xhost local:docker
# use xhost -local:docker to remove

docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:racket-user
#+end_src

*** build
#+name: &musl-build-racket-user
#+begin_src screen
docker build \
--tag tgbugs/musl:racket-user \
--build-arg UID=<<&UID>> \
--file musl/racket-user/Dockerfile musl/racket-user
#+end_src

*** file
#+name: &musl/racket-user
#+begin_src dockerfile :tangle ./musl/racket-user/Dockerfile
FROM tgbugs/musl:racket

COPY --from=tgbugs/common:user / /

<<&musl-file-user-base>>
#+end_src

** dynapad-base
*** build
#+name: &musl-build-dynapad-base
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:dynapad-base \
--file musl/dynapad-base/Dockerfile musl/dynapad-base
#+end_src
*** file
#+name: &musl/dynapad-base
#+begin_src dockerfile :tangle ./musl/dynapad-base/Dockerfile
FROM tgbugs/musl:racket
<<&build-world-common>>
#+end_src

*** world
#+name: world-dynapad-base
#+begin_src conf :tangle ./musl/dynapad-base/world
tgbugs-meta/dynapad-meta
#+end_src

#+begin_src conf
<<world-racket>>
dev-build/cmake
dev-libs/libconfig
sys-libs/db
dev-lang/tk
media-gfx/imagemagick
media-gfx/renderdoc
app-text/poppler
#+end_src
*** patches
**** media-gfx/renderdoc
https://bugs.gentoo.org/853856
https://git.alpinelinux.org/aports/tree/community/renderdoc/musl-fix.patch
#+begin_src diff :tangle ./docker-profile/base/musl-renderdoc-plthook-elf.patch :mkdirp yes
diff --git a/renderdoc/3rdparty/plthook/plthook_elf.c b/renderdoc/3rdparty/plthook/plthook_elf.c
index 612f689d6..907e7f63e 100644
--- a/renderdoc/3rdparty/plthook/plthook_elf.c
+++ b/renderdoc/3rdparty/plthook/plthook_elf.c
@@ -233,7 +233,11 @@ int plthook_open_by_address(plthook_t **plthook_out, void *address)
     struct link_map *lmap = NULL;
 
     *plthook_out = NULL;
+#ifdef __GLIBC__
     if (dladdr1(address, &info, (void**)&lmap, RTLD_DL_LINKMAP) == 0) {
+#else
+    if (dladdr(address, &info) == 0) {
+#endif
         set_errmsg("dladdr error");
         return PLTHOOK_FILE_NOT_FOUND;
     }
@@ -243,7 +247,7 @@ int plthook_open_by_address(plthook_t **plthook_out, void *address)
 
 static int plthook_open_executable(plthook_t **plthook_out)
 {
-#if defined __linux__
+#if defined __linux__ && defined __GLIBC__
     return plthook_open_real(plthook_out, _r_debug.r_map);
 #elif defined __sun
     const char *auxv_file = "/proc/self/auxv";
diff --git a/renderdoc/os/os_specific.h b/renderdoc/os/os_specific.h
index cc9a6b09e..844597450 100644
--- a/renderdoc/os/os_specific.h
+++ b/renderdoc/os/os_specific.h
@@ -31,6 +31,7 @@
 
 #pragma once
 
+#include <time.h>
 #include <stdarg.h>
 #include <stddef.h>
 #include <stdint.h>
diff --git a/renderdoc/os/posix/linux/linux_hook.cpp b/renderdoc/os/posix/linux/linux_hook.cpp
index 4989e2865..0acb3ac0b 100644
--- a/renderdoc/os/posix/linux/linux_hook.cpp
+++ b/renderdoc/os/posix/linux/linux_hook.cpp
@@ -36,6 +36,10 @@
 #include "plthook/plthook.h"
 #include "strings/string_utils.h"
 
+#ifndef __GLIBC__
+#define RTLD_DEEPBIND 0
+#endif
+
 Threading::CriticalSection libLock;
 
 RDOC_EXTERN_CONFIG(bool, Linux_Debug_PtraceLogging);
#+end_src

https://git.alpinelinux.org/aports/tree/community/renderdoc/no-execinfo.patch
#+begin_src diff :tangle ./docker-profile/base/musl-renderdoc-execinfo.patch :mkdirp yes
diff --git a/renderdoc/os/posix/linux/linux_callstack.cpp b/renderdoc/os/posix/linux/linux_callstack.cpp
index f0b44b0..558765d 100644
--- a/renderdoc/os/posix/linux/linux_callstack.cpp
+++ b/renderdoc/os/posix/linux/linux_callstack.cpp
@@ -27,7 +27,6 @@
 #define _GNU_SOURCE
 #endif
 
-#include <execinfo.h>
 #include <link.h>
 #include <stdio.h>
 #include <string.h>
@@ -66,7 +65,7 @@ private:
   {
     void *addrs_ptr[ARRAY_COUNT(addrs)];
 
-    int ret = backtrace(addrs_ptr, ARRAY_COUNT(addrs));
+    int ret = 0;
 
     numLevels = 0;
     if(ret > 0)
#+end_src
** dynapad-user
*** build
#+name: &musl-build-dynapad-user
#+begin_src screen
docker build \
--tag tgbugs/musl:dynapad-user \
--build-arg UID=<<&UID>> \
--file musl/dynapad-user/Dockerfile musl/dynapad-user
#+end_src

*** file
#+name: &musl/dynapad-user
#+begin_src dockerfile :tangle ./musl/dynapad-user/Dockerfile
FROM tgbugs/musl:dynapad-base

COPY --from=tgbugs/common:user / /

<<&musl-file-user-base>>
#+end_src

** dynapad
*** run
Once you have created the =tgbugs/musl:dynapad= image (see the build
section below) you can use this command to run it and commit on close
each time so as not to lose any work. You will probably want to mount
any additional directories you will need .e.g for images using =-v=.

**** linux
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v ~/git/dynapad:/home/dynapad/git/dynapad \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:dynapad \
sh -c 'pushd ~/git/dynapad && racketcgc -it apps/paddraw/paddraw.rkt'

# docker commit $(docker ps -lq) tgbugs/musl:dynapad
#+end_src
**** macos
See [[#macos-notes][macos notes]] for notes on getting docker working
with XQuartz.  Assuming everything is set up correctly you can the run
the following.
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-v ~/git/dynapad:/home/dynapad/git/dynapad \
-e DISPLAY=host.docker.internal:0 \
-it tgbugs/musl:dynapad \
sh -c 'pushd ~/git/dynapad && racketcgc -it apps/paddraw/paddraw.rkt'

# docker commit $(docker ps -lq) tgbugs/musl:dynapad
#+end_src

#+begin_src bash
xattr -d -r -s com.apple.quarantine /Applications/Docker.app
#+end_src

*** build
Since we need to mount the git directory from outside the image we
can't use a docker file. Commit the image after these steps are
finished (the commands above do that automatically).

If your UID is something other than 1000 you will probably want to
rebuild =tgbugs/musl:dynapad-user= so that your UID matches.

#+begin_src bash
docker pull tgbugs/musl:dynapad-user

docker run \
-v ~/git/dynapad:/home/dynapad/git/dynapad \
-it tgbugs/musl:dynapad-user
docker commit $(docker ps -lq) tgbugs/musl:dynapad
#+end_src

In the image run the following and then exit, the commit will be made
automatically. *NOTE* You may need to remove =build_musl= if it
already exists.

#+begin_src bash
function dynapad-build-all () {
local build_dir SUBPATH SO_SUFFIX
#local PATH
#PATH=~/git/NOFORK/racket/racket/bin/:/usr/bin:/bin
# command -v racketcgc
# command -v racocgc
build_dir=${1:-build}
    mkdir ${build_dir}
    pushd ${build_dir}
        cmake .. -G Ninja -DCMAKE_BUILD_TYPE=Debug || return $?
        ninja || return $?
    popd
    SUBPATH=$(racketcgc -e "(display (path->string (system-library-subpath)))")
    SO_SUFFIX=$(racketcgc -e "(display (bytes->string/utf-8 (system-type 'so-suffix)))")
    mkdir -p dynapad/compiled/bc/native/${SUBPATH}
    racocgc ctool --cgc \
            ++ldf -Wl,-rpath,"${PWD}/${build_dir}/" \
            --ld dynapad/compiled/bc/native/${SUBPATH}/libdynapad_rkt${SO_SUFFIX} \
            "${PWD}/${build_dir}/libdynapad${SO_SUFFIX}"
    racocgc pkg install dynapad-collects/ dynapad/
    racocgc make apps/paddraw/paddraw.rkt
    racocgc make apps/uberapp/uberapp.rkt
}
#+end_src

#+begin_src bash
dynapad-build-all build_musl
#+end_src

** sparcron
:PROPERTIES:
:CUSTOM_ID: sparcron
:END:
*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:sparcron
#+end_src

*** build
#+name: &musl-build-sparcron
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:sparcron \
--file musl/sparcron/Dockerfile musl/sparcron
#+end_src

*** file
# when deriving from multiple parent worlds docker does not compose
# well at all, so we have to pick a primary world line so to speak
#+name: &musl/sparcron
#+begin_src dockerfile :tangle ./musl/sparcron/Dockerfile
FROM tgbugs/musl:binpkg-only

<<&build-world-common>>

RUN echo 1  # if you need to remerge without the world bump the number (ref:sparcron-echo)

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q -uDN dev-python/sparcur \
<<&archive-or-rm>>
#+end_src

*** world
# FIXME sparcur ebuild could pull this all in under test + cron or
# something like that , but the variant use flags issue rears its head,
# and more to the point, redict and rabbitmq-server could be running
# elsewhere and are requirements for this image, or meta ebuild but not
# actually for sparcur. In principle We could move to managing some of
# these world files as meta ebuilds in some cases.
# TODO probably don't need pip anymore except for debug?
# FIXME ideally we wouldn't need xdg-user-dirs at runtime, only during build
# but that would likely require the use of a generic builder instead of
# reuse of the base image for the -user variant as the builder
#+name: world-sparcron
#+begin_src conf :tangle ./musl/sparcron/world
tgbugs-meta/sparcron-meta
#+end_src

#+begin_src conf
dev-db/redict
dev-python/pip
dev-python/sparcur
net-misc/rabbitmq-server
www-servers/uwsgi
x11-misc/xdg-user-dirs
#+end_src

** sparcron-user
:PROPERTIES:
:CUSTOM_ID: sparcron-user
:END:
*** run
# FIXME SIGH yes indeed chown modifies the parent system permissions
# somehow this doesn't surprise me given how janky docker is in here
To run as a daemon use =-d= option in then =docker ps= to get the
container id, and then =docker logs -f= the container.

Development =docker run= example.
#+begin_src screen
_f=$(basename ~/ni/dev/sparc-curation-*.json)
docker run \
-v /var/lib/sparc/files:/var/lib/sparc/files \
-v /var/lib/sparc/.local/share/sparcur:/var/lib/sparc/.local/share/sparcur \
-v /var/lib/sparc/.cache:/var/lib/sparc/.cache \
-v /var/lib/sparc/.config/sparcur/docker-config.yaml:/var/lib/sparc/.config/sparcur/config.yaml \
-v ~/ni/dev/secrets-sparcron.yaml:/var/lib/sparc/.config/orthauth/secrets.yaml \
-v ~/ni/dev/${_f}:/var/lib/sparc/.config/orthauth/${_f} \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-e SCIGRAPH_API=http://192.168.1.207:9000/scigraph \
-it tgbugs/musl:sparcron-user
#+end_src

Production =docker run= example. Must be run as root.
#+name: &run-sparcron
#+begin_src screen
function docker-run-sparcron () {
_f=$(su sparc -c 'basename /var/lib/sparc/.config/pyontutils/sparc-curation-*.json')
docker run \
-d \
-v /var/lib/sparc/files:/var/lib/sparc/files \
-v /var/lib/sparc/.local/share/sparcur:/var/lib/sparc/.local/share/sparcur \
-v /var/lib/sparc/.cache:/var/lib/sparc/.cache \
-v /var/lib/sparc/.config/sparcur/docker-config.yaml:/var/lib/sparc/.config/sparcur/config.yaml \
-v /var/lib/sparc/.config/pyontutils/secrets.yaml:/var/lib/sparc/.config/orthauth/secrets.yaml \
-v /var/lib/sparc/.config/pyontutils/${_f}:/var/lib/sparc/.config/orthauth/${_f} \
-e SCIGRAPH_API=https://scigraph.olympiangods.org/scigraph \
-e UWSGI_SOCKET_SPARCRON=0.0.0.0:7260 \
-p 7260:7260 \
-it tgbugs/musl:sparcron-user
}
#+end_src

Production image restart.
#+name: &restart-sparcron
#+begin_src bash
function docker-restart-sparcron () {
_image=tgbugs/musl:sparcron-user
_cid_old=$(docker ps -lqf ancestor=${_image} -f status=running)
docker stop ${_cid_old} || return $?
docker-run-sparcron
_cid_new=$(docker ps -lqf ancestor=${_image})
docker logs -f ${_cid_new}
}
#+end_src

Production image upgrade.
#+name: &upgrade-sparcron
#+begin_src bash
function upgrade-sparcron () {
_image=tgbugs/musl:sparcron-user
_image_new=${_image}
_image_date=$(date -I --date $(docker inspect ${_image} --format '{{.Created}}'))
_cid_old=$(docker ps -lqf ancestor=${_image} -f status=running)
docker tag ${_image} ${_image}-${_image_date}
docker pull ${_image_new} || return $?
docker stop ${_cid_old} || return $?
docker-run-sparcron
_cid_new=$(docker ps -lqf ancestor=${_image_new})
docker logs -f ${_cid_new}
}
#+end_src

Production image archive.
#+begin_src bash :eval never :dir /su::
for img in $(docker image ls tgbugs/musl:sparcron-user-* --format '{{.Repository}}:{{.Tag}}'); do
docker save "${img}" | gzip > /mnt/str/sparc/docker-images/"${img//\//-}".tar.gz
done

for img in $(docker image ls tgbugs/musl:sparcron-user-* --format '{{.Repository}}:{{.Tag}}' | sort | head -n -1); do
docker image rm "${img}" || break
done
#+end_src
**** An example of how to rerun one or more datasets
As user in docker group (or if using =sudo= don't forget to add the =sudo= before the nested =docker ps=) run the following.
Multiple values for =dataset_id_to_rerun= can be provided separated by a space.
#+begin_src bash
docker exec --user 836 -it $(docker ps -lqf ancestor=tgbugs/musl:sparcron-user) pypy3 -m sparcur.sparcron.rerun ${dataset_id_to_rerun}
#+end_src
=dataset_id_to_rerun= can be a raw uuid, =N:dataset:{uuid}=, or =dataset:{uuid}=

If there was e.g. a network failure that caused multiple datasets to fail and they all need to be rerun you can use the following command.
#+begin_src bash
docker exec --user 836 -it $(docker ps -lqf ancestor=tgbugs/musl:sparcron-user) pypy3 -m sparcur.sparcron.rerun --all
#+end_src

you can also check the status by running
#+begin_src bash
docker exec --user 836 -it $(docker ps -lqf ancestor=tgbugs/musl:sparcron-user) pypy3 -m sparcur.sparcron.status
#+end_src

**** An example of how to force rerun (e.g. after cache clear)
as user in docker group
#+begin_src bash
docker exec --user 836 -it $(docker ps -lqf ancestor=tgbugs/musl:sparcron-user) /bin/bash
#+end_src
# -v ${PWD}/bin:/tmp/bin  sadly cannot mount at this stage

as sparc user move the old caches out, or better just do it in the exec shell
#+begin_src bash
pushd /var/lib/sparc/.cache/sparcur
mv pennsieve-meta pennsieve-meta-$(date -I)
popd
pushd /var/lib/sparc/.cache
mv idlib idlib-$(date -I)
popd
pypy3 -m sparcur.sparcron.rerun
#+end_src

**** An example of how to rapidly redeploy changes using the tools here.
# FIXME move to build section probably?
1. There is a bug. Oh no! e.g. you forgot to bump __internal_version__ so nothing reruns.
2. Change sparcur code, commit, push.
3. bump ~echo~ in [[(sparcron-echo)]]
4. ~C-ucvt~ aka src_elisp[:eval never]{(org-babel-tangle '(4))} or ~./source.org tangle~

On the build machine.
#+begin_src bash :noweb yes
builder-arb --usepkg=y --onlydeps --onlydeps-with-rdeps=n sparcur
builder-arb --nodeps sparcur

./source.org build-image tgbugs/musl:sparcron  # FIXME need a way to enable blocking for sequential operation
./source.org build-image tgbugs/musl:sparcron-user

<<&musl-build-sparcron>> &&

<<&musl-build-sparcron-user>> &&

docker push tgbugs/musl:sparcron; docker push tgbugs/musl:sparcron-user
#+end_src
If the buildcache is missing for sparcron, move the rebuild to the top of
sparcron-user.

On prod as root.
#+begin_src bash :noweb yes :eval never
<<&run-sparcron>>
<<&upgrade-sparcron>>
upgrade-sparcron
#+end_src

*** test
Since tests currently use screen, the suggestion mentioned in
=*musl**package-builder***run= to add the =_path_= vars to your shell
rc file (e.g. [[~/.bashrc]]) applies here as well. In the future if we
move away from using =ob-screen= then passing via =--user-init-file=
on the command line is also and option.

#+name: &test-sparcron-user
#+begin_src screen
<<&screen-set-vars>>
docker run \
\
-v ${_path_sparcron_sparcur_config}:/var/lib/sparc/.config/sparcur/config.yaml \
-v ${_path_sparcron_secrets}:/var/lib/sparc/.config/orthauth/secrets.yaml \
-v ${_path_sparcron_gsaro}:/var/lib/sparc/.config/orthauth/$(basename ${_path_sparcron_gsaro}) \
\
-v ${_path_dockerfiles}/musl/sparcron-user/entrypoints/test.sh:/etc/entrypoints/test.sh \
--entrypoint /etc/entrypoints/test.sh \
--rm \
tgbugs/musl:sparcron-user
#+end_src

**** unused :noexport:
Since we are passing these via ref:&screen-set-vars at the moment we
do not need these, and if we switch to running via bash or something
then we can use =:var _path_sparcron_sparcur_config=asdf= directly.
#+name: &var-sparcron-sparcur-config
#+begin_src elisp
path-sparcron-sparcur-config
#+end_src

#+name: &var-sparcron-secrets
#+begin_src elisp
path-sparcron-secrets
#+end_src

#+name: &var-sparcron-gsaro
#+begin_src elisp
path-sparcron-gsaro
#+end_src
*** build
#+name: &musl-build-sparcron-user
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:sparcron-user \
--file musl/sparcron-user/Dockerfile musl/sparcron-user
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/sparcron-user/Dockerfile
ARG START_IMAGE=tgbugs/musl:sparcron

FROM ${START_IMAGE}

ADD --chown=836:836 idlib-config.yaml /var/lib/sparc/.config/idlib/config.yaml
ADD --chown=836:836 sparcur-config.yaml /var/lib/sparc/.config/sparcur/config.yaml

ARG HOME=/var/lib/sparc

WORKDIR $HOME

RUN \
su "$(id -nu 836)" -c "xdg-user-dirs-update; rmdir Desktop Pictures Documents Public Downloads Templates Music Videos > /dev/null 2>&1;"

ADD entrypoints /etc/entrypoints
ADD entrypoint.sh /etc/entrypoint.sh

ENTRYPOINT ["/etc/entrypoint.sh"]
#+end_src

*** configs
This config provides sane defaults, if you need to add a section (such
as a =datasets-no:=) then use =-v= in docker run to mount over the
location of this file.
#+begin_src yaml :tangle ./musl/sparcron-user/sparcur-config.yaml
auth-stores:
  secrets:
    path: '{:user-config-path}/orthauth/secrets.yaml'
auth-variables:
  google-api-service-account-file-readonly:
    path: google api saro
  hypothesis-api-key: hypothesis api default-user
  hypothesis-group: hypothesis group sparc-curation
  remote-organization: N:organization:618e8dd9-f8d2-4dc4-9abb-c6aaab2e78a0
  remote-organizations:
    - N:organization:618e8dd9-f8d2-4dc4-9abb-c6aaab2e78a0  # sparc
    - N:organization:f08e188e-2316-4668-ae2c-8a20dc88502f  # rejoin
#+end_src

# TODO
#+begin_src yaml :tangle ./musl/sparcron-user/idlib-config.yaml
auth-stores:
  secrets:
    path: '{:user-config-path}/orthauth/secrets.yaml'
auth-variables:
  protocols-io-api-client-token:
    path: protocols-io api client-token
#+end_src

*** entrypoints
# https://forums.gentoo.org/viewtopic-t-1014086-start-0.html
# sigh this really doesn't need the net admin cap just to start rabbit
Slowly inching toward being able to run this via a service manager.
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/sparcron-user/entrypoint.sh :mkdirp yes
if [ -n "${UWSGI_SOCKET_SPARCRON}" ]; then
    echo "UWSGI_SOCKET_SPARCRON=${UWSGI_SOCKET_SPARCRON}" >> /etc/conf.d/sparcron-server
fi

rc-status
touch /run/openrc/softlevel
/etc/init.d/redict start
/etc/init.d/rabbitmq start
/etc/init.d/sparcron-server start

# testing and ensure that there is an existing cache on first run
runuser -u sparc -- pypy3.11 -m sparcur.sparcron

runuser -u sparc -- sh -c 'EPYTHON=pypy3.11 PYTHONBREAKPOINT=0 celery --app sparcur.sparcron worker -n wcron -Q cron,default --concurrency=1 --beat --schedule-filename ./sparcur-cron-schedule --loglevel=INFO' &

runuser -u sparc -- sh -c 'EPYTHON=pypy3.11 PYTHONBREAKPOINT=0 celery --app sparcur.sparcron worker -n wexport -Q export --loglevel=INFO'
# FIXME missing rmeta stuff ?

#/etc/init.d/redict stop
#/etc/init.d/rabbitmq stop
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./musl/sparcron-user/entrypoints/test.sh :mkdirp yes
if [ -n "${UWSGI_SOCKET_SPARCRON}" ]; then
    echo "UWSGI_SOCKET_SPARCRON=${UWSGI_SOCKET_SPARCRON}" >> /etc/conf.d/sparcron-server
fi

rc-status
touch /run/openrc/softlevel

# check that services are working as expected
/etc/init.d/redict start
RED_OOPS=$?
/etc/init.d/rabbitmq start
RAB_OOPS=$?
/etc/init.d/sparcron-server start
SVR_OOPS=$?

[ ${RED_OOPS} -eq 0 ] || { echo redict failed to start;}
[ ${RAB_OOPS} -eq 0 ] || { echo rabbitmq failed to start;}
[ ${SVR_OOPS} -eq 0 ] || { echo sparcron-server failed to start;}
[ ${RED_OOPS} -ne 0 ] || [ ${RAB_OOPS} -ne 0 ] || [ ${SVR_OOPS} -ne 0 ] && { echo at least on service failed; exit 1;}

su sparc -c 'pypy3 -m sparcur.sparcron' || { echo startup check failed; exit 2;}
#+end_src

** sparcron-live
so the problem here is at least twofold
1. package.accept_keywords needs to be set appropriately for the packages in questions
2. we somehow have to build and rebuild the 9999 ebuilds automatically knowing that we
   also want to have the versioned ebuilds present too, this might be a case where
   ~ACCEPT_KEYWORDS='**'~ along with ~--buildpkgonly~ might be what we need and live.world
   being separate might help

*** build
#+name: &musl-build-sparcron-live
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:sparcron-live \
--file musl/sparcron-live/Dockerfile musl/sparcron-live
#+end_src

*** rebuild
#+begin_src screen
builder-smart-live-rebuild
builder-smart-also-live-rebuild
#+end_src

FIXME for the time being we are just going to do the full rebuild will
figure out how to reduce rework later.
#+begin_src bash
./source.org build-image tgbugs/musl:sparcron-live --no-cache
./source.org build-image tgbugs/musl:sparcron-live-user
#+end_src

sanity check to make sure updated as expected
#+begin_src bash
pushd ~/git/sparc-curation > /dev/null 2>&1;
git fetch > /dev/null 2>&1;
git rev-parse origin/master;
popd > /dev/null 2>&1

docker run --rm --entrypoint /usr/bin/python tgbugs/musl:sparcron-live-user -c "import sparcur as mod; print(mod.__version__)"
#+end_src

#+begin_src bash
docker push tgbugs/musl:sparcron-live-user; docker push tgbugs/musl:sparcron-live
#+end_src

*** file
#+name: &musl/sparcron-live
#+begin_src dockerfile :tangle ./musl/sparcron-live/Dockerfile
FROM tgbugs/musl:binpkg-only

ADD package.accept_keywords /etc/portage/package.accept_keywords/99-live-sparcron

<<&build-world-common>>
#+end_src

*** world
#+name: world-sparcron-live
#+begin_src conf :tangle ./musl/sparcron-live/world
=tgbugs-meta/sparcron-meta-9999
#+end_src

*** package.accept_keywords
#+begin_src conf :tangle ./musl/sparcron-live/package.accept_keywords
tgbugs-meta/*::tgbugs-overlay **
dev-python/*::tgbugs-overlay **
#+end_src

** sparcron-live-user
*** build
#+name: &musl-build-sparcron-live-user
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:sparcron-live-user \
--build-arg START_IMAGE=tgbugs/musl:sparcron-live \
--file musl/sparcron-user/Dockerfile musl/sparcron-user
#+end_src

** protc
See [[*kg-dev-user]].
*** world
#+name: world-protc
#+begin_src conf
dev-libs/redland
#+end_src
* musl static
Same metadata as musl except with =static-libs= in order to produce
statically linked binaries.
** static-xorg
*** run
debug
#+begin_src screen
docker run \
--net host \
--add-host local.binhost:127.0.0.1 \
--volumes-from local-repos-snap \
-v ~/files/binpkgs/multi:/var/cache/binpkgs \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=${DISPLAY} \
--rm \
-it \
tgbugs/musl:static-xorg
#+end_src
*** build
# XXX warning, the harfbuzz/freetype issue can probably be avoided here
# because only freetype has the static-libs use flag
#+name: &musl-build-static-xorg
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:static-xorg \
--build-arg PROFILE='docker-profile:tgbugs/musl/static/x' \
--build-arg START_IMAGE='tgbugs/musl:updated' \
--file musl/xorg/static.Dockerfile musl/xorg
#+end_src
# --build-arg START_IMAGE='tgbugs/musl:xorg' \
*** file
#+begin_src dockerfile :tangle ./musl/xorg/static.Dockerfile
<<&xorg-nox-common-1>>
<<&profile-static-x>>
<<&xorg-nox-common-2>>
<<&xorg-nox-common-3>>
#+end_src
** static-package-builder
*** populate 0
#+name: &musl-run-static-xorg-quickpkg
#+begin_src bash
quickpkg-image tgbugs/musl:static-xorg
#+end_src
*** run
#+begin_src bash
docker run tgbugs/musl:static-package-builder
docker commit $(docker ps -lq) tgbugs/musl:static-package-builder-snap

docker run \
--volumes-from local-repos-snap \
-v ${_path_binpkgs}:/var/cache/binpkgs \
-v ${_path_distfiles}:/var/cache/distfiles \
tgbugs/musl:static-package-builder-snap \
emerge --color=y -q --keep-going -uDN @docker

docker commit $(docker ps -lq) tgbugs/musl:static-package-builder-snap
#+end_src
# --volumes-from cross-sbcl \
# FIXME racket failing with mkostemp failures during raco make or
# something !? what the fuck?  how was this not caught before ?!
*** build
#+name: &musl-build-static-package-builder
#+begin_src screen
docker build \
--tag tgbugs/musl:static-package-builder \
--file musl/package-builder/static.Dockerfile musl/package-builder
#+end_src
*** file
FIXME sbcl patches require a manual step [[&sbcl-funs]].
#+name: &file-package-builder-static
#+begin_src dockerfile :tangle ./musl/package-builder/static.Dockerfile
FROM tgbugs/musl:static-xorg

<<&musl-package-builder-common>>

# overwrite the regular world file
ADD static.world /etc/portage/sets/docker
ADD sets/static.builder /etc/portage/sets/builder

# sbcl patches and env
# FIXME beware may need to rerun the copy command in [[&sbcl-funs]]
# FIXME turns out that there is no way to detect these patches and dispatch
# the hack is to set docs or source in the non-static so that there is a mismatch
ADD patches/dev-lisp /etc/portage/patches/dev-lisp
ADD static-sbcl.env /etc/portage/env/dev-lisp/sbcl
#+end_src
*** sets
**** builder
#+name: set-builder-musl-static
#+begin_src conf :tangle ./musl/package-builder/sets/static.builder
<<set-builder-common>>
#+end_src

*** world
#+name: world-static-package-builder
#+begin_src conf :tangle ./musl/package-builder/static.world
<<world-package-builder-common-x>>
<<world-sbcl>>
<<world-static-sbcl-c-libs>>
#+end_src

#+name: world-static-sbcl-c-libs
#+begin_src conf
dev-libs/openssl
dev-libs/libgit2
dev-libs/gmp
dev-libs/capstone
dev-libs/mpfr
dev-libs/redland
#+end_src
** static-binpkg-only
*** build
#+name: &musl-build-static-binpkg-only
#+begin_src bash
docker build \
--tag tgbugs/musl:static-binpkg-only \
--file musl/binpkg-only/static.Dockerfile musl/binpkg-only
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/binpkg-only/static.Dockerfile
FROM tgbugs/musl:static-xorg
<<&musl-binpkg-only-common>>
#+end_src

** sbcl
*** test
#+name: &test-musl-sbcl
#+begin_src screen
test-image tgbugs/musl:sbcl
#+end_src

#+header: :shebang "#!/usr/bin/env sh\nset -e"
#+begin_src bash :tangle ./bin/test/tgbugs/musl/sbcl/test :mkdirp yes :noweb yes
<<&bash-at-abs-path>>
sbcl --noinform --noprint --non-interactive --load test.lisp
#+end_src

#+begin_src lisp :tangle ./bin/test/tgbugs/musl/sbcl/test.lisp :mkdirp yes
(let ((have-static-executable (find-symbol "LINKAGE-TABLE-INDEX" 'sb-sys)))
  (format t "~a ~a~%" (lisp-implementation-type) (lisp-implementation-version))
  (format t "core path: ~a~%" (truename sb-ext:*core-pathname*))
  (format t "obj path exists: ~a~%" (probe-file (merge-pathnames "sbcl.o" sb-ext:*core-pathname*)))
  (format t "static executables: ~a~%" (and have-static-executable t))
  (unless have-static-executable
    (error "this version of sbcl does not support static executables"))
  (format t "ok~%")
  (values))
#+end_src
# these are build time features and don't show up in the image itself
# (and (member :sb-linkable-runtime *features*) (member :sb-prelink-linkage-table *features*))

*** build
#+name: &musl-build-sbcl
#+begin_src screen
<<&docker-build>>
--tag tgbugs/musl:sbcl \
--file musl/sbcl/Dockerfile musl/sbcl
#+end_src
*** file
#+begin_src dockerfile :tangle ./musl/sbcl/Dockerfile
FROM tgbugs/musl:static-binpkg-only
<<&build-world-common>>
#+end_src
*** world
#+name: world-sbcl
#+begin_src conf :tangle ./musl/sbcl/world
dev-lisp/sbcl
tgbugs-meta/emacs-meta
#+end_src

#+begin_src conf
<<world-emacs>>
<<world-sbcl-base>>
#+end_src
# FIXME why did we have these on the list before? some build order issue or something? weird ... maybe it has been fixed now?
# dev-lisp/uiop
# dev-lisp/asdf
#+name: world-sbcl-base
#+begin_src conf
dev-lisp/sbcl
#+end_src
*** patches
[[file:~/git/NOFORK/sbcl/README.static-executable]]
make the patches since they are too much to tangle
#+name: &sbcl-funs
#+begin_src bash :noweb yes :tangle ./bin/sbcl-funs.sh
function sbcl-clone-fetch () {
local hrp rp
hrp="./<<&helper-repos()>>"
rp="${hrp}/sbcl"

if [[ ! -d "${hrp}" ]]; then
mkdir "${hrp}"
fi

if [[ ! -d "${rp}" ]]; then
pushd "${hrp}"
  git clone https://github.com/sbcl/sbcl.git
popd
fi

pushd "${rp}"
  git fetch
  git remote | grep -q tgbugs
  exists=$?
  if [ ${exists} -ne 0 ]; then
      git remote add tgbugs https://github.com/tgbugs/sbcl.git
  fi
  git fetch tgbugs
popd

}

function sbcl-get-static-versions () {
local hrp rp
hrp="./<<&helper-repos()>>"
rp="${hrp}/sbcl"

pushd "${rp}" > /dev/null 2>&1
git branch -r --list 'tgbugs/static-executable-v2-*' | awk -F- '{ print $NF }'
popd > /dev/null 2>&1

}

function sbcl-get-portage-versions () {
ls /var/db/repos/gentoo/dev-lisp/sbcl | grep sbcl | sed 's/\.ebuild//' | awk -F- '{ print $2 }'
}

function sbcl-get-portage-versions-external () {
#docker container inspect local-portage-snap > /dev/null || \
#docker create -v /var/db/repos/gentoo --name local-portage-snap docker.io/gentoo/portage:latest /bin/true

#--volumes-from local-portage-snap \
#--rm docker.io/gentoo/stage3:amd64-musl-hardened \

# unfortunately the changes we make in the profile
# mean that a different version would be emerged
# so we have to run this inside run-musl

docker run \
--volumes-from local-repos-snap \
--rm tgbugs/musl:updated \
sh -c "ls /var/db/repos/gentoo/dev-lisp/sbcl | grep sbcl | sed 's/\.ebuild//' | awk -F- '{ print $2 }'"
}

function sbcl-get-would-install () {
emerge -pq --nodeps sbcl 2>/dev/null | grep ebuild | awk '{$1=$1}1' | awk -F- '{ print $NF }'
}

function sbcl-get-would-install-external () {
# used to make sure we have generated the patch
# awk '{$1=$1}1' trims trailing whitespace
#docker container inspect local-portage-snap > /dev/null || \
#docker create -v /var/db/repos/gentoo --name local-portage-snap docker.io/gentoo/portage:latest /bin/true

#--volumes-from local-portage-snap \
#--rm docker.io/gentoo/stage3:amd64-musl-hardened \

# unfortunately the changes we make in the profile
# mean that a different version would be emerged
# so we have to run this inside run-musl

docker run \
--volumes-from local-repos-snap \
--rm tgbugs/musl:updated \
sh -c "emerge -pq --nodeps sbcl 2>/dev/null | grep ebuild | awk '{$1=$1}1' | awk -F- '{ print $NF }'"
}

function sbcl-to-generate () {
comm -12 <(sbcl-get-static-versions) <(sbcl-get-portage-versions)
}

function sbcl-ensure-latest () {
would_install=$(comm -12 <(sbcl-get-static-versions) <(sbcl-get-would-install))
[ -n "${would_install}" ] || return 1
}

function sbcl-static-patch () {
local version tag hrp rp spath tpath patch ipath sighpath
version=${1}
tag=sbcl-${version}
patch=static-${version}.patch
hrp="./<<&helper-repos()>>"
rp="${hrp}/sbcl"
spath=./<<&helper-repos()>>/sbcl/patches
tpath=./docker-profile/static/
ipath=patches/dev-lisp/sbcl-${version}/
sighpath=./<<&helper-repos()>>/sbcl/patches/dev-lisp/sbcl-${version}/${patch}
if [[ ! -f "${sighpath}" ]]; then
  sbcl-clone-fetch
  pushd "${rp}"
    mkdir -p ${ipath}
    # gentoo eapply_user happens after gentoo patches are applied so we have to compensate here :/
    git diff ${tag}..remotes/tgbugs/static-executable-v2-${version} ':(exclude)README*' | \
      sed 's,maybetime sh,maybetime sh -x,' > \
      ${ipath}/${patch}
  popd
fi
# XXX old, we renamed this so that they will work
# this portion has to be repeatable because we zap docker-profile on every tangle
# mkdir -p ${tpath}
# cp -a ${spath} ${tpath}
}

function sbcl-generate-patches () {
for version in $(sbcl-to-generate); do
  #echo generating for sbcl ${version};
  sbcl-static-patch ${version};
done
}
#+end_src

#+name: &musl-run-sbcl-generate-patches-etc
#+begin_src bash :noweb yes
docker run \
--volumes-from local-repos-snap \
-v ${_path_dockerfiles}:/dockerfiles \
-e _uid=$(if [ ${UID} -lt 60000 ]; then echo ${UID}; else echo 1000; fi) \
--rm tgbugs/musl:updated \
bash -c 'git config --global --add safe.directory /dockerfiles/helper-repos/sbcl; pushd /dockerfiles; source bin/sbcl-funs.sh; sbcl-clone-fetch && sbcl-ensure-latest && sbcl-generate-patches; CODE=$?; chown -R ${_uid}:${_uid} /dockerfiles/helper-repos/sbcl; exit ${CODE}' &&
./source.org non-tangled
#+end_src

#+name: &musl-run-sbcl-generate-patches-etc-old
#+begin_src bash
sbcl-clone-fetch && \
sbcl-ensure-latest && \
sbcl-generate-patches && \
./source.org non-tangled
#+end_src

# FIXME TODO consider using noweb eval like we do with
# [[&dev-repo-setup]] the main issue is that we probably
# don't want to have to maintain the full list of versions
# in this file, also this currently runs outside docker
#+name: &sbcl-generate-patches
#+begin_src bash :noweb no-export
<<&sbcl-funs>>
sbcl-generate-patches
#+end_src

Then run =./source.org tangle= and the patches will be copied over correctly in =do-non-tangled-files=.

#+name: &sbcl-env-static
#+begin_src ebuild :tangle ./docker-profile/static/sbcl.env :tangle no
post_src_configure() {
truncate -s $(head -n -2 "${CONFIG}" | wc -c) "${CONFIG}"
sbcl_feature "true" ":sb-linkable-runtime"
sbcl_feature "true" ":sb-prelink-linkage-table"
echo ') list)' >> "${CONFIG}"
}
#+end_src

# <<&sbcl-env>>
#+begin_src ebuild :noweb yes :tangle ./musl/package-builder/static-sbcl.env
<<&sbcl-env-static>>
#+end_src

** sbcl-user
*** run
#+begin_src bash
docker run \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/musl:sbcl-user
#+end_src

*** build
#+name: &musl-build-sbcl-user
#+begin_src screen
docker build \
--tag tgbugs/musl:sbcl-user \
--build-arg UID=<<&UID>> \
--file musl/sbcl-user/Dockerfile musl/sbcl-user
#+end_src

*** file
# TODO emacs setup etc.
#+begin_src dockerfile :tangle ./musl/sbcl-user/Dockerfile
FROM tgbugs/musl:sbcl

COPY --from=tgbugs/common:user / /

<<&musl-file-user-base>>

# FIXME builder to factor common here
<<&emacs-sanity>>

# TODO quicklisp https
#+end_src

** sbcl-stripped
*** run
#+begin_src screen
docker run \
--network host \
--rm \
-it tgbugs/musl:sbcl-stripped \
--eval '(ql:quickload :swank)' \
--eval '(defvar *swank-server* (swank:create-server :port 4009))'
#+end_src

alternate version with slynk
#+begin_src screen
docker run \
--network host \
--rm \
-it tgbugs/musl:sbcl-stripped \
--eval '(ql:quickload :slynk)' \
--eval '(defvar *slynk-server* (slynk:create-server :port 4005))'
#+end_src

now also old since quicklisp https is loaded by default via sbclrc
#+begin_src screen
docker run \
--network host \
--rm \
-it tgbugs/musl:sbcl-stripped \
--load /quicklisp/setup.lisp \
--eval '(asdf:load-system :dexador)' \
--eval '(push :dexador *features*)' \
--eval '(asdf:load-system :quicklisp-https-always)'
#+end_src

It is BEYOND stupid how difficult it is to configure iptables to be
able to connect to 4009 when =--network host= produces the desired
result immediately. No quick solution so moving on as usual. Sigh.

# -v /etc/ssl/certs:/etc/ssl/certs \
# -v /usr/share/ca-certificates:/usr/share/ca-certificates \
FIXME currently mounting ca-certificates from the host
but should probably also retain app-misc/ca-certificates
in the image so that there is at least a default I would
very much like to know how dex is fetching via https without
those certs though ... ah, it retains its own list of root
certs as bundled from mozilla (wowzers) so probably want
to build our own from the host

old
#+begin_src bash
docker run \
--network host \
--rm \
-it tgbugs/musl:sbcl-stripped \
--eval '(asdf:initialize-source-registry (list :source-registry (list :tree #p"/dexador") (list :tree #p"/quicklisp-https") :inherit-configuration))' \
--eval '(asdf:load-system :dexador)' \
--load /quicklisp.lisp \
--eval "(quicklisp-quickstart:install)" \
--eval '(asdf:load-system :quicklisp-https)' \
--eval "(ql:quickload :swank)" \
--eval '(defvar *swank-server* (swank:create-server :port 4009))' \
--eval '(sb-posix:readlink "/proc/1/exe")'

#+end_src

TODO figure out minimal requirements to safely bootstrap to dexador
so we can use ssl from within common lisp ... the most obvious answer
is to use curl which is tiny by comparison, or embed a known hash for
a specific dexador version? ... or make that a prerequisite? urg
more likely the correct thing to do is to grab dex, use it to grab
quicklisp-https, except that given how we are generating these images
there is zero reason not to just use git to securely retrieve the code
we need for bootstrap and leave it as an exercise for the reader if they
want to ... alternately libgit2 and cl-git are another way I think, but
still need to be side loaded if we don't have curl, so we use an existing

XXX dex has a bunch of deps ...

TODO if we want to experiment with woo we need libev

# --eval "(ql:quickload (:swank :dexador))" \

#+begin_src lisp
swank-api:*emacs-connection*
#+end_src

*** build
#+name: &musl-build-sbcl-stripped
#+begin_src screen
docker build \
--tag tgbugs/musl:sbcl-stripped \
--file musl/sbcl-stripped/Dockerfile musl/sbcl-stripped
#+end_src
*** file
# TODO add libgit2 so we can use cl-git
#+begin_src dockerfile :tangle ./musl/sbcl-stripped/Dockerfile :comments no
# syntax=docker/dockerfile:experimental
FROM tgbugs/musl:sbcl AS builder

WORKDIR /build

RUN \
lddtree -l /usr/bin/sbcl /usr/bin/openssl       | \
xargs -n 1 qfile -q                             | \
xargs      qdepends -rpq                        | sed 's/^[<>=~]+//' | sort -u | tr ' ' '\n' | grep -v -e '^virtual/' -e '^!' | \
xargs -n 1 qdepends -rpq                        | sed 's/^[<>=~]+//' | sort -u | tr ' ' '\n' | grep -v -e '^virtual/' -e '^!' | \
xargs -n 1 qdepends -rpq                        | sed 's/^[<>=~]+//' | sort -u | tr ' ' '\n' | grep -v -e '^virtual/' -e '^!' | \
xargs -n 1 qdepends -rpq -F '%{CATEGORY}/%{PN}' | sed 's/^[<>=~]+//' | sort -u | tr ' ' '\n' | grep -v -e '^virtual/' -e '^!' | \
cut -d':' -f1                                   | \
xargs qlist | grep -v -e 'usr/share/doc' -e 'usr/share/man' | xargs -P8 -I{} -n 1 rsync -aR {} /build

RUN \
ln -s usr/lib

#RUN \
#curl -O https://beta.quicklisp.org/quicklisp.lisp

#RUN \
#curl -O https://beta.quicklisp.org/quicklisp.lisp.asc

# amazing how utterly bad the ux is for gpg that it actively makes things less secure
#RUN \
#gpg --verify quicklisp.lisp.asc quicklisp.lisp

RUN \
git clone --branch version-2021-02-13 --depth=1 https://github.com/quicklisp/quicklisp-client.git quicklisp

RUN \
git clone --depth=1 https://github.com/rudolfochrist/ql-https.git /tmp/ql-https

# quicklisp-https does not actually force https >_<
#RUN \
#git clone --depth=1 https://github.com/snmsts/quicklisp-https.git quicklisp/local-projects/quicklisp-https

COPY bootstrap-1.lisp /bootstrap-1.lisp
COPY bootstrap-2.lisp /bootstrap-2.lisp

RUN --network=none \
sbcl --no-userinit --non-interactive \
--load /bootstrap-1.lisp

RUN \
sbcl --core /tmp/continue.core --no-userinit --non-interactive \
--load /bootstrap-2.lisp

COPY quicklisp-https-always quicklisp/local-projects/quicklisp-https-always

FROM scratch

WORKDIR /

COPY --from=builder /build /

COPY bootstrap-3.lisp /.sbclrc

RUN --network=none \
["/usr/bin/sbcl", \
"--no-userinit", \
"--non-interactive", \
"--load", "/.sbclrc"]

ENTRYPOINT ["/usr/bin/sbcl"]
#+end_src

*** quicklisp https bootstrap files
#+name: lisp-ql-https-bootstrap-1
#+begin_src lisp :tangle ./musl/sbcl-stripped/bootstrap-1.lisp :mkdirp yes
(asdf:initialize-source-registry
 (list :source-registry (list :tree #p"/tmp/ql-https")
       (list :tree #p"/build/quicklisp") :inherit-configuration))
(defpackage #:ql-setup)
; don't forget the trailing slash for files >_<
(defvar ql-setup::*quicklisp-home* (probe-file "/build/quicklisp/"))
(load "/tmp/ql-https/ql-setup.lisp")
(asdf:load-system :ql-https)
(save-lisp-and-die "/tmp/continue.core" :purify nil)
#+end_src

#+name: lisp-ql-https-bootstrap-2
#+begin_src lisp :tangle ./musl/sbcl-stripped/bootstrap-2.lisp :mkdirp yes
(setf ql-https::*quietly-use-https* t)
(quicklisp:setup)
(ql:quickload :dexador) ; FIXME this breaks when building kg-dev-user
#+end_src

=bootstrap-3.lisp= doubles as clrc for sbcl-stripped.

#+name: lisp-ql-https-bootstrap-3
#+begin_src lisp :tangle ./musl/sbcl-stripped/bootstrap-3.lisp :mkdirp yes
(let ((quicklisp-init
        (if (= 0 (sb-unix:unix-getuid))
            ;; use sb-unix:unix-getuid instead of sb-posix:getuid
            ;; because sb-posix: is not available at bootstrap-3 time
            "/quicklisp/setup.lisp"
            (merge-pathnames
             "quicklisp/setup.lisp"
             (user-homedir-pathname)))))
  (if (probe-file quicklisp-init)
      (load quicklisp-init)
      (error "path to quicklisp-init is missing!? ~a" quicklisp-init)))
#-dexador
(asdf:load-system :dexador :verbose nil)
#-dexador
(push :dexador *features*)
#-quicklisp-https-always
(asdf:load-system :quicklisp-https-always :verbose nil)
#-quicklisp-https-always
(push :quicklisp-https-always *features*)
#+end_src

There are many other things that could go in clrc but these are
critical for ensuring that quicklisp always uses https.
#+name: lisp-ql-https-clrc
#+begin_src lisp :tangle ./musl/sbcl-stripped/clrc :mkdirp yes
(let ((quicklisp-init
        (merge-pathnames
         "quicklisp/setup.lisp"
         (user-homedir-pathname))))
  (if (probe-file quicklisp-init)
      (load quicklisp-init)
      (error "path to quicklisp-init is missing!? ~a" quicklisp-init)))
#-dexador
(asdf:load-system :dexador :verbose nil)
#-dexador
(push :dexador *features*)
#-quicklisp-https-always
(asdf:load-system :quicklisp-https-always :verbose nil)
#-quicklisp-https-always
(push :quicklisp-https-always *features*)
#+end_src

*** quicklisp-https-always
#+name: lisp-qlha-lisp
#+begin_src lisp :tangle ./musl/sbcl-stripped/quicklisp-https-always/quicklisp-https-always.lisp :mkdirp yes
(defpackage quicklisp-https-always
  (:use :cl)
  (:export :setup))

(in-package :quicklisp-https-always)

,#+quicklisp
(defun fetch-via-dexador (url file &key (follow-redirects t) quietly (maximum-redirects 10))
  "Request URL and write the body of the response to FILE."
  (declare (ignorable follow-redirects quietly maximum-redirects))
  (let ((url-obj (ql-http:url url)))
    (setf (ql-http::scheme url-obj) "https")
    (let ((url-string (ql-http::urlstring url-obj)))
      (format t "FETCHING VIA dex: ~s~%" url-string)
      (dex:fetch url-string file :if-exists :supersede)))
  (values (make-instance 'ql-http::header :status 200)
          (probe-file file)))

(defun setup (&key (overwirte t) (methods '("https" "http")))
  (declare (ignorable overwirte methods))
  ,#+quicklisp
  (dolist (x methods)
    (when (or (not (find x ql-http:*fetch-scheme-functions* :test 'equal :key 'first))
              overwirte)
      (setf ql-http:*fetch-scheme-functions*
            (acons x 'fetch-via-dexador
                   (remove x ql-http:*fetch-scheme-functions* :key 'first :test 'equal))))))

(setup)
#+end_src

#+name: lisp-qlha-asdf
#+begin_src lisp :tangle ./musl/sbcl-stripped/quicklisp-https-always/quicklisp-https-always.asd
(in-package :asdf-user)

(defsystem quicklisp-https-always
  :version "0.0.0"
  :author "Tom Gillespie" ; adapted from "SANO Masatoshi"
  :license "MIT"
  :depends-on (:dexador)
  :components ((:file "quicklisp-https-always"))
  :description "Patch to force quicklisp to always use https. Adapted from quicklisp-https.")
#+end_src

*** extra :noexport:
extras
#+begin_src bash
lddtree -l /usr/bin/sbcl | xargs -n 1 qfile -q | xargs qlist
qdepends -rp -q sbcl | xargs -n 1 qdepends -rp | xargs -n 1 qdepends -rp

qdepends -rp -q sbcl | xargs -n 1 qdepends -rp | xargs -n 1 qdepends -rpq | xargs -n 1 qdepends -rp -q -F '%{CATEGORY}/%{PN}' | sort -u | cut -d':' -f1 | qlist

qlist /usr/bin/sbcl
qlist openssl
lddtree -l /usr/bin/sbcl
#+end_src

some insights into relative runtime sizes the results for these are
compressed and I have made zero effort to strip these builds for light
weight images because the point of is to be fully functional with the
source code and whatever documentation comes along in most cases, more
to the point what this means is that baring some serious work to slim
down these things, a full polyglot runtime is annoyingly large

the above written before I check haskell ... LOL HASKELL
racket has 3 full implementations in my distribution, but still, very, very large
for comparison the other schemes are all tiny under 10M
gcc and llvm both pushing 70M, rust 91M ... anyway ... no way are we going to be
able to optimize the size of these things, so the minimal sbcl image is for other
projects, not for prrequaestor which depends on far too many other runtimes

#+begin_src bash
pushd ~/files/binpkgs/multi
ls -alh \
dev-lisp/sbcl \
net-libs/nodejs \
dev-lang/python \
dev-python/pypy3 \
app-editors/emacs \
dev-lang/ghc \
dev-scheme/racket \
sys-devel/gcc \
sys-devel/llvm \
dev-lang/rust \
dev-lang/erlang \
dev-lang/perl \
dev-vcs/git \
dev-libs/openssl

#+end_src

#+begin_src dockerfile
# ugh python is huge ... so is emacs
RUN \
   mkdir /cpython \
&& qlist python-3.10 | xargs -P8 -I{} -n 1 rsync -aR {} /cpython \
&& du -hxd1 /cpython \
&& mkdir /pypy3
&& qlist pypy3       | xargs -P8 -I{} -n 1 rsync -aR {} /pypy3 \
&& du -hxd1 /pypy3 \
&& mkdir /emacs \
&& qlist emacs       | xargs -P8 -I{} -n 1 rsync -aR {} /emacs \
&& du -hxd1 /emacs
&& mkdir /nodejs \
&& qlist nodejs       | xargs -P8 -I{} -n 1 rsync -aR {} /nodejs \
&& du -hxd1 /nodejs
# 123M /cpython
# 194M /pypy3
# 162M /emacs
# 49M  /nodejs
# 4.0M /curl

# TODO keep other things needed for e.g. prrequaestor
RUN \
echo git nifstd-tools neurondm |
xargs      qdepends -rpq                        | sort -u | tr ' ' '\n' | grep -v '^!<net-misc/openssh' | \
xargs -n 1 qdepends -rpq                        | sort -u | tr ' ' '\n' | grep -v '^!<net-misc/openssh' | \
xargs -n 1 qdepends -rpq                        | sort -u | tr ' ' '\n' | grep -v '^!<net-misc/openssh' | \
xargs -n 1 qdepends -rpq -F '%{CATEGORY}/%{PN}' | sort -u | tr ' ' '\n' | grep -v '^!<net-misc/openssh' | \
cut -d':' -f1

#+end_src

#+begin_src lisp
sb-sys:*linkage-info*
(defun linfo (&key (make-undefined nil))
  (let ((ht (car sb-sys:*linkage-info*))
        (undefined-entries (cdr sb-sys:*linkage-info*))
        out)
    (loop
      :for key :being :the :hash-keys :in ht :using (hash-value idx)
      :for datap := (listp key)
      :for name := (if datap (first key) key)
      :for undefinedp := (not (null (or (member key undefined-entries :test #'equal)
                                        (member name make-undefined :test #'equal))))
      :do (push (cons idx (list name datap undefinedp)) out))
    (let ((*print-pretty* nil))
      (prin1 (mapcar #'cdr (sort out #'< :key #'car))))))
#+end_src
* gnu
may not need this if we can use crossdev to build glibc sbcl on musl that that seems a stretch
** eselect-repo
*** build
# TODO see if we can get the prologue running everywhere not sure if ti will tangle though
#+name: &gnu-build-eselect-repo
#+begin_src screen :prologue (format "echo running block %S" (plist-get (nth 1 (org-element-at-point)) :name))
docker build \
--tag tgbugs/gnu:eselect-repo \
--network host \
--add-host local.binhost:127.0.0.1 \
--file gnu/eselect-repo/Dockerfile gnu/eselect-repo
#+end_src

*** file
#+begin_src dockerfile :tangle ./gnu/eselect-repo/Dockerfile
FROM docker.io/gentoo/stage3:amd64-hardened-openrc

<<&gentoo-file-eselect-repo-common-1>>

<<&profile-gnu>>

<<&gentoo-file-eselect-repo-common-2>>

RUN \
eselect profile set docker-profile:tgbugs/gnu \
&& env-update

<<&gentoo-file-eselect-repo-common-3>>
#+end_src

** updated
Unfortunately running =-uDN= pulls in all sorts of stuff when we least want it, so just roll with updated here too.
FIXME actually ... updated is ... bad? ideally we just go to package-builder directly so ... wait no we do still need quickpkg grr.
Maybe quickpkg-new from the package builder or something?
Well for now we keep them in parallel until we can switch to the raw package builder.
*** build
#+name: &musl-build-updated
#+begin_src screen
docker build \
--tag tgbugs/gnu:updated \
--network host \
--add-host local.binhost:127.0.0.1 \
--file gnu/updated/Dockerfile gnu/updated
#+end_src

*** file
#+name: &gnu/updated
#+begin_src dockerfile :tangle ./gnu/updated/Dockerfile
FROM tgbugs/gnu:eselect-repo
<<&updated-common>>
#+end_src

** package-builder
*** populate 0
#+name: &gnu-run-package-builder-quickpkg
#+begin_src bash
quickpkg-image tgbugs/gnu:package-builder
#+end_src

*** build
#+name: &gnu-build-package-builder
#+begin_src screen
docker build \
--tag tgbugs/gnu:package-builder \
--file gnu/package-builder/Dockerfile gnu/package-builder
#+end_src

*** file
#+begin_src dockerfile :tangle ./gnu/package-builder/Dockerfile
FROM tgbugs/gnu:eselect-repo

<<&musl-package-builder-common>>

# XXX FIXME running this causes portage invalid location for some reason???
# maybe because the folder existing prevents the tmp/ folder from being created ???
# ADD 99-sbcl /usr/x86_64-pc-linux-musl/etc/portage/package.use/99-sbcl
#+end_src
*** crossdev
# hack to reuse musl common for file
#+begin_src conf :tangle ./gnu/package-builder/repo_name
<<&crossdev-repo_name>>
#+end_src

#+begin_src conf :tangle ./gnu/package-builder/layout.conf
<<&crossdev-layout>>
#+end_src

#+begin_src conf :tangle ./gnu/package-builder/crossdev.conf
<<&crossdev-conf>>
#+end_src

#+begin_src conf :tangle ./gnu/package-builder/sbcl.env
#+end_src
*** sets
**** builder
#+name: set-builder-gnu
#+begin_src conf :tangle ./gnu/package-builder/sets/builder
<<set-builder-musl>>
#+end_src

*** world
#+name: world-package-builder-gnu
#+begin_src conf :tangle ./gnu/package-builder/world
dev-lisp/sbcl
#+end_src

#+begin_src conf
<<world-sbcl-base>>
#+end_src
*** sets
**** license
#+name: set-license-package-builder-gnu
#+begin_src conf :tangle ./gnu/package-builder/sets/license
#+end_src

**** live
#+name: set-live-package-builder-gnu
#+begin_src conf :tangle ./gnu/package-builder/sets/live
#+end_src

#+name: set-also-live-package-builder-gnu
#+begin_src conf :tangle ./gnu/package-builder/sets/also-live
#+end_src

*** cross
**** issues
Building cross packages from the builder isn't the most straightforward process.
#+name: gnu-cross-musl-emerge-info
#+begin_src bash :noweb yes :results code :wrap example
<<&builder-vars>>
docker run \
<<&builder-args-gnu>>
--rm \
${_tg_pbs} \
x86_64-pc-linux-musl-emerge --info
#+end_src

# FIXME cross build repos missing proper configuration for compression and archive type (even though there aren't very many packages)

Examples of the issue.
#+begin_example
PKGDIR="/usr/x86_64-pc-linux-musl/var/cache/binpkgs/"
PORTAGE_CONFIGROOT="/usr/x86_64-pc-linux-musl/"
#+end_example

However, =FEATURES=buildpkg= is set it seems. So this might just be an
issue of setting mounts for source/target pkgdirs, e.g.
#+begin_example
-v ~/files/binpkgs/cross/gnu/x86_64-pc-linux-musl:/usr/x86_64-pc-linux-musl/var/cache/binpkgs/
#+end_example
or more sanely =${CBUILD}/${CHOST}= ?
#+begin_example
-v ~/files/binpkgs/cross/x86_64-pc-linux-gnu/x86_64-pc-linux-musl:/usr/x86_64-pc-linux-musl/var/cache/binpkgs/
#+end_example
except that we then have to maintain a mapping from our shorthand to =CBUILD=.

FIXME currently broken ?!??
#+begin_example
!!! Invalid binary package: '/var/cache/binpkgs/dev-util/strace/strace-6.1-3.gpkg.tar', Unsupported binary package format from '/var/cache/binpkgs/dev-util/strace/strace-6.1-3.gpkg.tar'
>>> Installing (1 of 5) dev-util/strace-6.1::gentoo
>>> Emerging (2 of 5) dev-lisp/asdf-3.3.5::gentoo for /usr/x86_64-pc-linux-musl/

!!! Invalid binary package: '/usr/x86_64-pc-linux-musl/var/cache/binpkgs/app-arch/xz-utils/xz-utils-5.4.2-2.xpak'
!!! Missing metadata key(s): CATEGORY, PF, SLOT. This binary package is not
!!! recoverable and should be deleted.
#+end_example

#+RESULTS: gnu-cross-musl-emerge-info
#+begin_example
#+end_example
**** sbcl
Yeah ... the usual issue with cross compiling zlib/zstd support.
# TODO may need/want to tangle to gnu/package-builder/cross/x86_64-pc-linux-musl or something?
#+begin_src conf :tangle ./gnu/package-builder/99-sbcl
<<&config-cross-sbcl-package.use>>
#+end_src

We no longer need to set additional =FEATURES= for cross-make because
portage now defaults to =binpkg-multi-instance= and crossdev includes
=buildpkg= in =FEATURES= by default.
#+begin_src conf :tangle ./gnu/package-builder/cross-make.defaults
BINPKG_FORMAT="gpkg"
BINPKG_COMPRESS="zstd"
BINPKG_COMPRESS_FLAGS_ZSTD="--ultra -22"
EMERGE_DEFAULT_OPTS="--jobs <<&emerge-jobs()>> --binpkg-respect-use=y"
#+end_src

work around lack of cross compile support in =src_compile= during system-bootstrap
and also do not reset because we have to use the
get the core location for host sbcl for bootstrap_lisp since =SBCL_HOME= is reset to point to CHOST aka target location
a general solution for the ebuild would be to detect when ~CBUILD != CHOST~ and find the cbuild sbcl
and fail over to clisp if sbcl is not present, not quite system bootstrap in that case because you could
somehow provide an alternate sbcl, but that should still be ok, the use of =has_version= is fundamentally
broken for cross compile in this context (see e.g. [[/usr/lib/portage/pypy3/phase-helpers.sh]])

#+begin_src bash :tangle ./gnu/package-builder/sbcl-cross.env
src_compile() {
	local bindir="${WORKDIR}"/sbcl-binary
	local bootstrap_lisp="sh ${bindir}/run-sbcl.sh --no-sysinit --no-userinit --disable-debugger"
	local core

	if use system-bootstrap ; then
		# must set core location since SBCL_HOME is changed below
		core=$(sbcl --noinform --noprint --quit --eval '(format t "~a~%" sb-ext:*core-pathname*)')
		bootstrap_lisp="sbcl --core ${core} --no-sysinit --no-userinit --disable-debugger"
	fi

	# Bug #869434
	append-cppflags -D_GNU_SOURCE

	# clear the environment to get rid of non-ASCII strings, see bug #174702
	# set HOME for paludis
	env - HOME="${T}" PATH="${PATH}" \
		CC="$(tc-getCC)" AS="$(tc-getAS)" LD="$(tc-getLD)" \
		CPPFLAGS="${CPPFLAGS}" CFLAGS="${CFLAGS}" ASFLAGS="${ASFLAGS}" LDFLAGS="${LDFLAGS}" \
		SBCL_HOME="/usr/$(get_libdir)/sbcl" SBCL_SOURCE_ROOT="/usr/$(get_libdir)/sbcl/src" \
		GNUMAKE=make ./make.sh \
		"${bootstrap_lisp}" \
		|| die "make failed"

	# need to set HOME because libpango(used by graphviz) complains about it
	if use doc; then
		env - HOME="${T}" PATH="${PATH}" \
			CL_SOURCE_REGISTRY="(:source-registry :ignore-inherited-configuration)" \
			ASDF_OUTPUT_TRANSLATIONS="(:output-translations :ignore-inherited-configuration)" \
			make -C doc/manual info html || die "Cannot build manual"
		env - HOME="${T}" PATH="${PATH}" \
			CL_SOURCE_REGISTRY="(:source-registry :ignore-inherited-configuration)" \
			ASDF_OUTPUT_TRANSLATIONS="(:output-translations :ignore-inherited-configuration)" \
			make -C doc/internals info html || die "Cannot build internal docs"
	fi
}
#+end_src

Script to use to cross-built sbcl to build a native built musl sbcl.
#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./bin/musl-bootstrap-from-cross-sbcl.sh :mkdirp yes
emerge --usepkg --nodeps -1q dev-lisp/asdf dev-lisp/uiop
emerge --usepkgonly dev-lisp/sbcl || {
# we don't need to be able to merge old versions that are not in tree
# because once we cross bootstrap our pure musl process with system-bootstrap
# will keep everything up to date, yes a new system will have to bootstrap from
# a new version, but that is a given, XXX watch out for missing ebuild after long period of not updating though?
PKGDIR=/usr/x86_64-pc-linux-musl/var/cache/binpkgs/ USE='-zstd -system-bootstrap' emerge --usepkgonly dev-lisp/sbcl
env-update
source /etc/profile
emerge -q --buildpkg=n dev-lisp/sbcl
# double rebuild for completeness, not strictly required, but keeps the provenance simpler
emerge -q dev-lisp/sbcl
}
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle ./bin/musl-bootstrap-sbcl-static.sh :mkdirp yes
emerge --usepkg --nodeps -1q dev-lisp/asdf dev-lisp/uiop
emerge --usepkgonly dev-lisp/sbcl || {
USE='-source' emerge --usepkgonly dev-lisp/sbcl
env-update
source /etc/profile
emerge -q dev-lisp/sbcl  # rebuild with patch
}
#+end_src

#+name: &cross-bootstrap-sbcl
#+begin_src screen
function gnu-cross-musl-sbcl () {
# FIXME super hacked way to determine whether we already bootstrapped
gnu-builder-run sh -c 'set -o pipefail; eval $(x86_64-pc-linux-musl-emerge --info | grep PKGDIR=); cat ${PKGDIR}/Packages | grep -v multicall | grep "^USE:.\+system-bootstrap"' || gnu-cross-musl-sbcl-build
return $?
}

function gnu-cross-musl-sbcl-build () {
# build the starting glibc linked sbcl
gnu-builder-run emerge -q dev-lisp/sbcl
# FIXME env-update and source /etc/profile are needed to ensure that SBCL_HOME is set correctly in the environment after install
# otherwise sbcl will look for sbcl.core in the current directory (making it look like the package is broken)
gnu-builder-run env-update
# gnu-builder-run source /etc/profile

gnu-builder-run x86_64-pc-linux-musl-emerge -q --keep-going sys-libs/musl || return $?  # FIXME not sure if needed

# configure the gnu cross to musl environment
# must use x86_64-pc-linux-musl NOT x86_64-gentoo-linux-musl because the gentoo musl images are x86_64-pc-linux-musl
# NOTE this filpped some time around 2024-04-01 from gentoo -> pc yep, it is the 17.0 to 23.0 profile change
# https://github.com/gentoo/releng/blob/master/releases/specs/amd64/musl-hardened/stage1-23.spec
# so probably should be sourcing that from the target profile or something?
docker run \
-v ${_path_dockerfiles}/gnu/package-builder/sbcl-cross.env:/tmp/sbcl-cross.env \
-v ${_path_dockerfiles}/gnu/package-builder/99-sbcl:/tmp/99-sbcl \
-v ${_path_dockerfiles}/gnu/package-builder/cross-make.defaults:/tmp/cross-make.defaults \
${_tg_pbs} \
sh -c \
"
mkdir -p               /usr/x86_64-pc-linux-musl/etc/portage/env/dev-lisp
cp /tmp/sbcl-cross.env /usr/x86_64-pc-linux-musl/etc/portage/env/dev-lisp/sbcl
mkdir -p        /usr/x86_64-pc-linux-musl/etc/portage/package.use/;\
cp /tmp/99-sbcl /usr/x86_64-pc-linux-musl/etc/portage/package.use/99-sbcl;\
cat /tmp/cross-make.defaults >> /usr/x86_64-pc-linux-musl/etc/portage/make.conf\
"

docker commit --change='CMD ["/bin/bash"]' $(docker ps -lqf ancestor=${_tg_pbs}) ${_tg_pbs}  # TODO possibly write to own lineage or make own script and --rm

# this symlink is critical to get the whole thing to work otherwise the tools-for-build/determine-endianness error will show up
# FIXME the string is getting split when passing through the function
gnu-builder-run sh -c '[ -f /lib/ld-musl-x86_64.so.1 ] || ln -s /usr/x86_64-pc-linux-musl/usr/lib/libc.so /lib/ld-musl-x86_64.so.1'
# the symlink trick we do for ld doesn't work for libzstd so have to
# USE='-zlib -zstd' must be set here (or rather in the crossdev package.use) to avoid tools-for-build/grovel-headers issues with missing libs
#gnu-builder-run sh -c "USE='-zlib -zstd' x86_64-pc-linux-musl-emerge -q --keep-going dev-lisp/sbcl"
#gnu-builder-run x86_64-pc-linux-musl-emerge -q --keep-going --usepkg dev-lisp/sbcl

# FIXME somehow only x86_64-pc-linux-musl-emerge is installed? did we miss a cross build?
# since we have sbcl installed on the builder we can use system-bootstrap to reduce useflag noise
gnu-builder-run sh -c 'source /etc/profile; export USE=system-bootstrap; x86_64-pc-linux-musl-emerge -q --keep-going dev-lisp/sbcl; unset USE;'

# reminder can't use LD_LIBRARY_PATH=/usr/x86_64-pc-linux-musl/usr/lib because it breaks the builder (gnu) binaries
}

function musl-bootstrap-sbcl () {
# finish the bootstrap to produce the musl sbcl binpkg by building in the musl builder itself
builder-run musl-bootstrap-from-cross-sbcl.sh
# this has the nice side effect that we ensure that the native sbcl is always present in the builder
# so that any updates to sbcl will be able to use system-bootstrap without issue
# FIXME we do need to watch out for cases where we lose and ebuild for an older version
# but I think portage may handle that automatically for us? not quite sure
}

function static-musl-bootstrap-sbcl () {
static-builder-run musl-bootstrap-sbcl-static.sh
}

function cross-bootstrap-sbcl () {
# all together now
gnu-cross-musl-sbcl
musl-bootstrap-sbcl
}
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :noweb yes :tangle ./bin/gnu-cross-musl-sbcl.sh
<<&cross-bootstrap-sbcl>>
cross-bootstrap-sbcl
#+end_src

**** ghc
XXX this is still broken

1. have to have ghc installed in the build environment
2. configure for some stage wants ncurses to be present otherwise it will fail

The =/lib/ld-musl-x86_64.so.1= symlink is also necessary for this to
work as it was in the case of sbcl. This must be a in oversight/bug in
the cross compilation environment. This hack might allow things that
link musl directly to work, but I think it also may be deferring the
issue since the underlying problem without the symlink is that the
compilers don't work because something is getting crossed up and the
build process is trying to run musl binaries on a gnu system instead
of running the gnu equivalents to build the musl bits. This works for
sbcl because we can disable all linking via use flags, but we can't do
that with haskell, and as a result the build process tries to run a
program that it built for another arch in a later step ... which is
sad and unfortunate. =-gmp= makes no difference because ld is called
to link different bits of ghc together.

Setting =ROOT=/usr/x86_64-gentoo-linux-musl= also doesn't fix the issue.

#+begin_src bash :tangle ./bin/gnu-cross-musl-ghc.sh
USE=-doc emerge --usepkg -q dev-lang/ghc
x86_64-gentoo-linux-musl-emerge --usepkg --nodeps -q1 sys-libs/ncurses
USE='ghcbootstrap -doc -gmp' x86_64-gentoo-linux-musl-emerge --usepkg --nodeps -q dev-lang/ghc
#+end_src

if you can somehow get the cross compile to work then you could run the following
#+begin_src bash :tangle ./bin/musl-bootstrap-from-cross-ghc.sh
emerge --onlydeps -1q --usepkg dev-lang/ghc
emerge --usepkgonly dev-lang/ghc || {
PKGDIR=/usr/x86_64-gentoo-linux-musl/var/cache/binpkgs/ USE='ghcbootstrap -doc -gmp' emerge -K dev-lang/ghc
emerge dev-lang/ghc
}
#+end_src

***** issues
looks like relative paths are not being calculated correctly but are coming from fs root? instead of ROOT ???
#+begin_example
x86_64-gentoo-linux-musl-ld: cannot find /usr/x86_64-gentoo-linux-musl/tmp/portage/dev-lang/ghc-9.0.2-r4/temp/ghc2151_0/ghc_8.o inside /usr/x86_64-pc-linux-gnu/x86_64-gentoo-linux-musl/binutils-bin/2.39/../../../../x86_64-gentoo-linux-musl
x86_64-gentoo-linux-musl-ld: cannot find /usr/x86_64-gentoo-linux-musl/tmp/portage/dev-lang/ghc-9.0.2-r4/temp/ghc2151_0/ghc_7.o inside /usr/x86_64-pc-linux-gnu/x86_64-gentoo-linux-musl/binutils-bin/2.39/../../../../x86_64-gentoo-linux-musl
`x86_64-gentoo-linux-musl-ld' failed in phase `Merge objects'. (Exit code: 1)
make[1]: *** [libraries/ghc-prim/ghc.mk:4: libraries/ghc-prim/dist-install/build/GHC/Prim/Ext.o] Error 1
make[1]: *** Waiting for unfinished jobs....
x86_64-gentoo-linux-musl-ld: cannot find /usr/x86_64-gentoo-linux-musl/tmp/portage/dev-lang/ghc-9.0.2-r4/temp/ghc2142_0/ghc_6.o inside /usr/x86_64-pc-linux-gnu/x86_64-gentoo-linux-musl/binutils-bin/2.39/../../../../x86_64-gentoo-linux-musl
x86_64-gentoo-linux-musl-ld: cannot find /usr/x86_64-gentoo-linux-musl/tmp/portage/dev-lang/ghc-9.0.2-r4/temp/ghc2142_0/ghc_5.o inside /usr/x86_64-pc-linux-gnu/x86_64-gentoo-linux-musl/binutils-bin/2.39/../../../../x86_64-gentoo-linux-musl
`x86_64-gentoo-linux-musl-ld' failed in phase `Merge objects'. (Exit code: 1)
make[1]: *** [libraries/ghc-prim/ghc.mk:4: libraries/ghc-prim/dist-install/build/GHC/CString.o] Error 1
make: *** [Makefile:128: ghc/stage2/build/tmp/ghc-stage2] Error 2
#+end_example

I'm trying to cross compile ghc from gnu -> musl, and the linker is struggling to find the correct path.
build.log http://sprunge.us/R4Sl23
It seems that there are two absolute paths being combined, but ld expects the 2nd path to be relative.

possibly due to ROOT issues, or maybe because the makefile has a bug?

***** bug
#+begin_question :id 1 :type irc :server irc.libera.chat :channel #gentoo
crossdev question: there are a number of packages e.g. sbcl, ghc where certain paths fail to account for e.g. the /usr/x86_64-gentoo-linux-musl prefix
the end result is that we wind up with paths like:
/usr/x86_64-gentoo-linux-musl/usr/x86_64-gentoo-linux-musl/usr/share/common-lisp/source/asdf/version.lisp-expr
instead of:
/usr/x86_64-gentoo-linux-musl/usr/share/common-lisp/source/asdf/version.lisp-expr
in some cases this also breaks the linker
is there an know way to fix things like this?
#+end_question

#+begin_answer :id 1
sam says submit a bug because it seems uncommon
#+end_answer

#+begin_src bash
emerge sys-devel/crossdev
crossdev --stage4 --stable --portage --usepkg --target x86_64-gentoo-linux-musl
x86_64-gentoo-linux-musl-emerge -q --keep-going sys-libs/musl
# at this point you can try to build sbcl
x86_64-gentoo-linux-musl-emerge -q --keep-going dev-lisp/sbcl

# this hack allows the sbcl build to succeed, without it the build fails much earlier
[ -f /lib/ld-musl-x86_64.so.1 ] || ln -s /usr/x86_64-gentoo-linux-musl/usr/lib/libc.so /lib/ld-musl-x86_64.so.1

#+end_src

** crossdev
*** build
#+name: &gnu-build-crossdev
#+begin_src screen
docker build \
--tag tgbugs/gnu:crossdev \
--network host \
--add-host local.binhost:127.0.0.1 \
--file gnu/crossdev/Dockerfile gnu/crossdev
#+end_src
*** file
# FIXME masking issues somehow blocking musl ... seems like the issue may be that we haven't dumped stuff, also masked by CHOST? wat?
#+begin_src dockerfile :tangle ./gnu/crossdev/Dockerfile
FROM tgbugs/gnu:eselect-repo

#emerge -q -uDN \  # yeahno
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q \
   --getbinpkg \
   --keep-going \
   sys-devel/crossdev \
<<&archive-or-rm>>

# FIXME why isn't this finding the binpkgs ???
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge --usepkg --getbinpkgonly \
   cross-x86_64-pc-linux-musl/musl \
   cross-x86_64-pc-linux-musl/linux-headers \
   cross-x86_64-pc-linux-musl/gcc \
   cross-x86_64-pc-linux-musl/binutils \
;  crossdev --stable --target x86_64-pc-linux-musl --stage4 \
<<&archive-or-rm>>
#+end_src

** ghc-cross :do_not_use:
This is very much not working right now, our solution to use alpine to
get a ghc that works with musl 1.2.3 works. I suspect that the ghc
ebuilds are not set up to work with cross compiling for some reason.
*** run
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/gnu:ghc-cross
#+end_src

Test that the cross compiled version is working as expected.
#+begin_src bash
docker run \
-it tgbugs/gnu:ghc-cross \
/usr/x86_64-pc-linux-musl/usr/bin/ghci
#+end_src
*** build
#+begin_src screen
docker build \
--tag tgbugs/gnu:ghc-cross \
--network host \
--add-host local.binhost:127.0.0.1 \
--file gnu/ghc-cross/Dockerfile gnu/ghc-cross
#+end_src
*** file
#+begin_src dockerfile :tangle ./gnu/ghc-cross/Dockerfile
FROM tgbugs/gnu:crossdev

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q \
   --getbinpkg \
   --keep-going \
   dev-lang/ghc \
<<&archive-or-rm>>

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
x86_64-pc-linux-musl-emerge -q \
   --getbinpkg \
   --keep-going \
   sys-libs/musl \
   sys-libs/zlib \
<<&archive-or-rm>>

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
USE=ghcbootstrap x86_64-pc-linux-musl-emerge -q \
   dev-lang/ghc \
<<&archive-or-rm>>
#+end_src
*** patch
See https://gitlab.haskell.org/ghc/ghc/-/issues/17944
and https://gitlab.haskell.org/ghc/ghc/-/issues/12416
and https://gitlab.haskell.org/ghc/ghc/-/merge_requests/2943
and https://gitlab.haskell.org/ghc/ghc/-/merge_requests/4466
**** ignore Winline (fails)
evil but generalized solution to fatal inline warnings, this is a bootstrap build after all
#+begin_src diff
--- a/rts/ghc.mk
+++ b/rts/ghc.mk
@@ -355,7 +355,6 @@
 WARNING_OPTS += -Wstrict-prototypes
 WARNING_OPTS += -Wmissing-prototypes
 WARNING_OPTS += -Wmissing-declarations
-WARNING_OPTS += -Winline
 WARNING_OPTS += -Wpointer-arith
 WARNING_OPTS += -Wmissing-noreturn
 WARNING_OPTS += -Wnested-externs
#+end_src

but then linking fails

#+begin_example
`x86_64-pc-linux-musl-ld' failed in phase `Merge objects'. (Exit code: 1)
make[1]: *** [libraries/ghc-prim/ghc.mk:4: libraries/ghc-prim/dist-install/build/GHC/Prim/Ext.o] Error 1
#+end_example

not sure if related https://gitlab.haskell.org/ghc/ghc/-/issues/17962

also seems to fail at other steps, but the build processes more or less works
as expected if you go in an just call make as root ... very likely because the
environment is totally different

**** previous attempts (fails)
=mkdir -p /usr/x86_64-pc-linux-musl/etc/portage/patches/dev-lang/ghc-9.0.2=
#+begin_src diff :tangle ./docker-profile/base/ghc-inlining-old-do-not-use.patch
--- a/rts/sm/Evac.c       2021-09-15 15:27:32.000000000 -0000
+++ b/rts/sm/Evac.c       2022-12-01 19:52:51.123156616 -0000
@@ -58,7 +58,7 @@
 #define MAX_THUNK_SELECTOR_DEPTH 16
 
 static void eval_thunk_selector (StgClosure **q, StgSelector *p, bool);
-STATIC_INLINE void evacuate_large(StgPtr p);
+static void evacuate_large(StgPtr p);
 
 /* -----------------------------------------------------------------------------
    Allocate some space in which to copy an object.

#+end_src

from ghc fc8a7f8f2aed3420dcbe2c5c25a525634779166f
#+begin_src diff ./docker-profile/base/ghc-inlining.patch
diff --git a/includes/Rts.h b/includes/Rts.h
index 1e5a60262b..027d5173a1 100644
--- a/includes/Rts.h
+++ b/includes/Rts.h
@@ -37,12 +37,17 @@ extern "C" {
 #include "HsFFI.h"
 #include "RtsAPI.h"
 
-// Turn off inlining when debugging - it obfuscates things
+// Disencourage gcc from inlining when debugging - it obfuscates things
 #if defined(DEBUG)
 # undef  STATIC_INLINE
 # define STATIC_INLINE static
 #endif
 
+// Fine grained inlining control helpers.
+#define ATTR_ALWAYS_INLINE __attribute__((always_inline))
+#define ATTR_NOINLINE      __attribute__((noinline))
+
+
 #include "rts/Types.h"
 #include "rts/Time.h"
 
diff --git a/rts/sm/Evac.c b/rts/sm/Evac.c
index e660fad1d8..8595a80c38 100644
--- a/rts/sm/Evac.c
+++ b/rts/sm/Evac.c
@@ -58,7 +58,7 @@
 #define MAX_THUNK_SELECTOR_DEPTH 16
 
 static void eval_thunk_selector (StgClosure **q, StgSelector *p, bool);
-STATIC_INLINE void evacuate_large(StgPtr p);
+ATTR_NOINLINE static void evacuate_large(StgPtr p);
 
 /* -----------------------------------------------------------------------------
    Allocate some space in which to copy an object.
@@ -134,8 +134,13 @@ alloc_for_copy (uint32_t size, uint32_t gen_no)
    The evacuate() code
    -------------------------------------------------------------------------- */
 
-/* size is in words */
-STATIC_INLINE GNUC_ATTR_HOT void
+/* size is in words
+
+   We want to *always* inline this as often the size of the closure is static,
+   which allows unrolling of the copy loop.
+
+ */
+ATTR_ALWAYS_INLINE GNUC_ATTR_HOT static inline void
 copy_tag(StgClosure **p, const StgInfoTable *info,
          StgClosure *src, uint32_t size, uint32_t gen_no, StgWord tag)
 {
@@ -194,7 +199,7 @@ copy_tag(StgClosure **p, const StgInfoTable *info,
 }
 
 #if defined(PARALLEL_GC) && !defined(PROFILING)
-STATIC_INLINE void
+ATTR_ALWAYS_INLINE static inline void
 copy_tag_nolock(StgClosure **p, const StgInfoTable *info,
          StgClosure *src, uint32_t size, uint32_t gen_no, StgWord tag)
 {
@@ -231,7 +236,7 @@ copy_tag_nolock(StgClosure **p, const StgInfoTable *info,
  ,* pointer of an object, but reserve some padding after it.  This is
  ,* used to optimise evacuation of TSOs.
  ,*/
-static bool
+ATTR_ALWAYS_INLINE static inline bool
 copyPart(StgClosure **p, StgClosure *src, uint32_t size_to_reserve,
          uint32_t size_to_copy, uint32_t gen_no)
 {
@@ -283,7 +288,7 @@ spin:
 
 
 /* Copy wrappers that don't tag the closure after copying */
-STATIC_INLINE GNUC_ATTR_HOT void
+ATTR_ALWAYS_INLINE GNUC_ATTR_HOT static inline void
 copy(StgClosure **p, const StgInfoTable *info,
      StgClosure *src, uint32_t size, uint32_t gen_no)
 {
@@ -301,7 +306,7 @@ copy(StgClosure **p, const StgInfoTable *info,
    that has been evacuated, or unset otherwise.
    -------------------------------------------------------------------------- */
 
-static void
+ATTR_NOINLINE static void
 evacuate_large(StgPtr p)
 {
   bdescr *bd;
-- 
2.37.4
#+end_src

We haven't resolved all the issues yet ...
#+begin_example
In file included from includes/Rts.h:244,

                 from rts/Interpreter.c:8:0: error: 
rts/Interpreter.c: In function interpretBCO:

includes/rts/StablePtr.h:32:8: error:
     warning: inlining failed in call to deRefStablePtr: call is unlikely and code size would grow [-Winline]
       32 | StgPtr deRefStablePtr(StgStablePtr sp)
          |        ^~~~~~~~~~~~~~
   |
32 | StgPtr deRefStablePtr(StgStablePtr sp)
   |        ^

rts/Interpreter.c:1112:45: error:
     note: called from here
     1112 |                   ioAction = (StgClosure *) deRefStablePtr (
          |                                             ^~~~~~~~~~~~~~~~
     1113 |                       rts_breakpoint_io_action);
          |                       ~~~~~~~~~~~~~~~~~~~~~~~~~
     |
1112 |                   ioAction = (StgClosure *) deRefStablePtr (
     |                                             ^

#+end_example

let's see if 9.2.4 will compile any better
#+begin_src bash
mkdir -p /usr/x86_64-pc-linux-musl/etc/portage/repos.conf
cp /etc/portage/repos.conf/eselect-repo.conf /usr/x86_64-pc-linux-musl/etc/portage/repos.conf/
ACCEPT_KEYWORDS='**' FEATURES=-distcc USE=ghcbootstrap x86_64-pc-linux-musl-emerge =dev-lang/ghc-9.2.4::haskell   
#+end_src

... ugh nope, this one fails well before 9.0.2 does
#+begin_example
config.status: executing src commands
# wc on OS X has spaces in its output, which libffi's Makefile
# doesn't expect, so we tweak it to sed them out
mv libffi/build/Makefile libffi/build/Makefile.orig
sed "s#wc -w#wc -w | sed 's/ //g'#" < libffi/build/Makefile.orig > libffi/build/Makefile
"touch" libffi/stamp.ffi.static-shared.configure
make: *** [Makefile:128: ghc/stage2/build/tmp/ghc-stage2] Error 2
#+end_example

when running make in =libraries/ghc-prim= we find +some+ many additional issues
I have no idea why these are showing up in the musl cross builds when they are not in the native gnu case
a sampling of the kinds of issues
#+begin_example
In function 'updateRemembSetPushTSO':

rts/sm/NonMovingMark.c:693:20: error:
     warning: inlining failed in call to 'finish_upd_rem_set_mark': --param max-inline-insns-single limit reached [-Winline]
      693 | STATIC_INLINE void finish_upd_rem_set_mark(StgClosure *p)
          |                    ^~~~~~~~~~~~~~~~~~~~~~~
    |
693 | STATIC_INLINE void finish_upd_rem_set_mark(StgClosure *p)
    |                    ^

rts/sm/NonMovingMark.c:719:9: error:
     note: called from here
      719 |         finish_upd_rem_set_mark((StgClosure *) tso);
          |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |
719 |         finish_upd_rem_set_mark((StgClosure *) tso);
    |         ^
rts/sm/NonMovingMark.c: In function 'updateRemembSetPushStack':

rts/sm/NonMovingMark.c:693:20: error:
     warning: inlining failed in call to 'finish_upd_rem_set_mark': --param max-inline-insns-single limit reached [-Winline]
      693 | STATIC_INLINE void finish_upd_rem_set_mark(StgClosure *p)
          |                    ^~~~~~~~~~~~~~~~~~~~~~~
    |
693 | STATIC_INLINE void finish_upd_rem_set_mark(StgClosure *p)
    |                    ^

rts/sm/NonMovingMark.c:734:13: error:
     note: called from here
      734 |             finish_upd_rem_set_mark((StgClosure *) stack);
          |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |
734 |             finish_upd_rem_set_mark((StgClosure *) stack);
    |             ^

rts/sm/NonMovingMark.c:693:20: error:
     warning: inlining failed in call to 'finish_upd_rem_set_mark': --param max-inline-insns-single limit reached [-Winline]
      693 | STATIC_INLINE void finish_upd_rem_set_mark(StgClosure *p)
          |                    ^~~~~~~~~~~~~~~~~~~~~~~
    |
693 | STATIC_INLINE void finish_upd_rem_set_mark(StgClosure *p)
    |                    ^

rts/sm/NonMovingMark.c:734:13: error:
     note: called from here
      734 |             finish_upd_rem_set_mark((StgClosure *) stack);
          |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    |
734 |             finish_upd_rem_set_mark((StgClosure *) stack);
    |    
#+end_example

** sbcl-cross
*** run
#+begin_src bash
docker run \
--volumes-from local-repos-snap \
-v /mnt/str/portage/distfiles:/var/cache/distfiles \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
-it tgbugs/gnu:sbcl-cross
#+end_src

Test that the cross compiled version is working as expected.
#+begin_src bash
docker run \
-it tgbugs/gnu:sbcl-cross \
/usr/x86_64-pc-linux-musl/usr/bin/sbcl --core /usr/x86_64-pc-linux-musl/usr/lib/sbcl/sbcl.core
#+end_src
*** build
#+begin_src screen
docker build \
--tag tgbugs/gnu:sbcl-cross \
--network host \
--add-host local.binhost:127.0.0.1 \
--file gnu/sbcl-cross/Dockerfile gnu/sbcl-cross
#+end_src
*** file
#+begin_src dockerfile :tangle ./gnu/sbcl-cross/Dockerfile
FROM tgbugs/gnu:crossdev

# sbcl crossdev build looks in the wrong place for asdf and uiop
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q \
   --getbinpkg \
   --keep-going \
   dev-lisp/asdf \
   dev-lisp/uiop \
   dev-lisp/sbcl \
<<&archive-or-rm>>

# TODO move these into the builder probably?
ADD alt-ld.patch /etc/portage/patches/dev-lisp/sbcl/alt-ld.patch
ADD alt-ld.patch /usr/x86_64-pc-linux-musl/etc/portage/patches/dev-lisp/sbcl/alt-ld.patch
ADD 99-sbcl /usr/x86_64-pc-linux-musl/etc/portage/package.use/99-sbcl

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
x86_64-pc-linux-musl-emerge -q \
   --getbinpkg \
   --keep-going \
   sys-libs/musl \
   app-arch/zstd \
   sys-libs/zlib \
<<&archive-or-rm>>

RUN \
ln -s /usr/x86_64-pc-linux-musl/usr/lib/libc.so /lib/ld-musl-x86_64.so.1

RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
x86_64-pc-linux-musl-emerge -q \
   dev-lisp/asdf \
   dev-lisp/uiop \
   dev-lisp/sbcl \
<<&archive-or-rm>>
#+end_src

Until we can figure out how to get the cross build to link this
correctly we leave it out.  We don't really need it since this build
is only for a bootstrapping sbcl on musl. There are native ways to do
this inside of the sbcl toolset itself, but for now this is easier.
# */* static-libs static
#+name: &config-cross-sbcl-package.use
#+begin_src conf :tangle ./gnu/sbcl-cross/99-sbcl
dev-lisp/sbcl -zlib -zstd  # crossdev compile reference errors
#+end_src

# XXX we should be able to get rid of this now that it has been fixed upstream I think
#+begin_src diff :tangle ./gnu/sbcl-cross/alt-ld.patch
diff --git a/make-target-contrib.sh b/make-target-contrib.sh
index 217b5b2e0..45406f506 100755
--- a/make-target-contrib.sh
+++ b/make-target-contrib.sh
@@ -29,8 +29,12 @@ if [ -z "$CC" ]; then
     fi
 fi
 
+if [ -z "${LD}" ]; then
+    LD=ld
+fi
+
 unset EXTRA_CFLAGS # avoid any potential interference 
-export CC LANG LC_ALL
+export CC LD LANG LC_ALL
 
 # Load our build configuration
 . output/build-config
diff --git a/src/runtime/GNUmakefile b/src/runtime/GNUmakefile
index 0543c1244..284755e5c 100644
--- a/src/runtime/GNUmakefile
+++ b/src/runtime/GNUmakefile
@@ -24,7 +24,6 @@ SBCL_PAXCTL ?= :
 LINKFLAGS += -g
 DEPEND_FLAGS = -MM
 GREP = grep
-LD = ld
 
 # By default, don't make and use a library, just use the object files.
 LIBSBCL = $(OBJS)
#+end_src

#+begin_src bash
git clone https://github.com/sbcl/sbcl.git
pushd sbcl
git remote add daewok https://github.com/daewok/sbcl.git
git fetch daewok
git checkout daewok/static-executable
#+end_src

** musl/cross/sbcl
TODO loop this in so that we can keep the cross build image up to date so that
we aren't bootstrapping cross builds with ancient versions of sbcl, not sure
the extent to which that matters, but at a certain point musl might update and
the we could be in trouble, also requires that we loop in gnu sbcl-cross and
have some logic for new sbcl releases

the general objective here is to avoid the situation where there is rot in the
bootstrapping process because we always use the docker image the same way there
has been rot with clojure pulling everything from maven, less of a risk in this
case but still something to keep an eye on

another way to do this would be to emerge the crossbuilt binpkg directly and then
go with system-bootstrap in that way instead the sideloading that we do here which
requires modification of the ebuild, yeah, this seems cleaner than cooking up this
image since it avoids the need for more docker build commands entirely

*** build
#+name: &gnu-build-musl-cross-sbcl
#+begin_src screen
docker build \
--tag tgbugs/musl:cross-sbcl \
--file musl/cross/sbcl/Dockerfile musl/cross/sbcl

docker rm cross-sbcl
docker create -v /sbcl --name cross-sbcl tgbugs/musl:cross-sbcl /bin/true
#+end_src

*** file
#+begin_src dockerfile :tangle ./musl/cross/sbcl/Dockerfile
FROM docker.io/library/busybox:latest

WORKDIR /

COPY --from=tgbugs/gnu:package-builder-snap /usr/x86_64-pc-linux-musl/usr/lib/sbcl /sbcl
COPY --from=tgbugs/gnu:package-builder-snap /usr/x86_64-pc-linux-musl/usr/bin/sbcl /sbcl/src/runtime/sbcl

RUN \
mkdir -p /sbcl/obj/sbcl-home \
&& ln -s /sbcl/contrib /sbcl/obj/sbcl-home/contrib \
&& mkdir -p /sbcl/output \
&& ln -s /sbcl/sbcl.core /sbcl/output/sbcl.core

VOLUME /sbcl
#+end_src

* other
** bootstrap attempt 3
So, this nearly works, except for the fact that you can't actually
use bind mounts from inside a nested docker container because the
mount source location is always on the docker daemon file system
not on the client file system (sigh). You can't even find it in
the normal docker wsl container and it pollution remains forever.

In conclusion. If you need to do this on windows, just do it in wsl2
directly e.g. via an ubuntu vm and don't bother with trying to use
docker windows for the bootstrap/build process, you can push the
images somewhere and pull the back to windows if you need them much
more easily.

There is still the issue of the mismatch between the daemon file
system and the client file system with the docker socket approach, but
just think host thoughts and you can fly (sigh). Have to run the binpkgs
mkdir setup bit on the host/daemon file system.

in the host environment you must already have
1. docker
2. +git+ nope, you just need to be able to see this section of the file
3. +emacs+ nope, just copy and paste

debug build
#+begin_src screen
docker run \
-v /var/run/docker.sock:/var/run/docker.sock \
-e _UID=<<&UID>> \
--rm \
-it \
alpine:latest
#+end_src

On windows.
#+begin_src powershell
docker run -v //var/run/docker.sock:/var/run/docker.sock --rm -it alpine:latest
#+end_src

step 0 don't use =docker build= at this level (duh!?)
#+begin_src bash
docker run \
-v /var/run/docker.sock:/var/run/docker.sock \
-u ${_UID} \
alpine:latest \
/dockerfiles/bin/alpine-bootstrap
#+end_src

#+header: :shebang "#!/usr/bin/env sh"
#+begin_src bash :tangle bin/alpine-bootstrap
DUID=48

addgroup -g ${DUID} docker

apk add bash emacs-nox git screen
apk add curl python3 # missed these one, needed to run the pkghost for the time being
apk add xdg-user-dirs
apk add docker
# hrm!? (no, doesn't work, because windows weirdness)
#apk add podman
#ln -s podman /usr/bin/docker

_UID=1000
USER_NAME=user

addgroup -g ${_UID} ${USER_NAME} \
&& adduser -HD -u ${_UID} -G ${USER_NAME} ${USER_NAME} \
&& adduser ${USER_NAME} docker

xdg-user-dirs-update

rmdir Desktop Pictures Documents Public Downloads Templates Music Videos > /dev/null 2>&1

git clone https://github.com/tgbugs/dockerfiles.git

dockerfiles/source.org setup  # TODO

cd dockerfiles

# watch out sometimes this can fail on first run?
# or uh, er running ./source.org build in it kills it?
# do the screen thing below (still debugging)
screen -s /bin/bash -dmS org-session -t server
_screen_socket=$(screen -ls | grep org-session | awk '{ print $1 }')

# XXX if something is misconfigured this can cause screen to terminate ???
# possibly due to missing python and/or missing curl? or possibly due to having only a single screen open? not clear
# looks like it was due to missing curl or missing python, or missing them and running in busybox sh or something?
./source.org build  # to start the binpkg server

# the screen manual is useless the behavior is insane and inconsistent
# but apparently this invocation will create and switch to the new window
# I have no idea how to get the forsaken thing to switch from cli if you
# somehow create a screen without a title so here we are ...
screen -S ${_screen_socket} -X screen -t build

./source.org build --repos # to bootstrap the images we need on the real system, the bootstrap environment has e.g. the distfiles, but
#+end_src

** alpine bootstrap
Barely working and incomplete bootstrap in alpine.
Might be useful for CI depending on how well the
tgbugs/musl:docker work progresses.

# this is cute but just git clone because there are a couple of legacy
# cases where git is needed in the host environment (unfortunately)
#+name: &aboot
#+begin_src elisp
(let ((u (pop argv)) (p (pop argv)) (enable-local-eval t)) (mkdir (file-name-directory p) t) (org-babel-do-load-languages 'org-babel-load-languages '((screen . t))) (url-handler-mode 1) (find-file u) (write-file p) (chmod p #o755) (find-file p) (org-sbe workflow))
#+end_src

#+begin_src screen
SHELL=/bin/bash screen -dmS org-session
# emacs -batch -eval "<<&aboot>>" https://raw.githubusercontent.com/tgbugs/dockerfiles/master/source.org ~/git/dockerfiles/source.org
#+end_src

# unfortunate for the builder mount ...
#+begin_src bash
docker run -v /var/run/docker.sock:/var/run/docker.sock -u ${UID} -it tgbugs/other:alpine-bootstrap
#+end_src

#+begin_src screen
<<&docker-build>>
--build-arg DUID=$(getent group docker | cut -d: -f3) \
--tag tgbugs/other:alpine-bootstrap \
--file other/alpine-bootstrap/Dockerfile other/alpine-bootstrap
#+end_src

#+begin_src dockerfile :tangle ./other/alpine-bootstrap/Dockerfile
FROM docker.io/library/alpine:latest

ARG DUID=48

RUN  \
addgroup -g ${DUID} docker

RUN \
apk add bash docker emacs-nox git screen

ARG UID=1000
ARG USER_NAME=user

RUN \
addgroup -g ${UID} ${USER_NAME} \
&& adduser -HD -u ${UID} -G ${USER_NAME} ${USER_NAME} \
&& adduser ${USER_NAME} docker

USER $USER_NAME

WORKDIR /home/${USER_NAME}

RUN \
xdg-user-dirs-update \
;  rmdir Desktop Pictures Documents Public Downloads Templates Music Videos > /dev/null 2>&1

RUN \
git clone https://github.com/tgbugs/dockerfiles.git

RUN \
dockerfiles/source.org setup

# RUN \
# mkdir -p ~/files/binpkgs/multi
#+end_src
** ubuntu-genera-base
*** file
#+begin_src dockerfile :tangle ./other/ubuntu-genera-base/Dockerfile
FROM docker.io/library/ubuntu:18.04

RUN apt update

RUN apt install -y \
curl \
inetutils-inetd \
vim \
telnet \
nfs-common \
nfs-kernel-server \
iproute2 \
libx11-6 \
xserver-xephyr \
x11-xserver-utils \
iputils-ping
#+end_src

*** build
# docker pull ubuntu:18.04
# docker run -it ubuntu:18.04

#+begin_src bash
docker build \
--tag tgbugs/other:ubuntu-genera-base \
--file other/ubuntu-genera-base/Dockerfile other/ubuntu-genera-base
#+end_src

** genera
A docker file that specifies and image that can run Open Genera 2.0.

We can't distribute the final image for a variety of reasons, however
the configured base image can be distributed and is a valuable
resource as a result.

Useful as a starting point for debugging why it won't work on other systems.

Nearly everything is working except that docker and NFS exports seem
to be fighting with each other.  Old comments on the web mention
issues with exporting overlayfs mounts to NFS, but this commit from
2017 <https://patchwork.kernel.org/project/linux-fsdevel/patch/
1508258671-10800-15-git-send-email-amir73il@gmail.com/> seems to have
fixed that issue.

Three entry points.
https://www.reddit.com/r/lisp/comments/lhsltk/lisp_implementations_similiar_to_old_lisp_machines/
https://gist.github.com/oubiwann/1e7aadfc22e3ae908921aeaccf27e82d
https://archives.loomcom.com/genera/genera-install.html
*** exploration
This will eventually become a docker file, but right now it is still
too experimental so the workflow is run and commit rather than build.

#+begin_src bash
xhost local:docker

# NET_ADMIN apparently needed for tuntap creation (bsd jails and vnets looking really good right now)
# SYS_ADMIN apparently needed to get NFS exports to work (bsd jails looking even better!?)
# generally though this is ok because we are really only using this docker image as a way to get
# an environment where genera will run

docker run -it \
-v ~/files/tmp/genera:/files \
-v /tmp/.X11-unix:/tmp/.X11-unix \
-e DISPLAY=$DISPLAY \
--device /dev/net/tun \
--cap-add NET_ADMIN \
--cap-add SYS_ADMIN \
tgbugs/other:ubuntu-genera-base
#+end_src

In the docker shell (will become the docker file or a script run in the docker file)
#+begin_src bash
#mkdir -p /dev/net
#mknod /dev/net/tun c 10 200

# tunnel creation
# ip tuntap delete dev tap0 mode tap  # to remove since it fights with the host
ip tuntap add dev tap0 mode tap
ip addr add 192.168.2.1/24 dev tap0
ip link set dev tap0 up

# inetd

echo "time      stream  tcp  nowait root internal" >> /etc/inetd.conf
echo "time      dgram   udp  wait   root internal" >> /etc/inetd.conf
echo "daytime   stream  tcp  nowait root internal" >> /etc/inetd.conf
echo "daytime   dgram   udp  wait   root internal" >> /etc/inetd.conf

service inetutils-inetd restart

# retrieve genera files TODO snapshot these to reduce redownload

mkdir genera
pushd genera
curl -LO https://archives.loomcom.com/genera/genera
chmod a+x genera
curl -L -O https://archives.loomcom.com/genera/worlds/Genera-8-5-xlib-patched.vlod
curl -L -O https://archives.loomcom.com/genera/worlds/VLM_debugger
curl -L -O https://archives.loomcom.com/genera/worlds/dot.VLM
mv dot.VLM .VLM
mkdir lib
pushd lib
curl -L -O https://archives.loomcom.com/genera/var_lib_symbolics.tar.gz
tar xvf var_lib_symbolics.tar.gz
chown -R root:root symbolics
ln -s /genera/lib/symbolics /var/lib/symbolics  # may fail
popd

sed -i 's,/home/seth,,' .VLM
echo "192.168.2.1    genera-vlm" >> /etc/hosts
echo "192.168.2.2    genera" >> /etc/hosts

# nfs XXX TODO broken

echo 'RPCNFSDCOUNT="--nfs-version 2 8"' >> /etc/default/nfs-kernel-server
echo 'RPCMOUNTDOPTS="--nfs-version 2 --manage-gids"' >> /etc/default/nfs-kernel-server
echo "/files genera(rw,sync,no_subtree_check,all_squash,anonuid=1000,anongid=1000)" >> /etc/exports
# we really want to export / but I'm seeing the following error
# exportfs: / does not support NFS export
#echo "/ genera(rw,sync,no_subtree_check,all_squash,anonuid=1000,anongid=1000)" >> /etc/exports

# I think rpcbind needs be be started, otherwise nfs-kernel-server may fail to start
# and/or NFS will not work at all
service rpcbind start

service nfs-kernel-server restart

# start genera using host X server

DISPLAY=:0.0; ./genera -coldloadgeometry 640x480+0+0 -geometry 1280x1024+0+0 &

# start genera using Xephyr (a bit more stable/predictable)

DISPLAY=:0.0; Xephyr -br -reset -terminate -ac -noreset -screen 1280x1024 :3 &
DISPLAY=:3.0; ./genera -coldloadgeometry 640x480+0+0 -geometry 1280x1024+0+0 &

#+end_src
* sckan
=base= must be built before =services=, obvious from errors or from
reading the docker files but doesn't jump out and is the inverse of
the order of specification (because services is rebuilt more
frequently).
** build and save
This is the block that has better flow control and will block until finished.
However, setting the return value to be non-zero on failure is not done yet.
#+name: &sckan-build-save-image
#+begin_src bash :noweb yes
# --tag run:sckan-build-save-image # XXX hack to run non-image blocks
set -e
<<&musl-def-build-sckan-base>>

<<&musl-def-build-sckan-services>>

d-build-sckan-base
d-build-sckan-services

<<&sckan-save-image>>
#+end_src
** save
To save a gzipped archive run
#+name: &sckan-save-image
#+begin_src screen
# first run the following to find the latest build, then
docker image ls tgbugs/sckan:data-*
_datetime=$(docker image ls tgbugs/sckan:data-* | sort | tail -n 1 | awk '{ print $2 }')
echo writing docker image to /tmp/docker-sckan-${_datetime}.tar.gz
docker save tgbugs/sckan:${_datetime} | gzip > /tmp/docker-sckan-${_datetime}.tar.gz
#+end_src

To restore from the archive run
#+begin_src bash
docker load --input sckan-data-2021-09-30T232453Z.tar.gz
#+end_src
** services
This combines the raw (data) base image with prefixes.conf and services.yaml.
Note that the image names for these are shifted so that they don't confuse users.
The logic is that base + services = data, but users don't know anything about services.
*** container
Test this with tgbugs/musl:kg-release-user
#+name: &create-sckan-data
#+begin_src bash
docker container inspect sckan-data > /dev/null && \
docker rm sckan-data
docker create -v /var/lib/blazegraph -v /var/lib/scigraph --name sckan-data tgbugs/sckan:latest /bin/true
#+end_src

#+name: &create-sckan-data-if-not-exists
#+begin_src bash
docker container inspect sckan-data > /dev/null || \
{ echo creating sckan-data image 1>&2; docker create -v /var/lib/blazegraph -v /var/lib/scigraph --name sckan-data tgbugs/sckan:latest /bin/true;
if [ -n "${_use_podman}" ]; then
[ -n "$(podman container ls -q --filter name=sckan-data --filter status=initialized)" ] || \
podman container init sckan-data
fi
}
#+end_src

*** build
#+name: &musl-def-build-sckan-services
#+begin_src bash
function d-build-sckan-services () {
[ -f ./source.org ] || { echo not in dockerfiles; return 1;}

local _sckanl _image_date

mkdir -p ./sckan/services/blazegraph

[ -d ./sckan/services/scigraph ] && rm -r ./sckan/services/scigraph
mkdir -p ./sckan/services/scigraph

pushd ./sckan/services

# blazegraph
# TODO from zip
_sckanl="$(ls -d /tmp/build/release-*-sckan | sort -u | tail -n 1)"
rsync -a ${_sckanl}/data/prefixes.conf blazegraph/ || return $?

# scigraph
sh ~/git/pyontutils/nifstd/scigraph/README.org tangle  # FIXME bad, assumes ~/git/pyontutils present on the host :/
~/git/pyontutils/nifstd/scigraph/bin/run-build-services-sparc || return $?
rsync -a /tmp/scigraph-build/sparc/services.yaml scigraph/ || return $?
rsync -a /tmp/scigraph-build/sparc/$(head -n 1 scigraph/services.yaml | cut -b3-) scigraph/ || return $?

popd

_image_date=$(date --utc +%Y-%m-%dT%H%M%SZ)

docker build \
--tag tgbugs/sckan:data-${_image_date} \
--tag tgbugs/sckan:latest \
--file sckan/services/Dockerfile sckan/services
}
#+end_src

#+name: &musl-build-sckan-services
#+begin_src screen
<<&musl-def-build-sckan-services>>
d-build-sckan-services
#+end_src

*** file
#+begin_src dockerfile :tangle ./sckan/services/Dockerfile
FROM docker.io/library/busybox:latest AS builder

WORKDIR /build

ADD --chown=834:834 blazegraph/ /build/var/lib/blazegraph
ADD --chown=835:835 scigraph/ /build/var/lib/scigraph

FROM tgbugs/sckan:base-latest
COPY --from=builder /build /
#+end_src

** base
*** build
# TODO release snapshots for these images

# FIXME scigraph log folder ownership is still fucked after all these years ffs
#+name: &musl-def-build-sckan-base
#+begin_src screen
function d-build-sckan-base () {
[ -f ./source.org ] || { echo not in dockerfiles; return 1;}

local _sckanl _zip _path _scigr _image_date

mkdir -p ./sckan/base/blazegraph

[ -d ./sckan/base/scigraph ] && rm -r ./sckan/base/scigraph
mkdir -p ./sckan/base/scigraph

pushd ./sckan/base

# TODO from zip
_sckanl="$(ls -d /tmp/build/release-*-sckan | sort -u | tail -n 1)"
rsync -a ${_sckanl}/data/blazegraph.jnl blazegraph/ || return $?

# TODO from zip
_zip=$(realpath /tmp/scigraph-build/sparc-sckan/LATEST)
_path="${_zip%.*}"
_scigr="${_path##*/}"
rsync -a ${_path} scigraph/ || return $?
ln -s /var/lib/scigraph/${_scigr} scigraph/graph || return $?

popd

_image_date=$(date --utc +%Y-%m-%dT%H%M%SZ)

docker build \
--tag tgbugs/sckan:base-${_image_date} \
--tag tgbugs/sckan:base-latest \
--file sckan/base/Dockerfile sckan/base
}
#+end_src

#+name: &musl-build-sckan-base
#+begin_src screen
<<&musl-def-build-sckan-base>>
d-build-sckan-base
#+end_src

*** file
# /build/sckan/ this could include the provenance data, but I think we should probably leave it out?
# TODO also consider splitting blazegraph and scigraph volume images ?
# TODO have a version of this that does the builds itself instead of just copying everything in
#+begin_src dockerfile :tangle ./sckan/base/Dockerfile
FROM docker.io/library/busybox:latest AS builder

WORKDIR /build

ADD --chown=834:834 blazegraph/ /build/var/lib/blazegraph
ADD --chown=835:835 scigraph/ /build/var/lib/scigraph

FROM scratch
COPY --from=builder /build /
#+end_src

* utils :noexport:
Portable UID that won't break on e.g. windows-nt where =${UID}= can be out of range.
#+name: &UID
#+begin_src bash :eval never
$(if [ ${UID} -lt 60000 ]; then echo ${UID}; else echo 1000; fi)
#+end_src

# FIXME --network host is ok for now, but we should probably try to
# switch to using --ssh or something since it is needed for building
# all binpkg-only images
#+name: &docker-build
#+begin_src bash
docker build \
--network host \
--add-host local.binhost:127.0.0.1 \
#+end_src

#+name: &build-world
#+begin_src dockerfile
FROM tgbugs/musl:binpkg-only
<<&build-world-common>>
#+end_src

#+name: &build-world-nox
#+begin_src dockerfile
FROM tgbugs/musl:binpkg-only-nox
<<&build-world-common>>
#+end_src

#+name: &build-world-common
#+begin_src dockerfile

ARG ARCHIVE

COPY set[s]/license /etc/portage/sets/license
COPY package.licens[e]/* /etc/portage/package.license/
COPY accept_keyword[s] /etc/portage/package.accept_keywords/tweaks
ADD world /etc/portage/sets/docker

# -ebuild locks is so much faster building acct-* ebuilds first is MUCH faster
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
FEATURES=ebuild-locks emerge -1 -q -uDN $(emerge -p @docker | grep -o 'acct-.\+$' | sed 's/^/=/') \
;  emerge -q --backtrack=99 -uDN @docker \
<<&archive-or-rm>>

# XXX unfortunately there is not an easy way to include or exclude code conditionally
# in a docker file, this adds noise but at least the behavior is consistent
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
if (emerge -p @license); then  \
FEATURES=ebuild-locks emerge -1 -q -uDN $(emerge -p @license | grep -o 'acct-.\+$' | sed 's/^/=/') \
;  emerge -q --backtrack=99 -uDN @license \
<<&archive-or-rm>>; \
else true; fi
#+end_src

#+name: &archive-or-rm
#+begin_src dockerfile
;  export CODE=$? \
;  echo CODE $CODE \
;  [[ -n ${ARCHIVE} ]] \
|| { rm -r /var/cache/distfiles/* > /dev/null 2>&1 \
   ; rm -r /var/cache/binpkgs/* > /dev/null 2>&1 \
   ; rm -r /var/log/portage/elog/summary.log > /dev/null 2>&1 \
   ; rm -r /var/log/emerge-fetch.log > /dev/null 2>&1 \
   ; rm -r /var/log/emerge.log > /dev/null 2>&1;} \
;  exit $CODE
#+end_src

A build helper for use during development when you need to add
packages you forgot but don't want to rebuild the whole world.
#+name: &build-helper-temp
#+begin_src dockerfile
RUN --mount=from=tgbugs/repos:latest,source=/var/db/repos,target=/var/db/repos,rw \
emerge -q -uDN \
#+end_src

* package to images for tweaks
find images that will contain packages once built, but only for explicitly world level packages
- use the dockerfile -> image mapping we have from dep resolution
- get a list of world files and build an inverse index of atom -> world file
- compare prefixes to find world file docker file mapping

#+begin_src elisp
; FIXME the prep work building the assoc lists is slow
(setq
 world-packages
 (let (out)
   (org-fold-core-save-visibility 'use-markers
     (save-excursion
       (org-babel-map-executables nil
         ;; (language body arguments switches name start coderef)
         (let* ((info (org-babel-get-src-block-info))
                (lang (nth 0 info))
                (body (orgstrap--expand-body info))
                (params (nth 2 info))
                (name (nth 4 info))
                (tangle (cdr (assoc :tangle params))))
           (when (and (string= lang "conf") tangle (not (string= tangle "no")) name (string-match "^world-" name))
             (setq out (cons
                        (cons
                         (substring tangle 2) ; drop ./ ; XXX FIXME error if doesnot start with ./
                         (with-temp-buffer
                           (insert body)
                           (evil-ex-global (point-min) (point-max) "^[ ]+#" "d")
                           (split-string (replace-regexp-in-string "#.+$" "" (buffer-string)) "\n" t " +"))) ; note, produces dupes
                        out))
             )))))
   out))

(setq
 package-worlds
 (let (out)
   (cl-loop
    for (world . packages) in world-packages do
    (cl-loop
     for package in packages do
     (let ((entry (assoc package out)))
       (if entry
           (setcdr entry (cons world (cdr entry)))
         (setq out (cons (list package world) out))))))
   out))

(setq
 world-to-path
 (mapcar (lambda (l)  (cons (car l) (reverse (cdr (reverse (string-split (car l) "/")))) )) world-packages))

(setq
 path-to-tags
 (mapcar (lambda (p) (cons (reverse (cdr (reverse (string-split (cdr p) "/")))) (car p))) (reverse tag-produced-by-docker-file)))

(defun package-paths (atom)
  (cl-multiple-value-bind
      (op cpv cp slot repo use-str version)
      (portage-package-atom atom)
    (if cp
        (let (path (worlds (assoc cp package-worlds)))
          (when worlds
            (cl-loop
             for world in (cdr worlds)
             when (let* ((w-path (assoc world world-to-path)))
                    (setq path (cdr w-path)))
             collect
             (string-join path "/"))))
      (message "should probably error here, bad atom? %s" atom))))

(defun package-tags (atom)
  (cl-multiple-value-bind
      (op cpv cp slot repo use-str version)
      (portage-package-atom atom)
    (if cp
        (let (tags (worlds (assoc cp package-worlds)))
          (when worlds
            (cl-loop
             for world in (cdr worlds)
             when (let* ((w-path (assoc world world-to-path))
                         (p-tags (and w-path (assoc (cdr w-path) path-to-tags))))
                    (setq tags (cdr p-tags)))
             collect
             tags)))
      (message "should probably error here, bad atom? %s" atom))))

(defun tangle-tweak-accept-keywords ()
  "given an accept_keywords file write accept_keywords for all affected images"
  (let* ((tweak-code
         (org-fold-core-save-visibility 'use-markers
           (save-excursion
             (org-babel-goto-named-src-block "tweak-accept-keywords")
             (nth 1 (org-babel-get-src-block-info)))))
         (atoms (split-string (replace-regexp-in-string "#.+$" "" tweak-code) "\n" t " +"))
         (paths (cl-loop for atom in atoms append (cl-loop for path in (package-paths atom) collect path)))
         (files (cl-loop for path in paths collect (expand-file-name "accept_keywords" path))))
    (message ":atoms %s :paths %s :files %s" atoms paths files)
    (cl-loop
     for file in files do ; TODO derive list from the config file and search all packages, means we need cpv code
     (with-temp-buffer
       (insert tweak-code)
       (write-file file)))))

(defvar portage-atom-re
  (rx-let ((ver-rev (index)
            (group-n index
             (group
              (one-or-more digit))
             (group
              (zero-or-more
               (group
                "."
                (one-or-more digit))))
             (group
              (optional
               (any "a-z")))
             (group
              (zero-or-more
               (group
                "_"
                (group
                 (or
                  "pre"
                  "p"
                  "beta"
                  "alpha"
                  "rc"))
                (zero-or-more digit))))
             (optional
              (group
               "-r"
               (group ; rev
                (one-or-more digit)))))))
    (rx-let ((cp-re
              (group ; group must be here for + op 3 to work
               (seq
                (any word "+")
                (zero-or-more (any word "+" "." "-")))
               "/"
               (seq
                (any word "+")
                (*? (any word "+" "-"))) ; note the non greedy XXX implementation is missing _ ??? but docstring says should be there ???
               (optional
                (group
                 "-"
                 (ver-rev 30))))))
      (rx-let ((op (group (or (any "=" "~") (seq (any ">" "<") (optional "=")))))
               (cpv-re (group cp-re "-" (ver-rev 50)))
               (slot-separator ":")
               (slot-loose (group (one-or-more (any word "+" "." "/" "*" "=" "-"))))
               (repo
                (optional
                 (seq
                  "::"
                  (group-n 80
                   word
                   (zero-or-more (any word "-")))))))
        (rx
         line-start
         (group-n 100 ; without-use
           (or
            (group-n 150 ; op
              op
              cpv-re)
            (group-n 200 ; star
              cpv-re
              "*")
            (group-n 225 ; simple XXX be aware that if you go over 255 match groups everything falls apart
              cp-re))
           (optional
            (group-n 70
              slot-separator
              slot-loose))
           repo)
         (optional
          (group-n 90 "[" (zero-or-more anything) "]"))
         line-end)))))

(defun portage-package-atom (string)
  "see portage.dep._atom_re.pattern"
  'category-package-version
  ;; FIXME apparently emacs will happily match _all_ of these not just the first ???
  ;; which is kind of annoying? or I have no idea, it doesn't go in order so if it
  ;; finds a shorter match it marks it
  (let ((without-use 100)
        (op 150)
        (star 200)
        (simple 225)
        (ver-rev-cp 30)
        (ver-rev-cpv 50)
        (slot 70)
        (repo 80)
        (use-str 90))
    (let ((match-point (string-match portage-atom-re string)))
      ;;(message ":match-data %S" (match-data))
      ;;(message ":match-point %s" match-point)
      (cl-multiple-value-bind
          (op cpv cp slot repo use-str version)
          (cond
           ((match-string op string)
            (values
             (match-string (+ op 1) string)
             (match-string (+ op 2) string)
             (match-string (+ op 3) string)
             (match-string slot string)
             (match-string repo string)
             (match-string use-str string)
             ;;(match-string (- (+ op (length (match-data))) 2) string) ; FIXME index will surely be off
             ;;(match-string (- (+ op (length (match-data))) 1) string) ; FIXME index will surely be off
             ;;(match-string (+ op (length (match-data)))) ; FIXME index will surely be off
             (match-string ver-rev-cpv string)))
           ((match-string star string)
            (values
             "=*"
             (match-string (+ star 1) string)
             (match-string (+ star 2) string)
             (match-string slot string)
             (match-string repo string)
             (match-string use-str string)
             nil))
           ((match-string simple string)
            (values
             nil
             (match-string (+ simple 1) string)
             (match-string (+ simple 1) string)
             (match-string slot string)
             (match-string repo string)
             (match-string use-str string)
             nil))
           ((match-string without-use string)
            (values nil nil nil nil nil nil nil))
           (t (values nil nil nil nil nil nil nil)))
        (list op cpv cp slot repo use-str version)))))

#+end_src

[[file:/usr/lib/python3.11/site-packages/portage/dep/__init__.py::def _get_atom_re]]

#+begin_src elisp
(portage-package-atom "dev-python/pyontutils")
(portage-package-atom "=dev-python/pyontutils")
(portage-package-atom "=dev-python/pyontutils-1")
(portage-package-atom "dev-python/pyontutils-1*")
(portage-package-atom "dev-python/pyontutils-1.2*")
(portage-package-atom ">=dev-python/pyontutils-1.2.3::tgbugs-overlay[test]")
(portage-package-atom ">=dev-python/pyontutils-1.2.3:0::tgbugs-overlay[test]")
(portage-package-atom ">=dev-python/pyontutils-1.2.3-r3:0::tgbugs-overlay[test]")
(portage-package-atom "p")
(portage-package-atom "p/p")
(portage-package-atom "p/p-1")
(portage-package-atom "=p/p-1")
(portage-package-atom "=p/p-1.0")

(portage-package-atom "cat-cat/pack::tgbugs-repo")
(portage-package-atom "cat-cat/pack")
(portage-package-atom "=cat-cat/pack")
#+end_src

#+begin_src elisp
(append
 (cons '("tags") (cons '("-") (mapcar #'list (cl-remove-duplicates (package-tags "app-editors/emacs") :test #'string=))))
 (cons '("-") (cons '("paths") (cons '("-") (mapcar #'list (cl-remove-duplicates (package-paths "app-editors/emacs") :test #'string=))))))
#+end_src

* automatic image dependency resolution
yeah we could do it with make or something
but technically all the contents are in this file
so we can use org to do it as well
#+begin_src bash
grep -r FROM --include='*Dockerfile'
#+end_src

TODO also need to track cases where artifacts have a cryptic dependencies
on other images e.g. openjdk, pypy3, sbcl-cross, etc. where we break strict
dependency tracking by e.g. building a package in a every so slightly tweaked
environment to avoid circular dependencies and then quickpkg to get the result

we're going to need context so we can find the build command
and then work backwards from the build commands to the Dockerfile
and then to the tangle target to the block contents
#+begin_src elisp :lexical yes :results none
(defun find-build-command (libc name)
  (let ((blockname (format "&%s-build-%s" libc name)))
    '()))

(setq
 docker-file-depends-on-tag
 (let (out)
   (org-fold-core-save-visibility 'use-markers
       (save-excursion ; TODO also need to save fold state
         (org-babel-map-executables nil
           ;; (language body arguments switches name start coderef)
           (let* ((info (org-babel-get-src-block-info))
                  (lang (nth 0 info))
                  (body (orgstrap--expand-body info))
                  (params (nth 2 info))
                  (name (nth 4 info ))
                  (tangle (cdr (assoc :tangle params))))
             (when (and (string= lang "dockerfile") tangle (not (string= tangle "no")))
               (setq out (cons
                          (cons
                           (substring tangle 2) ; drop ./ ; XXX FIXME error if doesnot start with ./
                           (with-temp-buffer
                             (insert body)
                             ;; TODO need to handle these kinds of cases
                             ;; ARG PROFILE_IMAGE=tgbugs/musl:profile-x
                             ;; ARG START_IMAGE=tgbugs/musl:pypy3
                             ;; don't bother trying to replicate this, it is
                             ;; vastly easier to just use evil-ex-global for now
                             (evil-ex-global (point-min) (point-max) "^\\(FROM\\|COPY --from=\\|RUN --mount=from=\\)" "d" t)
                             ;;(evil-ex-global (point-min) (point-max) "--from=builder" "d" nil) ; keep builder to make it easy to identify multi stage builds
                             (evil-ex-substitute (point-min) (point-max) '("^COPY --from=\\([^ ]+\\) .+$") "\\1")
                             (evil-ex-substitute (point-min) (point-max) '("^RUN --mount=from=\\([^ ,]+\\).+$") "\\1")
                             (split-string (replace-regexp-in-string "^FROM\\| as .+$" "" (buffer-string)) "\n" t " +"))) ; note, produces dupes
                          out))
               ))))
     out)))

(setq
 tag-produced-by-docker-file
 (let (out)
   (org-fold-core-save-visibility 'use-markers
       (save-excursion
         (org-babel-map-executables nil
           ;; (language body arguments switches name start coderef)
           (let* ((info (org-babel-get-src-block-info))
                  (lang (nth 0 info))
                  (body (orgstrap--expand-body info))
                  (params (nth 2 info))
                  (name (nth 4 info))
                  (tangle (cdr (assoc :tangle params)))
                  )
             (when (and name
                        (or (string= lang "screen")
                            (string= lang "bash"))
                        (string-match "&\\([^-]+\\)-build-\\(.+\\)" name))
               (setq out (cons
                          (cons
                           name
                           (with-temp-buffer
                             (insert (replace-regexp-in-string "\\\\\n" " " body))
                             (goto-char (point-min))
                             ;;(evil-ex-substitute (point-min) (point-max) '("\\\\n") " " "g") ; why doesn't this work ???
                             (evil-ex-global (point-min) (point-max) "^docker build" "d" t)
                             (goto-char (point-min))
                             ;;(re-search-forward "^--tag \\([^ ]+\\)[ \n]")
                             ;;(re-search-forward "^--file \\([^ ]+\\)[ \n]")

                             ;; TODO need to handle these kinds of cases
                             ;; --build-arg PROFILE_IMAGE='tgbugs/musl:profile-static-x' \
                             ;; --build-arg START_IMAGE='tgbugs/musl:updated' \
                             (cl-loop
                              for s in (split-string (replace-regexp-in-string " --" "\t" (buffer-string)) "\t" t " +")
                              when (string-match "^\\(tag\\|file\\) \\([^ ]+\\)" s)
                              collect (cons (match-string 1 s) (match-string 2 s)))))
                          out)))))
         (cl-loop
          for (name . alist) in out append
          (cl-loop
           with file = (cdr (assoc "file" alist)) ; there can be multiple tag
           for tag-pair in (cl-remove-if-not (lambda (p) (string= (car p) "tag")) alist)
           collect (cons
                    (cdr tag-pair)
                    file)))))))

(setq
 tag-depends-on-tag
 (cl-loop
  for (tag . file) in tag-produced-by-docker-file
  collect
  (cons
   tag
   (cdr (assoc file docker-file-depends-on-tag)))))

(setq
 docker-file-depends-on-docker-file
 (cl-loop
  for (file . tags) in docker-file-depends-on-tag
  collect
  (cons
   file
   (cl-loop
    for tag in tags
    collect (or (cdr (assoc tag tag-produced-by-docker-file)) tag)))))

#+end_src

#+begin_src elisp :results none
(load "~/ni/dev/elisp/trees.el")
#+end_src

#+begin_src elisp
(let ((adj (multi-to-adj tag-depends-on-tag)))
  (string-join
   (cl-loop
    for r in '(nil t)
    collect
    (format-tree nil (adj->nested adj :reverse r) :layout :down :node-formatter (lambda (n) (format "%s" n))))))
#+end_src

* Bootstrap :noexport:

#+name: orgstrap
#+begin_src elisp :results none :lexical yes :noweb yes
(defvar-local refresh      nil)
(defvar-local repos        nil)
(defvar-local sync-gentoo  nil)
(defvar-local resnap       nil)
(defvar-local live-rebuild nil)
(defvar-local no-pkg-bldr  nil)
(defvar-local only-static  nil)
(defvar-local use-podman   nil)

(defvar-local emerge-jobs "<<&default-emerge-jobs()>>")
(defvar-local host-binpkgs-port "<<&host-binpkgs-port()>>")
(defvar-local host-binpkgs-repo-name "<<&host-binpkgs-repo-name()>>")

(defvar-local path-dockerfiles (directory-file-name default-directory))
(defvar-local path-distfiles (expand-file-name "<<&default-host-distfiles-path()>>"))
(defvar-local path-binpkgs-root (expand-file-name "<<&default-host-binpkgs-root-path()>>"))
(defvar-local path-binpkgs (expand-file-name host-binpkgs-repo-name path-binpkgs-root))
(defvar-local path-distcc-hosts (expand-file-name "<<&default-host-distcc-hosts-path()>>"))
(defvar-local path-ssh (expand-file-name "<<&default-host-ssh-path()>>"))

(defvar-local path-sparcron-sparcur-config nil)
(defvar-local path-sparcron-secrets nil)
(defvar-local path-sparcron-gsaro nil)
; FIXME should we only mount the key itself to avoid config issues internally ? probably yes ? also known_hosts issues, the docker build ssh-agent doesn't really help here

(defvar-local no-screen-start-gui nil)
(defvar-local cli-debug (member "--debug" argv))

(when cli-debug
  (toggle-debug-on-error))

(defun dockerfiles-source-path-check (&optional do-error in-test)
  (let* ((check-paths
          (list
           (cons path-distfiles 'path-distfiles)
           (cons path-binpkgs-root 'path-binpkgs-root)
           (cons path-binpkgs 'path-binpkgs)
           (cons path-distcc-hosts 'path-distcc-hosts)
           (cons path-ssh 'path-ssh)
           (cons path-sparcron-sparcur-config 'path-sparcron-sparcur-config)
           (cons path-sparcron-secrets 'path-sparcron-secrets)
           (cons path-sparcron-gsaro 'path-sparcron-gsaro)
           ;; we check patch source paths because the target paths are deleted and recreated during tangle
           (cons (expand-file-name "<<&helper-repos()>>/sbcl/patches/dev-lisp" default-directory) 'path-sbcl-patch-source)))
         (dne-paths
          (cl-loop
           for (f . var) in check-paths
           unless (or
                   (and (not (null f))
                        (not (string= f ""))
                        (file-exists-p f))
                   (and (not in-test) (string-prefix-p "path-sparcron-" (symbol-name var))))
           collect (cons var f))))
    (when dne-paths
      (funcall (if do-error #'error #'warn) "missing paths %S" dne-paths))))

;; patch screen startup to work with :terminal (or) TODO needs to be upstreamed
(defun dockerfiles-source-org-babel-prep-session:screen (_session params)
  "Prepare SESSION according to the header arguments specified in PARAMS."
  (let* ((session (cdr (assq :session params)))
         (cmd (cdr (assq :cmd params)))
         (terminal (cdr (assq :terminal params)))
         (screenrc (cdr (assq :screenrc params)))
         (process-name (concat "org-babel: terminal (" session ")")))
    (if terminal
        (apply 'start-process process-name "*Messages*"
               terminal `("-T" ,(concat "org-babel: " session) "-e" ,org-babel-screen-location
                          "-c" ,screenrc "-mS" ,session ,cmd))
      (apply 'start-process process-name "*Messages*"
             org-babel-screen-location `("-c" ,screenrc "-DmS" ,session ,cmd)))
    ;; XXX: Is there a better way than the following?
    (while (not (org-babel-screen-session-socketname session))
      ;; wait until screen session is available before returning
      )))

(when noninteractive ; prevent wrong-type-argument keymapp nil errors due to our old friend `org-install-agenda-files-menu'
  ;; this is a pitfall that can happen as a result of any number of things, but the one that hit this file was running e.g.
  ;; ./source.org build-image run:sckan-build-save-image
  ;; to actually trigger the issue you need to run a #+begin_src bash :noweb yes AND THEN ACTUALLY NOWEB SOMETHING IN
  ;; anything less than all noweb yes and actually nowebbing something in will not trigger the issue
  (setq org-agenda-file-menu-enabled nil))

(defun screen-x (session &rest args)
  "run screen -X <ARGS> ... in org screen session SESSION."
  (let ((socket (org-babel-screen-session-socketname session)))
    (when socket
      (apply 'start-process (concat "org-babel: screen (" session ")") "*Messages*"
             org-babel-screen-location
             `("-S" ,socket "-X" ,@args)))))

(defun make-screen-vars ()
  (apply #'format
         "export _use_podman=%s _refresh=%s _repos=%s _sync_gentoo=%s _resnap=%s _live_rebuild=%s _nopkgbldr=%s _only_static=%s \\
_in_path_dockerfiles=%S _in_path_distfiles=%S _in_path_binpkgs_root=%S _in_path_binpkgs=%S _in_path_distcc_hosts=%S _in_path_ssh=%S \\
%s %s %s%s;"
         (append
          (mapcar (lambda (b) (if b 1 ""))
                  (list use-podman refresh repos sync-gentoo resnap live-rebuild no-pkg-bldr only-static))
          (list path-dockerfiles path-distfiles path-binpkgs-root path-binpkgs path-distcc-hosts path-ssh)
          ;; FIXME TODO see whether whether we can use the path-sparcron approach for all paths?
          ;; the difference seems to be whether it makes sense to embed default paths in this file
          ;; because in some cases e.g. paths with identifiers in their name, defaults are not helpful
          (mapcar (lambda (s)
                    (if (symbol-value s)
                        (format "_%s=%S" (string-replace "-" "_" (symbol-name s)) (symbol-value s))
                      ""))
                  '(path-sparcron-sparcur-config path-sparcron-secrets path-sparcron-gsaro))
          (list (if (string= system-type "windows-nt") " MSYS_NO_PATHCONV=1" "")))))

(defvar retfile-cache-directory (expand-file-name "retfile/cache/" default-directory))
(defun retfile (cypher checksum path-or-url &rest alternates)
  "Retrieve a remote file and ensure its checksum matches." ; adapted from `reval-minimal'
  (let* (done (o url-handler-mode) (csn (symbol-name checksum))
              (cache-path (concat retfile-cache-directory (substring csn 0 2) "/" csn
                                  "-" (file-name-nondirectory path-or-url))))
    (url-handler-mode)
    (unwind-protect
        (cl-loop for path-or-url in (cons cache-path (cons path-or-url alternates))
                 do (when (file-exists-p path-or-url)
                      (let* ((buffer (find-file-noselect path-or-url :nowarn :rawfile)) ; FIXME yes ffns is bad
                             (buffer-checksum (intern (secure-hash cypher buffer))))
                        (if (eq buffer-checksum checksum)
                            (progn
                              (unless (string= path-or-url cache-path)
                                (let ((parent-path (file-name-directory cache-path))
                                      make-backup-files)
                                  (unless (file-directory-p parent-path)
                                    (make-directory parent-path t))
                                  (with-current-buffer buffer
                                    (write-file cache-path))))
                              (setq done t)
                              (kill-buffer buffer))
                          (kill-buffer buffer) ; kill so cannot accidentally evaled
                          (error "reval: checksum mismatch! %s" path-or-url))))
                 until done)
      (unless o
        (url-handler-mode 0)))
    cache-path))

(defun fetch-ghc ()
  ;; hack to fetch ghc pacakges since bootstrap is not possible on musl
  (let ((src-928
         (retfile
          'sha256 '2b1d9ce61b88dcbcc2d32da1b7da50122e9dd0afe805edbcf05fb16f0f0b74d6
          "https://files.olympiangods.org/ghc-9.2.8-1.gpkg.tar"))
        (src-902
         (retfile
          'sha256 'a277f6ba1d19716eec8a5fad031a072796d79ae0e1ebeb0952aa18af7f848627
          "https://files.olympiangods.org/ghc-9.0.2-r4-5.gpkg.tar"))
        (ghc-path (expand-file-name "dev-lang/ghc" path-binpkgs))))
  nil)

(defun run-setup ()
  ;; docker buildx is installed
  ;; docker daemon config ; XXX this can't be done due to bad design of docker (investigate podman)
  ;; docker client config
  ;;  remote repo key
  ;; binpkgs path
  ;; portage ssh keys
  ;; distcc hosts file

  ;; in alpine:latest around 2023-07-11 it seems to be sufficient to just set the binpkgs paths
  ;; at least when running in windows for bootstrap we don't seem to need any further configuration
  (let ((bp path-binpkgs-root)
        (missing-buildx
         (with-temp-buffer
           (let* ((return-code (apply #'call-process (if use-podman "podman" "docker") nil (current-buffer) nil '("buildx" "version"))))
             (not (= 0 return-code)))))
        (missing-configs
         (and
          (not use-podman)
          (cl-remove-if #'file-exists-p
                        '("~/.docker/config.json" "/etc/docker/daemon.json")))))
    (cl-loop
     for sp in (list host-binpkgs-repo-name "cross/gnu/x86_64-pc-linux-musl" "cross/gnu/x86_64-gentoo-linux-musl" )
     do (make-directory (expand-file-name sp bp) 'parents))
    (unless (file-exists-p (expand-file-name (concat host-binpkgs-repo-name "/Packages") bp))
      ;; FIXME this goes to network during setup which is back
      ;; FIXME no feedback in the terminal when this is running since we run it in bash not screen
      (execute-src-block "&run-quickpkg-first-time"))
    ;; TODO docker buildx version || oops
    (when missing-buildx
      (message "Missing docker buildx."))
    (when missing-configs
      ;; even if we can't create /etc/docker/daemon.json we can warn if it is missing (for now)
      (message "Missing docker configs %S" missing-configs))
    (when (or missing-buildx missing-configs)
      ;; sigh ... the must collect errors pattern ...
      (error "Setup incomplete."))))

(defun fix-ocbe-source ()
  "it would seem that blanking `enable-local-eval' resets this"
  (setq-local
   org-confirm-babel-evaluate
   (lambda (lang body)
     (not (or (and (member lang '("elisp" "emacs-lisp"))
                   (or (string= body "value")
                       ;; XXX WARNING BE SURE TO REVIEW ALL INSTANCES OF elisp-safe-block
                       (string-prefix-p "; elisp-safe-block
" body)))
              (and (member lang '("python"))
                   ;; XXX WARNING BE SURE TO REVIEW ALL INSTANCES OF py-safe-block
                   (string-prefix-p "# py-safe-block
" body))
              (and (member lang '("bash"))
                   (string-prefix-p "# bash-safe-block
" body))
              (and (member lang '("screen"))
                   (string= (cl-subseq body 0 15) "HISTFILE=~/.org")))))))

(fix-ocbe-source)

(defun dedupe-lines (blockname)
  (let* ((info (save-excursion
                 (org-save-outline-visibility 'use-markers
                   (let ((obs (org-babel-find-named-block blockname)))
                     (if obs (goto-char obs)
                       (error "No block named %s" blockname)))
                   (org-babel-get-src-block-info))))
         (body (org-babel--expand-body info)))
    (string-join (sort (cl-remove-duplicates
                        (split-string body "\n")
                        :test #'string=)
                       #'string<)
                 "\n")))

(defun do-non-tangled-files ()
  "process any non-tangled files generated in other ways
currently only sbcl static uses this but other things might need it in the future"
  ;; FIXME during bootstrap this doesn't work because we have to already
  ;; have done docker operations to get the state we need to generate
  ;; the patches ... so the logical thing would be to enable this here for
  ;; manual workflows and to be able to run just this section at the right
  ;; point in +run-main i think+ run-musl
  ;; TODO run [[&sbcl-funs]] and friends as well
  ;; WARNING sbcl-generate-patches is currently dependent on tgbugs/musl:updated
  ;; we could make it dependent only on gentoo/stage3:amd64-musl-hardened
  ;; local-portage-snap but it would still need to run after tangle
  (let ((sbcl-static-patches-source "<<&helper-repos()>>/sbcl/patches")
        (sbcl-static-patches-target "musl/package-builder/patches"))
    (cl-loop
     for (source . target) in
     (list
      (cons
       sbcl-static-patches-source
       sbcl-static-patches-target))
     do
     (when (file-exists-p source)
       (copy-directory source target 'keep-time 'parents 'copy-contents)))))

(defmacro with-error-on-fail (&rest body)
  `(cl-letf (((symbol-function 'org-babel-eval-error-notify)
              (lambda (exit-code stderr)
                (when (> exit-code 0)
                  (error "Babel evaluation failed with %s%s%s"
                         exit-code
                         (if stderr "\n" "")
                         (if stderr stderr ""))))))
     ,@body))

(defmacro with-store-error-on-fail (&rest body)
  `(let (heh)
     (cl-letf
         (((symbol-function 'org-babel-eval-error-notify)
           (lambda (exit-code stderr)
             (when (> exit-code 0)
               (setq
                heh
                (format
                 "Babel evaluation failed with %s%s%s"
                 exit-code
                 (if stderr "\n" "")
                 (if stderr stderr "")))))))
       (let ((out ,@body))
         (if heh
             (signal 'error (cons out heh))
           out)
         ))))

(defun find-tag-build-scr-block (tag &optional execute no-cache)
  (save-excursion
    (goto-char (point-min))
    (re-search-forward (concat "--tag " tag " ")) ; trailing space to avoid shared prefix issues
    (let ((info (org-babel-get-src-block-info)))
      (when execute
        (when no-cache ; FIXME assumes --tag name \
          (insert "--no-cache "))
        (org-babel-execute-src-block)
        (undo))
      ;; always return the name we we execute build blocks they are nearly always screen
      ;; and the return value is thus nil
      (nth 4 info))))

(defun execute-tag-build-src-block (tag)
  (find-tag-build-scr-block 'execute))

(defun execute-src-block (name)
  (save-excursion ; can't use org-sbe because it calls some other random block !?
    (org-babel-goto-named-src-block name)
    (org-babel-execute-src-block)))

(defun cli-opt (opt)
  (let ((index (cl-position opt argv :test #'string=)))
    (and index (elt argv (1+ index)))))

(defun --results-silent (fun &rest args)
  "See `ow--results-silent'."
  (let ((result (car args))
        (result-params (cadr args)))
    (if (member "silent" result-params)
        result
      (apply fun args))))

(when (string= system-type "windows-nt")
  ;; ensure that when tangling files that might be mounted into
  ;; a unix environment that they won't have \r at bol
  ;; as far as I can tell there is no way to set this in the
  ;; temp buffer when running org-babel-tangle so we do it here
  (setq-default buffer-file-coding-system 'utf-8-unix)
  (let ((git-bash-path "C:/Program Files/Git/bin/"))
    (unless (file-exists-p git-bash-path)
      ;; if wsl or wsl2 is installed then the bash on path is likely
      ;; to be the system32 bash which can drop output and commands
      ;; and could easily damage a system if run
      (error "git bash is not installed"))
    (add-to-list 'exec-path git-bash-path)
    (unless
        (string=
         (executable-find "bash")
         (expand-file-name "bash.exe" git-bash-path))
      (error "git present but git bash is missing"))))

(defun build-no-gui ()
  (execute-src-block "bash-async-package-server-cli")
  (sleep-for 0.1)
  (execute-src-block "bash-async-workflow-cli"))

(defun toggle-comint-scroll-to-bottom-on-output (&optional uarg)
  (interactive "P")
  (setq
   comint-scroll-to-bottom-on-output
   (not comint-scroll-to-bottom-on-output)))

(when (or noninteractive no-screen-start-gui)
  (let ((setup (member "setup" argv))
        (build (member "build" argv))
        (tangle (member "tangle" argv))
        (non-tangled (member "non-tangled" argv))
        (build-image (member "build-image" argv)) ; FIXME yes the naming is annoying
        (fetch-ghc (member "fetch-ghc" argv))
        (test (member "test" argv)))
    (let ((no-screen    (and build (member "--no-screen" argv)))
          (refresh      (and build (member "--refresh" argv)))
          (repos        (and build (member "--repos" argv)))
          (sync-gentoo  (and build (member "--sync-gentoo" argv)))
          (resnap       (and build (member "--resnap" argv)))
          (only-static  (and build (member "--only-static" argv)))
          (live-rebuild (and build (member "--live-rebuild" argv)))
          (no-pkg-bldr  (and build (or
                                    (member "--no-build" argv)
                                    (member "--no-pkg-bldr" argv))))
          (use-podman (member "--podman" argv))
          (cli-images  (cl-remove-if (lambda (s) (string= s "--no-cache")) (cdr build-image))) ; must come last will build all listed images in order they are listed
          (cli-no-cache (member "--no-cache" argv))
          (cli-check-paths (member "--check-paths" argv))
          (cli-pretend (member "--pretend" argv)) ; FIXME sequencing wrt build-image and cli-images
          (cli-uif (cli-opt "--user-init-file"))
          (cli-ued (cli-opt "--user-emacs-directory"))
          (cli-path-dockerfiles  (cli-opt "--path-dockerfiles"))
          (cli-path-distfiles    (cli-opt "--path-distfiles"))
          (cli-path-binpkgs-root (cli-opt "--path-binpkgs-root"))
          (cli-path-binpkgs      (cli-opt "--path-binpkgs"))
          (cli-path-distcc-hosts (cli-opt "--path-distcc-hosts"))
          (cli-path-ssh          (cli-opt "--path-ssh"))
          (cli-ssc (cli-opt "--path-sparcron-sparcur-config"))
          (cli-ss  (cli-opt "--path-sparcron-secrets"))
          (cli-sg  (cli-opt "--path-sparcron-gsaro")))
      (unless (or no-screen (string= system-type "windows-nt"))
        ;; need to be able to pass argv to the emacs subprocess
        (setq argv nil)) ; FIXME should we allow the uif to see argv? probably not?
      (when cli-uif
        (load cli-uif)
        (setq user-init-file cli-uif))
      (let ((path-dockerfiles (or cli-path-dockerfiles path-dockerfiles ""))
            (path-distfiles (or cli-path-distfiles path-distfiles ""))
            (path-binpkgs-root (or cli-path-binpkgs-root path-binpkgs-root ""))
            (path-binpkgs (or cli-path-binpkgs path-binpkgs ""))
            (path-distcc-hosts (or cli-path-distcc-hosts path-distcc-hosts ""))
            (path-ssh (or cli-path-ssh path-ssh ""))
            (path-sparcron-sparcur-config (or cli-ssc path-sparcron-sparcur-config ""))
            (path-sparcron-secrets (or cli-ss path-sparcron-secrets ""))
            (path-sparcron-gsaro (or cli-sg path-sparcron-gsaro "")))
        (dockerfiles-source-path-check cli-check-paths test) ; FIXME TODO can we use --pretend here too?
        ;; FIXME need an extensible way to handle this probably? see prrequaestor for inspiration?
        ;; or not quite because we need a way to pass variable values from options, but that means
        ;; that the elisp does have to explicitly make the bridge unless we come up with a fully
        ;; dynamic/generative option, see how frequently we actually need it, vs using cli-uif
        (when (or build build-image test setup)
          (let ((no-screen (or no-screen no-screen-start-gui))
                (toload '((shell . t))))
            (unless no-screen
              (setq toload (cons '(screen . t) toload)))
            (org-babel-do-load-languages
             'org-babel-load-languages
             toload)
            (unless no-screen
              (fmakunbound 'org-babel-prep-session:screen)
              (defalias 'org-babel-prep-session:screen #'dockerfiles-source-org-babel-prep-session:screen))))
        (when setup
          (run-setup))
        (when fetch-ghc
          (fetch-ghc))
        (when no-screen-start-gui
          ;; FIXME better handling of differential dispatch please ...
          (when build
            (fix-ocbe-source)
            (setq comint-scroll-to-bottom-on-output t)
            (build-no-gui)
            (switch-to-buffer
             (display-buffer "org-session"))
            ;; have to pass build on argv so variables will be set
            ;; correctly but we don't actually want to run build
            (setq build nil)))
        (when build
          ;; bash session async and screen are now both implemented
          ;; general steps:
          ;; check that setup was run correctly, if not, fail
          ;; start screen or start an emacs process that is not in batch mode
          ;; start up the packages server
          ;; run build
          (if (or no-screen (string= system-type "windows-nt"))
              (let ((emacs-command (expand-file-name invocation-name invocation-directory)))
                (apply
                 #'call-process
                 `(
                   ,emacs-command nil 0 nil
                   "-no-init-file"
                   "-no-site-file"
                   "-eval"
                   ,(let*
                        ((orgstrap-shebang (dedupe-lines "orgstrap-shebang"))
                         print-level print-length
                         (reuse
                          (with-temp-buffer
                            (insert orgstrap-shebang)
                            (goto-char 0)
                            (re-search-forward "^emacs ")
                            (re-search-forward "-eval ")
                            (read (read (thing-at-point 'string))))))
                      (push '(defvar no-screen-start-gui t) (nthcdr 2 reuse))
                      (prin1-to-string reuse))
                   ,(buffer-file-name)
                   ,@argv)))
            (let* ((_ (require 'url-http))
                   (package-server-running
                    (condition-case out-err
                        (url-http-file-exists-p (format "http://localhost:%s/%s/Packages" host-binpkgs-port host-binpkgs-repo-name))
                      (file-error ; FIXME not sure why also need the Cannot assign requested address case?
                       (unless (member (caddr out-err) '("Connection refused" "Cannot assign requested address"))
                         (signal (car out-err) (cdr out-err))))) ))
              ;; because of how ob-screen calls work we have to test whether
              ;; the package server is running from elisp otherwise we don't
              ;; know what we need to do at startup FIXME also there isn't
              ;; a convenient way to capture the return value if something
              ;; fails here :/
              (fix-ocbe-source)
              (unless package-server-running
                (let ((pkg-process (execute-src-block "package-server-cli")))
                  (while (eq (process-status pkg-process) 'run)
                    (sleep-for 0.1)))
                (let ((xs-process (screen-x "org-session" "screen")))
                  (while (eq (process-status xs-process) 'run)
                    (sleep-for 0.1))))
              (let ((process (execute-src-block "workflow-cli")))
                ;; prevent emacs exit from killing the subprocess before it has finished
                ;; send to the screen session
                (while (eq (process-status process) 'run)
                  (sleep-for 0.1))))))
        (when build-image ; FIXME sometimes leaving from this can still produce Wrong type argument: keymapp, nil
          (fix-ocbe-source)
          (if cli-images
              (let* ((org-confirm-babel-evaluate (lambda (_l _b))) ; XXX BEWARE THESE WILL RUN WITHOUT PROMPTING
                     (max-iml 0)
                     backtrace-on-error-noninteractive ; we mostly know where the error is if we hit this
                     (image-blocks
                      (cl-loop
                       for image in cli-images
                       do (let ((iml (length image)))
                            (when (> iml max-iml)
                              (setq max-iml iml))) ; FIXME consider collecting image names, printing them and only then executing them
                       collect (cons
                                image
                                (with-error-on-fail ; FIXME sequencing here is a bit off, so we fail to get the block name
                                 (find-tag-build-scr-block image (not cli-pretend) cli-no-cache))))))
                (message "image src blocks:")
                (cl-loop
                 for (image . block) in image-blocks do
                 (message "%s %s %s" image (make-string (- max-iml (length image)) ?\s) block)))
            (let (backtrace-on-error-noninteractive)
              (user-error "no images to build"))))
        (when tangle
          (unless (fboundp #'dockerfile-mode)
            (define-derived-mode dockerfile-mode prog-mode "Dockerfile"
              "Stub to avoid comment-start issues"
              (set (make-local-variable 'comment-start) "#")))
          (org-babel-do-load-languages
           'org-babel-load-languages
           '((python . t)))
          (let (enable-local-eval)
            ;; this pattern is required when tangling to avoid infinite loops
            (revert-buffer nil t nil)
            (setq-local find-file-literally nil))
          (fix-ocbe-source)
          (org-babel-tangle)
          (do-non-tangled-files))
        (when non-tangled
          (do-non-tangled-files))
        (when test
          ;; sckan
          ;; kg-dev
          ;; kg-release
          ;; sparcron (probably not? should do it)
          (fix-ocbe-source)
          (save-excursion ; make sure ./bin/workflow-funs.sh is up to date
            (goto-char (point-min))
            (re-search-forward "^#\\+name: &workflow-funs-main-to-tangle$" nil t)
            (org-babel-tangle '(4)))
          ;; collect named test blocks by pattern
          (let (blocks errors)
            (save-excursion
              (goto-char (point-min))
              (while (re-search-forward "^#\\+name: \\(&test-.+\\)" nil t)
                (setq blocks (cons (match-string 1) blocks))
                (let* ((info-raw (org-babel-get-src-block-info))
                       (info (if (string= (nth 0 info-raw) "screen")
                                 (cons "bash" (cdr info-raw))
                               info-raw)))
                  ;; we use setf here instead of passing :results in params below
                  ;; because if "none" is specified on a block directly then it will
                  ;; appear after silent and "result silenced" will still print :/
                  (setf (alist-get :results (nth 2 info)) "raw silent") ; heh setf power
                  (setf (nth 1 info)
                        (concat
                         "source ./bin/workflow-funs.sh\n" ; ensure test-image is defined
                         "exec 2>&1\n"
                         (nth 1 info)))
                  (let* ((org-confirm-babel-evaluate (lambda (_l _b)))
                         (out
                          (unwind-protect
                              ;; see `ow-babel-execute-src-block' for the full implementation
                              (progn
                                (advice-add #'org-babel-insert-result :around #'--results-silent)
                                (condition-case out-err
                                    (with-store-error-on-fail
                                     (org-babel-execute-src-block ; ensure no session is set or we also lose return value
                                      nil info '(;(:results . "raw silent") ; see above about :results "none" interaction
                                                 (:session))))
                                  (error
                                   (setq errors (cons (car blocks) errors))
                                   (princ (cddr out-err) #'external-debugging-output)
                                   (cadr out-err))))
                            (advice-remove #'org-babel-insert-result #'--results-silent)
                            )))
                    (princ out #'external-debugging-output)
                    (princ "\n\n" #'external-debugging-output)
                    nil))))
            (message "test blocks run: %s" (reverse blocks))
            (when errors
              (message "the following tests failed: %s" (reverse errors))
              (kill-emacs 1))))))))
#+end_src

# #+name: &test-expect-fail
#+begin_src bash :none
false
#+end_src

Helper block to make it easier to use elisp functions as noweb inputs.
#+name: ident
#+begin_src elisp :var value=""
value
#+end_src

Helper blocks to pass variables to screen from cli.
#+name: &screen-pass-vars
#+begin_src elisp :var value=(make-screen-vars)
value
#+end_src

Note that all screen blocks have an implicit eval.
#+name: &screen-set-vars
#+begin_src screen
<<&screen-pass-vars()>>
#+end_src

*** test with-error-on-fail
#+name: bash-exit-0-test
#+begin_src bash
exit 0
#+end_src

#+RESULTS: bash-exit-0-test

#+name: bash-exit-1-test
#+begin_src bash
exit 1
#+end_src

#+begin_src elisp :lexical yes
(let ((org-confirm-babel-evaluate (lambda (_l _b))))
  (with-error-on-fail (execute-src-block "bash-exit-0-test"))
  (message "before")
  (with-error-on-fail (execute-src-block "bash-exit-1-test"))
  (message "after"))
#+end_src

** Local Variables :ARCHIVE:
# close powershell comment #>
# Local Variables:
# indent-tabs-mode: nil
# org-adapt-indentation: nil
# org-edit-src-content-indentation: 0
# eval: (progn (setq-local orgstrap-min-org-version "8.2.10") (let ((a (org-version)) (n orgstrap-min-org-version)) (or (fboundp #'orgstrap--confirm-eval) (not n) (string< n a) (string= n a) (error "Your Org is too old! %s < %s" a n))) (defun orgstrap-norm-func--dprp-1-0 (body) (let ((p (read (concat "(progn\n" body "\n)"))) (m '(defun defun-local defmacro defvar defvar-local defconst defcustom)) print-quoted print-length print-level) (cl-labels ((f (b) (cl-loop for e in b when (listp e) do (or (and (memq (car e) m) (let ((n (nthcdr 4 e))) (and (stringp (nth 3 e)) (or (cl-subseq m 3) n) (f n) (or (setcdr (cddr e) n) t)))) (f e))) p)) (prin1-to-string (f p))))) (unless (boundp 'orgstrap-norm-func) (defvar-local orgstrap-norm-func orgstrap-norm-func-name)) (defun orgstrap-norm-embd (body) (funcall orgstrap-norm-func body)) (unless (fboundp #'orgstrap-norm) (defalias 'orgstrap-norm #'orgstrap-norm-embd)) (defun orgstrap-org-src-coderef-regexp (_fmt &optional label) (let ((fmt org-coderef-label-format)) (format "\\([:blank:]*\\(%s\\)[:blank:]*\\)$" (replace-regexp-in-string "%s" (if label (regexp-quote label) "\\([-a-zA-Z0-9_][-a-zA-Z0-9_ ]*\\)") (regexp-quote fmt) nil t)))) (unless (fboundp #'org-src-coderef-regexp) (defalias 'org-src-coderef-regexp #'orgstrap-org-src-coderef-regexp)) (defun orgstrap--expand-body (info) (let ((coderef (nth 6 info)) (expand (if (org-babel-noweb-p (nth 2 info) :eval) (org-babel-expand-noweb-references info) (nth 1 info)))) (if (not coderef) expand (replace-regexp-in-string (org-src-coderef-regexp coderef) "" expand nil nil 1)))) (defun orgstrap--confirm-eval-portable (lang _body) (not (and (member lang '("elisp" "emacs-lisp")) (let* ((body (orgstrap--expand-body (org-babel-get-src-block-info))) (body-normalized (orgstrap-norm body)) (content-checksum (intern (secure-hash orgstrap-cypher body-normalized)))) (eq orgstrap-block-checksum content-checksum))))) (unless (fboundp #'orgstrap--confirm-eval) (defalias 'orgstrap--confirm-eval #'orgstrap--confirm-eval-portable)) (let (enable-local-eval) (vc-find-file-hook)) (let ((ocbe org-confirm-babel-evaluate) (obs (org-babel-find-named-block "orgstrap"))) (if obs (unwind-protect (save-excursion (setq-local orgstrap-norm-func orgstrap-norm-func-name) (setq-local org-confirm-babel-evaluate #'orgstrap--confirm-eval) (goto-char obs) (org-babel-execute-src-block)) (when (eq org-confirm-babel-evaluate #'orgstrap--confirm-eval) (setq-local org-confirm-babel-evaluate ocbe)) (ignore-errors (org-set-visibility-according-to-property))) (warn "No orgstrap block."))))
# End:
